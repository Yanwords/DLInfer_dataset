{
 "1": {
  "name": "VERSION",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/setup.py",
  "lineno": "19",
  "column": "0",
  "context": "re so we don't import allennlp whilst setting up.\nVERSION = {}\nwith open(\"adversarialnlp/version.py\", \"r\") as ver",
  "context_lines": "#   X.YrcN  # Release Candidate\n#   X.Y     # Final release\n\n# version.py defines the VERSION and VERSION_SHORT variables.\n# We use exec here so we don't import allennlp whilst setting up.\nVERSION = {}\nwith open(\"adversarialnlp/version.py\", \"r\") as version_file:\n    exec(version_file.read(), VERSION)\n\n# make pytest-runner a conditional requirement,\n# per: https://github.com/pytest-dev/pytest-runner#considerations\n",
  "slicing": [
   "VERSION = {}\n",
   "    exec(version_file.read(), VERSION)\n",
   "      version=VERSION[\"VERSION\"],\n"
  ]
 },
 "2": {
  "name": "needs_pytest",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/setup.py",
  "lineno": "25",
  "column": "0",
  "context": "ithub.com/pytest-dev/pytest-runner#considerations\nneeds_pytest = {'pytest', 'test', 'ptr'}.intersection(sys.argv)\npytest_runner = ['pytest-runner'] if needs_pytest ",
  "context_lines": "with open(\"adversarialnlp/version.py\", \"r\") as version_file:\n    exec(version_file.read(), VERSION)\n\n# make pytest-runner a conditional requirement,\n# per: https://github.com/pytest-dev/pytest-runner#considerations\nneeds_pytest = {'pytest', 'test', 'ptr'}.intersection(sys.argv)\npytest_runner = ['pytest-runner'] if needs_pytest else []\n\nwith open('requirements.txt', 'r') as f:\n    install_requires = [l for l in f.readlines() if not l.startswith('# ')]\n\nsetup_requirements = [\n",
  "slicing": [
   "VERSION = {}\n",
   "with open(\"adversarialnlp/version.py\", \"r\") as version_file:\n",
   "    exec(version_file.read(), VERSION)\n",
   "needs_pytest = {'pytest', 'lstm', 'ptr'}.intersection(sys.argv)\n",
   "pytest_runner = ['pytest-runner'] if needs_pytest else []\n",
   "] + pytest_runner\n",
   "      version=VERSION[\"VERSION\"],\n"
  ]
 },
 "3": {
  "name": "pytest_runner",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/setup.py",
  "lineno": "26",
  "column": "0",
  "context": " {'pytest', 'test', 'ptr'}.intersection(sys.argv)\npytest_runner = ['pytest-runner'] if needs_pytest else []\n\nwith open('requirements.txt', 'r') as f:\n    inst",
  "context_lines": "    exec(version_file.read(), VERSION)\n\n# make pytest-runner a conditional requirement,\n# per: https://github.com/pytest-dev/pytest-runner#considerations\nneeds_pytest = {'pytest', 'test', 'ptr'}.intersection(sys.argv)\npytest_runner = ['pytest-runner'] if needs_pytest else []\n\nwith open('requirements.txt', 'r') as f:\n    install_requires = [l for l in f.readlines() if not l.startswith('# ')]\n\nsetup_requirements = [\n",
  "slicing": [
   "VERSION = {}\n",
   "with open(\"adversarialnlp/version.py\", \"r\") as version_file:\n",
   "    exec(version_file.read(), VERSION)\n",
   "needs_pytest = {'pytest', 'lstm', 'ptr'}.intersection(sys.argv)\n",
   "pytest_runner = ['pytest-runner'] if needs_pytest else []\n",
   "] + pytest_runner\n",
   "      version=VERSION[\"VERSION\"],\n"
  ]
 },
 "4": {
  "name": "install_requires",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/setup.py",
  "lineno": "29",
  "column": "4",
  "context": " []\n\nwith open('requirements.txt', 'r') as f:\n    install_requires = [l for l in f.readlines() if not l.startswith('# ')]\n\nsetup_requirements = [\n    # add other setup requ",
  "context_lines": "# per: https://github.com/pytest-dev/pytest-runner#considerations\nneeds_pytest = {'pytest', 'test', 'ptr'}.intersection(sys.argv)\npytest_runner = ['pytest-runner'] if needs_pytest else []\n\nwith open('requirements.txt', 'r') as f:\n    install_requires = [l for l in f.readlines() if not l.startswith('# ')]\n\nsetup_requirements = [\n    # add other setup requirements as necessary\n] + pytest_runner\n\n",
  "slicing": [
   "VERSION = {}\n",
   "with open(\"adversarialnlp/version.py\", \"r\") as version_file:\n",
   "    exec(version_file.read(), VERSION)\n",
   "needs_pytest = {'pytest', 'lstm', 'ptr'}.intersection(sys.argv)\n",
   "pytest_runner = ['pytest-runner'] if needs_pytest else []\n",
   "with open('requirements.txt', 'r') as f:\n",
   "    install_requires = [l for l in f.readlines() if not l.startswith('# ')]\n",
   "] + pytest_runner\n",
   "      version=VERSION[\"VERSION\"],\n",
   "      install_requires=install_requires,\n"
  ]
 },
 "5": {
  "name": "setup_requirements",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/setup.py",
  "lineno": "31",
  "column": "0",
  "context": "or l in f.readlines() if not l.startswith('# ')]\n\nsetup_requirements = [\n    # add other setup requirements as necessary\n] ",
  "context_lines": "needs_pytest = {'pytest', 'test', 'ptr'}.intersection(sys.argv)\npytest_runner = ['pytest-runner'] if needs_pytest else []\n\nwith open('requirements.txt', 'r') as f:\n    install_requires = [l for l in f.readlines() if not l.startswith('# ')]\n\nsetup_requirements = [\n    # add other setup requirements as necessary\n] + pytest_runner\n\nsetup(name='adversarialnlp',\n      version=VERSION[\"VERSION\"],\n",
  "slicing": [
   "setup_requirements = [\n",
   "      setup_requires=setup_requirements,\n"
  ]
 },
 "6": {
  "name": "extensions",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "34",
  "column": "0",
  "context": "inx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = ['sphinx.ext.autodoc',\n              'sphinx.ext.intersphinx',\n          ",
  "context_lines": "# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = ['sphinx.ext.autodoc',\n              'sphinx.ext.intersphinx',\n              'sphinx.ext.mathjax',\n              'sphinx.ext.ifconfig',\n              'sphinx.ext.viewcode',\n",
  "slicing": "extensions = ['sphinx.ext.autodoc',\n"
 },
 "7": {
  "name": "templates_path",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "42",
  "column": "0",
  "context": "ntain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can s",
  "context_lines": "              'sphinx.ext.ifconfig',\n              'sphinx.ext.viewcode',\n              'sphinx.ext.napoleon']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n",
  "slicing": "templates_path = ['_templates']\n"
 },
 "8": {
  "name": "source_suffix",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "48",
  "column": "0",
  "context": "st of string:\n#\n# source_suffix = ['.rst', '.md']\nsource_suffix = '.rst'\n\n# The master toctree document.\nmaster_doc = 'inde",
  "context_lines": "# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = ['.rst', '.md']\nsource_suffix = '.rst'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\n",
  "slicing": "source_suffix = '.rst'\n"
 },
 "9": {
  "name": "master_doc",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "51",
  "column": "0",
  "context": "e_suffix = '.rst'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject ",
  "context_lines": "#\n# source_suffix = ['.rst', '.md']\nsource_suffix = '.rst'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = 'AdversarialNLP'\ncopyright = '2018, Mohit Iyyer, Pasquale Minervini, Victor Sanh, Thomas Wolf, Rowan Zellers'\n",
  "slicing": [
   "master_doc = 'index'\n",
   "    (master_doc, 'AdversarialNLP.tex', 'AdversarialNLP Documentation',\n",
   "    (master_doc, 'adversarialnlp', 'AdversarialNLP Documentation',\n",
   "    (master_doc, 'AdversarialNLP', 'AdversarialNLP Documentation',\n"
  ]
 },
 "10": {
  "name": "project",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "54",
  "column": "0",
  "context": "'index'\n\n# General information about the project.\nproject = 'AdversarialNLP'\ncopyright = '2018, Mohit Iyyer, Pasquale Minervini",
  "context_lines": "source_suffix = '.rst'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = 'AdversarialNLP'\ncopyright = '2018, Mohit Iyyer, Pasquale Minervini, Victor Sanh, Thomas Wolf, Rowan Zellers'\nauthor = 'Mohit Iyyer, Pasquale Minervini, Victor Sanh, Thomas Wolf, Rowan Zellers'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n",
  "slicing": "project = 'AdversarialNLP'\n"
 },
 "11": {
  "name": "copyright",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "55",
  "column": "0",
  "context": "ion about the project.\nproject = 'AdversarialNLP'\ncopyright = '2018, Mohit Iyyer, Pasquale Minervini, Victor Sanh, Thomas Wolf, Rowan Zellers'\nauthor = 'Mohit Iyyer, Pasquale Minervini, Victor ",
  "context_lines": "# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = 'AdversarialNLP'\ncopyright = '2018, Mohit Iyyer, Pasquale Minervini, Victor Sanh, Thomas Wolf, Rowan Zellers'\nauthor = 'Mohit Iyyer, Pasquale Minervini, Victor Sanh, Thomas Wolf, Rowan Zellers'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n",
  "slicing": "copyright = '2018, Mohit Iyyer, Pasquale Minervini, Victor Sanh, Thomas Wolf, Rowan Zellers'\n"
 },
 "12": {
  "name": "author",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "56",
  "column": "0",
  "context": "nervini, Victor Sanh, Thomas Wolf, Rowan Zellers'\nauthor = 'Mohit Iyyer, Pasquale Minervini, Victor Sanh, Thomas Wolf, Rowan Zellers'\n\n# The version info for the project you're documen",
  "context_lines": "master_doc = 'index'\n\n# General information about the project.\nproject = 'AdversarialNLP'\ncopyright = '2018, Mohit Iyyer, Pasquale Minervini, Victor Sanh, Thomas Wolf, Rowan Zellers'\nauthor = 'Mohit Iyyer, Pasquale Minervini, Victor Sanh, Thomas Wolf, Rowan Zellers'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n",
  "slicing": [
   "author = 'Mohit Iyyer, Pasquale Minervini, Victor Sanh, Thomas Wolf, Rowan Zellers'\n",
   "     [author], 1)\n",
   "     author, 'AdversarialNLP', 'One line description of project.',\n"
  ]
 },
 "13": {
  "name": "version",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "63",
  "column": "0",
  "context": "the\n# built documents.\n#\n# The short X.Y version.\nversion = '0.1'\n# The full version, including alpha/beta/rc tags.\n",
  "context_lines": "# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = '0.1'\n# The full version, including alpha/beta/rc tags.\nrelease = '0.1'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n",
  "slicing": "version = '0.1'\n"
 },
 "14": {
  "name": "release",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "65",
  "column": "0",
  "context": "# The full version, including alpha/beta/rc tags.\nrelease = '0.1'\n\n# The language for content autogenerated by Sphin",
  "context_lines": "#\n# The short X.Y version.\nversion = '0.1'\n# The full version, including alpha/beta/rc tags.\nrelease = '0.1'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n",
  "slicing": "release = '0.1'\n"
 },
 "15": {
  "name": "language",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "72",
  "column": "0",
  "context": "\"language\" from the command line for these cases.\nlanguage = 'en'\n\n# List of patterns, relative to source directory,",
  "context_lines": "# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = 'en'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\n",
  "slicing": "language = 'en'\n"
 },
 "16": {
  "name": "exclude_patterns",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "77",
  "column": "0",
  "context": "so effect to html_static_path and html_extra_path\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n# The name of the Pygments (syntax highlighting) ",
  "context_lines": "language = 'en'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\n",
  "slicing": "exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n"
 },
 "17": {
  "name": "pygments_style",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "80",
  "column": "0",
  "context": " the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# If true, `todo` and `todoList` produce output, ",
  "context_lines": "# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n",
  "slicing": "pygments_style = 'sphinx'\n"
 },
 "18": {
  "name": "todo_include_todos",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "83",
  "column": "0",
  "context": "oList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output -------------------",
  "context_lines": "exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n",
  "slicing": "todo_include_todos = False\n"
 },
 "19": {
  "name": "html_theme",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "91",
  "column": "0",
  "context": "e documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'alabaster'\n\n# Theme options are theme-specific and customize ",
  "context_lines": "# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'alabaster'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n",
  "slicing": "html_theme = 'alabaster'\n"
 },
 "20": {
  "name": "html_static_path",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "102",
  "column": "0",
  "context": "lt.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n\n# -- Options for HTMLHelp output ---------------",
  "context_lines": "# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'AdversarialNLPdoc'\n\n\n",
  "slicing": "html_static_path = ['_static']\n"
 },
 "21": {
  "name": "htmlhelp_basename",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "108",
  "column": "0",
  "context": "-\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'AdversarialNLPdoc'\n\n\n# -- Options for LaTeX output ------------------",
  "context_lines": "# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'AdversarialNLPdoc'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n",
  "slicing": "htmlhelp_basename = 'AdversarialNLPdoc'\n"
 },
 "22": {
  "name": "latex_elements",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "113",
  "column": "0",
  "context": "ut ---------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').",
  "context_lines": "# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'AdversarialNLPdoc'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n\n    # The font size ('10pt', '11pt' or '12pt').\n",
  "slicing": "latex_elements = {\n"
 },
 "23": {
  "name": "latex_documents",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "134",
  "column": "0",
  "context": "or, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, 'AdversarialNLP.tex', 'Adversaria",
  "context_lines": "}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, 'AdversarialNLP.tex', 'AdversarialNLP Documentation',\n     'Mohit Iyyer, Pasquale Minervini, Victor Sanh, Thomas Wolf, Rowan Zellers', 'manual'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n",
  "slicing": "latex_documents = [\n"
 },
 "24": {
  "name": "man_pages",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "144",
  "column": "0",
  "context": "ile, name, description, authors, manual section).\nman_pages = [\n    (master_doc, 'adversarialnlp', 'AdversarialNLP",
  "context_lines": "]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, 'adversarialnlp', 'AdversarialNLP Documentation',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n",
  "slicing": "man_pages = [\n"
 },
 "25": {
  "name": "texinfo_documents",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "155",
  "column": "0",
  "context": "author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, 'AdversarialNLP', 'AdversarialNLP",
  "context_lines": "# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, 'AdversarialNLP', 'AdversarialNLP Documentation',\n     author, 'AdversarialNLP', 'One line description of project.',\n     'Miscellaneous'),\n]\n\n\n\n\n",
  "slicing": "texinfo_documents = [\n"
 },
 "26": {
  "name": "intersphinx_mapping",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/docs/conf.py",
  "lineno": "165",
  "column": "0",
  "context": "ntersphinx: refer to the Python standard library.\nintersphinx_mapping = {'https://docs.python.org/': None}\n",
  "context_lines": "     author, 'AdversarialNLP', 'One line description of project.',\n     'Miscellaneous'),\n]\n\n\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {'https://docs.python.org/': None}\n",
  "slicing": "intersphinx_mapping = {'https://docs.python.org/': None}\n"
 },
 "27": {
  "name": "adversarial",
  "type": "adversarialnlp.Adversarial",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/tutorials/usage.py",
  "lineno": "4",
  "column": "0",
  "context": "s.reading_comprehension.squad import SquadReader\n\nadversarial = Adversarial(dataset_reader=SquadReader, editor='lstm_lm', num_samples=10)\nexamples = adversarial.generate()",
  "context_lines": "from adversarialnlp import Adversarial\nfrom allennlp.data.dataset_readers.reading_comprehension.squad import SquadReader\n\nadversarial = Adversarial(dataset_reader=SquadReader, editor='lstm_lm', num_samples=10)\n",
  "slicing": [
   "adversarial = Adversarial(dataset_reader=SquadReader, editor='lstm_lm', num_samples=10)\n",
   "examples = adversarial.generate()\n"
  ]
 },
 "28": {
  "name": "examples",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/tutorials/usage.py",
  "lineno": "5",
  "column": "0",
  "context": "er=SquadReader, editor='lstm_lm', num_samples=10)\nexamples = adversarial.generate()",
  "context_lines": "from adversarialnlp import Adversarial\nfrom allennlp.data.dataset_readers.reading_comprehension.squad import SquadReader\n\nadversarial = Adversarial(dataset_reader=SquadReader, editor='lstm_lm', num_samples=10)\nexamples = adversarial.generate()",
  "slicing": [
   "adversarial = Adversarial(dataset_reader=SquadReader, editor='lstm_lm', num_samples=10)\n",
   "examples = adversarial.generate()\n"
  ]
 },
 "29": {
  "name": "LEVEL",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/run.py",
  "lineno": "7",
  "column": "4",
  "context": "ort sys\n\nif os.environ.get(\"ALLENNLP_DEBUG\"):\n    LEVEL = logging.DEBUG\nelse:\n    LEVEL = logging.INFO\n\nsys.path.insert(0,",
  "context_lines": "import logging\nimport os\nimport sys\n\nif os.environ.get(\"ALLENNLP_DEBUG\"):\n    LEVEL = logging.DEBUG\nelse:\n    LEVEL = logging.INFO\n\nsys.path.insert(0, os.path.dirname(os.path.abspath(os.path.join(__file__, os.pardir))))\nlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
  "slicing": [
   "    LEVEL = logging.DEBUG\n",
   "                    level=LEVEL)\n"
  ]
 },
 "30": {
  "name": "LEVEL",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/run.py",
  "lineno": "9",
  "column": "4",
  "context": "NNLP_DEBUG\"):\n    LEVEL = logging.DEBUG\nelse:\n    LEVEL = logging.INFO\n\nsys.path.insert(0, os.path.dirname(os.path.abspat",
  "context_lines": "import sys\n\nif os.environ.get(\"ALLENNLP_DEBUG\"):\n    LEVEL = logging.DEBUG\nelse:\n    LEVEL = logging.INFO\n\nsys.path.insert(0, os.path.dirname(os.path.abspath(os.path.join(__file__, os.pardir))))\nlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n                    level=LEVEL)\n\n",
  "slicing": [
   "    LEVEL = logging.INFO\n",
   "                    level=LEVEL)\n"
  ]
 },
 "31": {
  "name": "_MAJOR",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/version.py",
  "lineno": "1",
  "column": "0",
  "context": "_MAJOR = \"0\"\n_MINOR = \"1\"\n_REVISION = \"1-unreleased\"\n\nVERSION_S",
  "context_lines": "_MAJOR = \"0\"\n_MINOR = \"1\"\n_REVISION = \"1-unreleased\"\n\nVERSION_SHORT = \"{0}.{1}\".format(_MAJOR, _MINOR)\n",
  "slicing": [
   "_MAJOR = \"0\"\n",
   "_MINOR = \"1\"\n",
   "_REVISION = \"1-unreleased\"\n",
   "VERSION_SHORT = \"{0}.{1}\".format(_MAJOR, _MINOR)\n",
   "VERSION = \"{0}.{1}.{2}\".format(_MAJOR, _MINOR, _REVISION)\n"
  ]
 },
 "32": {
  "name": "_MINOR",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/version.py",
  "lineno": "2",
  "column": "0",
  "context": "_MAJOR = \"0\"\n_MINOR = \"1\"\n_REVISION = \"1-unreleased\"\n\nVERSION_SHORT = \"{0}.{",
  "context_lines": "_MAJOR = \"0\"\n_MINOR = \"1\"\n_REVISION = \"1-unreleased\"\n\nVERSION_SHORT = \"{0}.{1}\".format(_MAJOR, _MINOR)\n",
  "slicing": [
   "_MAJOR = \"0\"\n",
   "_MINOR = \"1\"\n",
   "_REVISION = \"1-unreleased\"\n",
   "VERSION_SHORT = \"{0}.{1}\".format(_MAJOR, _MINOR)\n",
   "VERSION = \"{0}.{1}.{2}\".format(_MAJOR, _MINOR, _REVISION)\n"
  ]
 },
 "33": {
  "name": "_REVISION",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/version.py",
  "lineno": "3",
  "column": "0",
  "context": "_MAJOR = \"0\"\n_MINOR = \"1\"\n_REVISION = \"1-unreleased\"\n\nVERSION_SHORT = \"{0}.{1}\".format(_MAJOR, _MINOR)\n",
  "context_lines": "_MAJOR = \"0\"\n_MINOR = \"1\"\n_REVISION = \"1-unreleased\"\n\nVERSION_SHORT = \"{0}.{1}\".format(_MAJOR, _MINOR)\n",
  "slicing": [
   "_MAJOR = \"0\"\n",
   "_MINOR = \"1\"\n",
   "_REVISION = \"1-unreleased\"\n",
   "VERSION_SHORT = \"{0}.{1}\".format(_MAJOR, _MINOR)\n",
   "VERSION = \"{0}.{1}.{2}\".format(_MAJOR, _MINOR, _REVISION)\n"
  ]
 },
 "34": {
  "name": "VERSION_SHORT",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/version.py",
  "lineno": "5",
  "column": "0",
  "context": "OR = \"0\"\n_MINOR = \"1\"\n_REVISION = \"1-unreleased\"\n\nVERSION_SHORT = \"{0}.{1}\".format(_MAJOR, _MINOR)\nVERSION = \"{0}.{1}.{2}\".format(_MAJOR, _MINOR, _RE",
  "context_lines": "_MAJOR = \"0\"\n_MINOR = \"1\"\n_REVISION = \"1-unreleased\"\n\nVERSION_SHORT = \"{0}.{1}\".format(_MAJOR, _MINOR)\n",
  "slicing": [
   "_MAJOR = \"0\"\n",
   "_MINOR = \"1\"\n",
   "_REVISION = \"1-unreleased\"\n",
   "VERSION_SHORT = \"{0}.{1}\".format(_MAJOR, _MINOR)\n",
   "VERSION = \"{0}.{1}.{2}\".format(_MAJOR, _MINOR, _REVISION)\n"
  ]
 },
 "35": {
  "name": "VERSION",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/version.py",
  "lineno": "6",
  "column": "0",
  "context": "\nVERSION_SHORT = \"{0}.{1}\".format(_MAJOR, _MINOR)\nVERSION = \"{0}.{1}.{2}\".format(_MAJOR, _MINOR, _REVISION)\n",
  "context_lines": "_MAJOR = \"0\"\n_MINOR = \"1\"\n_REVISION = \"1-unreleased\"\n\nVERSION_SHORT = \"{0}.{1}\".format(_MAJOR, _MINOR)\nVERSION = \"{0}.{1}.{2}\".format(_MAJOR, _MINOR, _REVISION)\n",
  "slicing": [
   "_MAJOR = \"0\"\n",
   "_MINOR = \"1\"\n",
   "_REVISION = \"1-unreleased\"\n",
   "VERSION_SHORT = \"{0}.{1}\".format(_MAJOR, _MINOR)\n",
   "VERSION = \"{0}.{1}.{2}\".format(_MAJOR, _MINOR, _REVISION)\n"
  ]
 },
 "36": {
  "name": "reader",
  "type": "adversarialnlp.dataset_readers.ActivityNetCaptionsDatasetReader",
  "class": "customized",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/dataset_readers/activitynet_captions_test.py",
  "lineno": "12",
  "column": "8",
  "context": "\n    def test_read_from_file(self, lazy):\n        reader = ActivityNetCaptionsDatasetReader(lazy=lazy)\n        instances = reader.read(FIXTURES_ROOT / 'a",
  "context_lines": "from adversarialnlp.tests.utils import FIXTURES_ROOT\n\nclass TestActivityNetCaptionsReader():\n    @pytest.mark.parametrize(\"lazy\", (True, False))\n    def test_read_from_file(self, lazy):\n        reader = ActivityNetCaptionsDatasetReader(lazy=lazy)\n        instances = reader.read(FIXTURES_ROOT / 'activitynet_captions.json')\n        instances = ensure_list(instances)\n\n        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n                     \"first_sentence\": \"A weight lifting tutorial is given .\".split(),\n",
  "slicing": [
   "        reader = ActivityNetCaptionsDatasetReader(lazy=lazy)\n",
   "        instances = reader.read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "        instances = ensure_list(instances)\n",
   "        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n",
   "        instance2 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        instance3 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        assert len(instances) == 3\n",
   "        for instance, expected_instance in zip(instances, [instance1, instance2, instance3]):\n",
   "            fields = instance.fields\n",
   "            assert [t.text for t in fields[\"first_sentence\"].tokens] == expected_instance[\"first_sentence\"]\n",
   "            assert [t.text for t in fields[\"second_sentence\"].tokens] == expected_instance[\"second_sentence\"]\n",
   "            assert fields[\"video_id\"].metadata == expected_instance[\"video_id\"]\n"
  ]
 },
 "37": {
  "name": "instances",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/tests/dataset_readers/activitynet_captions_test.py",
  "lineno": "13",
  "column": "8",
  "context": "tivityNetCaptionsDatasetReader(lazy=lazy)\n        instances = reader.read(FIXTURES_ROOT / 'activitynet_captions.json')\n        instances = ensure_list(instances)\n\n      ",
  "context_lines": "class TestActivityNetCaptionsReader():\n    @pytest.mark.parametrize(\"lazy\", (True, False))\n    def test_read_from_file(self, lazy):\n        reader = ActivityNetCaptionsDatasetReader(lazy=lazy)\n        instances = reader.read(FIXTURES_ROOT / 'activitynet_captions.json')\n        instances = ensure_list(instances)\n\n        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n                     \"first_sentence\": \"A weight lifting tutorial is given .\".split(),\n                     \"second_sentence\": \"The coach helps the guy in red with the proper body placement and lifting technique .\".split()}\n\n",
  "slicing": [
   "        reader = ActivityNetCaptionsDatasetReader(lazy=lazy)\n",
   "        instances = reader.read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "        instances = ensure_list(instances)\n",
   "        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n",
   "        instance2 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        instance3 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        assert len(instances) == 3\n",
   "        for instance, expected_instance in zip(instances, [instance1, instance2, instance3]):\n",
   "            fields = instance.fields\n",
   "            assert [t.text for t in fields[\"first_sentence\"].tokens] == expected_instance[\"first_sentence\"]\n",
   "            assert [t.text for t in fields[\"second_sentence\"].tokens] == expected_instance[\"second_sentence\"]\n",
   "            assert fields[\"video_id\"].metadata == expected_instance[\"video_id\"]\n"
  ]
 },
 "38": {
  "name": "instances",
  "type": "allennlp.common.util.ensure_list",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/dataset_readers/activitynet_captions_test.py",
  "lineno": "14",
  "column": "8",
  "context": "TURES_ROOT / 'activitynet_captions.json')\n        instances = ensure_list(instances)\n\n        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",",
  "context_lines": "    @pytest.mark.parametrize(\"lazy\", (True, False))\n    def test_read_from_file(self, lazy):\n        reader = ActivityNetCaptionsDatasetReader(lazy=lazy)\n        instances = reader.read(FIXTURES_ROOT / 'activitynet_captions.json')\n        instances = ensure_list(instances)\n\n        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n                     \"first_sentence\": \"A weight lifting tutorial is given .\".split(),\n                     \"second_sentence\": \"The coach helps the guy in red with the proper body placement and lifting technique .\".split()}\n\n",
  "slicing": [
   "        reader = ActivityNetCaptionsDatasetReader(lazy=lazy)\n",
   "        instances = reader.read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "        instances = ensure_list(instances)\n",
   "        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n",
   "        instance2 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        instance3 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        assert len(instances) == 3\n",
   "        for instance, expected_instance in zip(instances, [instance1, instance2, instance3]):\n",
   "            fields = instance.fields\n",
   "            assert [t.text for t in fields[\"first_sentence\"].tokens] == expected_instance[\"first_sentence\"]\n",
   "            assert [t.text for t in fields[\"second_sentence\"].tokens] == expected_instance[\"second_sentence\"]\n",
   "            assert fields[\"video_id\"].metadata == expected_instance[\"video_id\"]\n"
  ]
 },
 "39": {
  "name": "instance1",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/dataset_readers/activitynet_captions_test.py",
  "lineno": "16",
  "column": "8",
  "context": "      instances = ensure_list(instances)\n\n        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n                     \"first_sentence\": \"A weight l",
  "context_lines": "    def test_read_from_file(self, lazy):\n        reader = ActivityNetCaptionsDatasetReader(lazy=lazy)\n        instances = reader.read(FIXTURES_ROOT / 'activitynet_captions.json')\n        instances = ensure_list(instances)\n\n        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n                     \"first_sentence\": \"A weight lifting tutorial is given .\".split(),\n                     \"second_sentence\": \"The coach helps the guy in red with the proper body placement and lifting technique .\".split()}\n\n        instance2 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n                     \"first_sentence\": \"A man is seen speaking to the camera and pans out into more men standing behind him .\".split(),\n",
  "slicing": [
   "        reader = ActivityNetCaptionsDatasetReader(lazy=lazy)\n",
   "        instances = reader.read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "        instances = ensure_list(instances)\n",
   "        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n",
   "        instance2 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        instance3 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        assert len(instances) == 3\n",
   "        for instance, expected_instance in zip(instances, [instance1, instance2, instance3]):\n",
   "            fields = instance.fields\n",
   "            assert [t.text for t in fields[\"first_sentence\"].tokens] == expected_instance[\"first_sentence\"]\n",
   "            assert [t.text for t in fields[\"second_sentence\"].tokens] == expected_instance[\"second_sentence\"]\n",
   "            assert fields[\"video_id\"].metadata == expected_instance[\"video_id\"]\n"
  ]
 },
 "40": {
  "name": "instance2",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/dataset_readers/activitynet_captions_test.py",
  "lineno": "20",
  "column": "8",
  "context": "cement and lifting technique .\".split()}\n\n        instance2 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n                     \"first_sentence\": \"A man is s",
  "context_lines": "        instances = ensure_list(instances)\n\n        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n                     \"first_sentence\": \"A weight lifting tutorial is given .\".split(),\n                     \"second_sentence\": \"The coach helps the guy in red with the proper body placement and lifting technique .\".split()}\n\n        instance2 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n                     \"first_sentence\": \"A man is seen speaking to the camera and pans out into more men standing behind him .\".split(),\n                     \"second_sentence\": \"The first man then begins performing martial arts moves while speaking to he camera .\".split()}\n\n        instance3 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n                     \"first_sentence\": \"The first man then begins performing martial arts moves while speaking to he camera .\".split(),\n",
  "slicing": [
   "        reader = ActivityNetCaptionsDatasetReader(lazy=lazy)\n",
   "        instances = reader.read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "        instances = ensure_list(instances)\n",
   "        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n",
   "        instance2 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        instance3 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        assert len(instances) == 3\n",
   "        for instance, expected_instance in zip(instances, [instance1, instance2, instance3]):\n",
   "            fields = instance.fields\n",
   "            assert [t.text for t in fields[\"first_sentence\"].tokens] == expected_instance[\"first_sentence\"]\n",
   "            assert [t.text for t in fields[\"second_sentence\"].tokens] == expected_instance[\"second_sentence\"]\n",
   "            assert fields[\"video_id\"].metadata == expected_instance[\"video_id\"]\n"
  ]
 },
 "41": {
  "name": "instance3",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/dataset_readers/activitynet_captions_test.py",
  "lineno": "24",
  "column": "8",
  "context": " while speaking to he camera .\".split()}\n\n        instance3 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n                     \"first_sentence\": \"The first ",
  "context_lines": "                     \"second_sentence\": \"The coach helps the guy in red with the proper body placement and lifting technique .\".split()}\n\n        instance2 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n                     \"first_sentence\": \"A man is seen speaking to the camera and pans out into more men standing behind him .\".split(),\n                     \"second_sentence\": \"The first man then begins performing martial arts moves while speaking to he camera .\".split()}\n\n        instance3 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n                     \"first_sentence\": \"The first man then begins performing martial arts moves while speaking to he camera .\".split(),\n                     \"second_sentence\": \"He continues moving around and looking to the camera .\".split()}\n\n        assert len(instances) == 3\n\n        for instance, expected_instance in zip(instances, [instance1, instance2, instance3]):\n",
  "slicing": [
   "        reader = ActivityNetCaptionsDatasetReader(lazy=lazy)\n",
   "        instances = reader.read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "        instances = ensure_list(instances)\n",
   "        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n",
   "        instance2 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        instance3 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        assert len(instances) == 3\n",
   "        for instance, expected_instance in zip(instances, [instance1, instance2, instance3]):\n",
   "            fields = instance.fields\n",
   "            assert [t.text for t in fields[\"first_sentence\"].tokens] == expected_instance[\"first_sentence\"]\n",
   "            assert [t.text for t in fields[\"second_sentence\"].tokens] == expected_instance[\"second_sentence\"]\n",
   "            assert fields[\"video_id\"].metadata == expected_instance[\"video_id\"]\n"
  ]
 },
 "42": {
  "name": "fields",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/tests/dataset_readers/activitynet_captions_test.py",
  "lineno": "31",
  "column": "12",
  "context": ", [instance1, instance2, instance3]):\n            fields = instance.fields\n            assert [t.text for t in fields[\"first_",
  "context_lines": "                     \"first_sentence\": \"The first man then begins performing martial arts moves while speaking to he camera .\".split(),\n                     \"second_sentence\": \"He continues moving around and looking to the camera .\".split()}\n\n        assert len(instances) == 3\n\n        for instance, expected_instance in zip(instances, [instance1, instance2, instance3]):\n            fields = instance.fields\n            assert [t.text for t in fields[\"first_sentence\"].tokens] == expected_instance[\"first_sentence\"]\n            assert [t.text for t in fields[\"second_sentence\"].tokens] == expected_instance[\"second_sentence\"]\n",
  "slicing": [
   "        reader = ActivityNetCaptionsDatasetReader(lazy=lazy)\n",
   "        instances = reader.read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "        instances = ensure_list(instances)\n",
   "        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n",
   "        instance2 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        instance3 = {\"video_id\": \"v_bXdq2zI1Ms0\",\n",
   "        assert len(instances) == 3\n",
   "        for instance, expected_instance in zip(instances, [instance1, instance2, instance3]):\n",
   "            fields = instance.fields\n",
   "            assert [t.text for t in fields[\"first_sentence\"].tokens] == expected_instance[\"first_sentence\"]\n",
   "            assert [t.text for t in fields[\"second_sentence\"].tokens] == expected_instance[\"second_sentence\"]\n",
   "            assert fields[\"video_id\"].metadata == expected_instance[\"video_id\"]\n"
  ]
 },
 "43": {
  "name": "generator",
  "type": "adversarialnlp.generators.addsent.addsent_generator.AddSentGenerator",
  "class": "customized",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/addsent_generator_test.py",
  "lineno": "56",
  "column": "8",
  "context": "poch_generation_over_the_data_once(self):\n        generator = AddSentGenerator()\n        test_instances = squad_reader(FIXTURES_ROO",
  "context_lines": "class TestSwagGenerator():\n    # The Generator should work the same for lazy and non lazy datasets,\n    # so each remaining test runs over both.\n    def test_yield_one_epoch_generation_over_the_data_once(self):\n        generator = AddSentGenerator()\n        test_instances = squad_reader(FIXTURES_ROOT / 'squad.json')\n        batches = list(generator(test_instances, num_epochs=1))\n        # We just want to get the single-token array for the text field in the instance.\n        # instances = [tuple(instance.detach().cpu().numpy())\n",
  "slicing": [
   "        generator = AddSentGenerator()\n",
   "        test_instances = squad_reader(FIXTURES_ROOT / 'squad.json')\n",
   "        batches = list(generator(test_instances, num_epochs=1))\n",
   "        assert len(batches) == 5\n"
  ]
 },
 "44": {
  "name": "test_instances",
  "type": "adversarialnlp.generators.addsent.squad_reader.squad_reader",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/addsent_generator_test.py",
  "lineno": "57",
  "column": "8",
  "context": "):\n        generator = AddSentGenerator()\n        test_instances = squad_reader(FIXTURES_ROOT / 'squad.json')\n        batches = list(generator(test_instances, n",
  "context_lines": "    # The Generator should work the same for lazy and non lazy datasets,\n    # so each remaining test runs over both.\n    def test_yield_one_epoch_generation_over_the_data_once(self):\n        generator = AddSentGenerator()\n        test_instances = squad_reader(FIXTURES_ROOT / 'squad.json')\n        batches = list(generator(test_instances, num_epochs=1))\n        # We just want to get the single-token array for the text field in the instance.\n        # instances = [tuple(instance.detach().cpu().numpy())\n        #                 for batch in batches\n",
  "slicing": [
   "        generator = AddSentGenerator()\n",
   "        test_instances = squad_reader(FIXTURES_ROOT / 'squad.json')\n",
   "        batches = list(generator(test_instances, num_epochs=1))\n",
   "        assert len(batches) == 5\n"
  ]
 },
 "45": {
  "name": "batches",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/addsent_generator_test.py",
  "lineno": "58",
  "column": "8",
  "context": "quad_reader(FIXTURES_ROOT / 'squad.json')\n        batches = list(generator(test_instances, num_epochs=1))\n        # We just want to get the single-token arr",
  "context_lines": "    # so each remaining test runs over both.\n    def test_yield_one_epoch_generation_over_the_data_once(self):\n        generator = AddSentGenerator()\n        test_instances = squad_reader(FIXTURES_ROOT / 'squad.json')\n        batches = list(generator(test_instances, num_epochs=1))\n        # We just want to get the single-token array for the text field in the instance.\n        # instances = [tuple(instance.detach().cpu().numpy())\n        #                 for batch in batches\n        #                 for instance in batch['text'][\"tokens\"]]\n",
  "slicing": [
   "        generator = AddSentGenerator()\n",
   "        test_instances = squad_reader(FIXTURES_ROOT / 'squad.json')\n",
   "        batches = list(generator(test_instances, num_epochs=1))\n",
   "        assert len(batches) == 5\n"
  ]
 },
 "46": {
  "name": "instances",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/swag_generator_test.py",
  "lineno": "31",
  "column": "8",
  "context": "self.vocab.add_token_to_namespace('long')\n        instances = [\n                self.create_instance([\"this\", \"is\"",
  "context_lines": "        self.another_index = self.vocab.add_token_to_namespace('another')\n        self.yet_index = self.vocab.add_token_to_namespace('yet')\n        self.very_index = self.vocab.add_token_to_namespace('very')\n        self.long_index = self.vocab.add_token_to_namespace('long')\n        instances = [\n                self.create_instance([\"this\", \"is\", \"a\", \"sentence\"], [\"this\", \"is\", \"another\", \"sentence\"]),\n                self.create_instance([\"yet\", \"another\", \"sentence\"],\n                                     [\"this\", \"is\", \"a\", \"very\", \"very\", \"very\", \"very\", \"long\", \"sentence\"]),\n                ]\n\n",
  "slicing": [
   "        instances = [\n",
   "                return (instance for instance in instances)\n",
   "        self.instances = instances\n",
   "        first_tokens = [Token(t) for t in first_sentence]\n",
   "        second_tokens = [Token(t) for t in second_sentence]\n",
   "        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n",
   "                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n",
   "        return instance\n",
   "        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n",
   "        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n",
   "                              for instance in self.instances]\n",
   "        assert set(candidate_instances) == set(expected_instances)\n",
   "        for test_instances in (self.instances, self.lazy_instances):\n",
   "            generator = SwagGenerator(num_examples=1)\n",
   "            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "            batches = list(generator(test_instances))\n",
   "            instances = [tuple(instance.detach().cpu().numpy())\n",
   "                         for batch in batches\n",
   "                         for instance in batch['text'][\"tokens\"]]\n",
   "            assert len(instances) == 5\n",
   "            self.assert_instances_are_correct(instances)\n"
  ]
 },
 "47": {
  "name": "first_tokens",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/swag_generator_test.py",
  "lineno": "45",
  "column": "8",
  "context": ": List[str], second_sentence: List[str]):\n        first_tokens = [Token(t) for t in first_sentence]\n        second_tokens = [Token(t) for t in second_",
  "context_lines": "                return (instance for instance in instances)\n\n        self.instances = instances\n        self.lazy_instances = LazyIterable()\n\n    def create_instance(self, first_sentence: List[str], second_sentence: List[str]):\n        first_tokens = [Token(t) for t in first_sentence]\n        second_tokens = [Token(t) for t in second_sentence]\n        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n        return instance\n\n",
  "slicing": [
   "        instances = [\n",
   "                return (instance for instance in instances)\n",
   "        self.instances = instances\n",
   "        first_tokens = [Token(t) for t in first_sentence]\n",
   "        second_tokens = [Token(t) for t in second_sentence]\n",
   "        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n",
   "                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n",
   "        return instance\n",
   "        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n",
   "        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n",
   "                              for instance in self.instances]\n",
   "        assert set(candidate_instances) == set(expected_instances)\n",
   "        for test_instances in (self.instances, self.lazy_instances):\n",
   "            generator = SwagGenerator(num_examples=1)\n",
   "            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "            batches = list(generator(test_instances))\n",
   "            instances = [tuple(instance.detach().cpu().numpy())\n",
   "                         for batch in batches\n",
   "                         for instance in batch['text'][\"tokens\"]]\n",
   "            assert len(instances) == 5\n",
   "            self.assert_instances_are_correct(instances)\n"
  ]
 },
 "48": {
  "name": "second_tokens",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/swag_generator_test.py",
  "lineno": "46",
  "column": "8",
  "context": "kens = [Token(t) for t in first_sentence]\n        second_tokens = [Token(t) for t in second_sentence]\n        instance = Instance({'first_sentence': Tex",
  "context_lines": "        self.instances = instances\n        self.lazy_instances = LazyIterable()\n\n    def create_instance(self, first_sentence: List[str], second_sentence: List[str]):\n        first_tokens = [Token(t) for t in first_sentence]\n        second_tokens = [Token(t) for t in second_sentence]\n        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n        return instance\n\n    def assert_instances_are_correct(self, candidate_instances):\n",
  "slicing": [
   "        instances = [\n",
   "                return (instance for instance in instances)\n",
   "        self.instances = instances\n",
   "        first_tokens = [Token(t) for t in first_sentence]\n",
   "        second_tokens = [Token(t) for t in second_sentence]\n",
   "        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n",
   "                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n",
   "        return instance\n",
   "        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n",
   "        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n",
   "                              for instance in self.instances]\n",
   "        assert set(candidate_instances) == set(expected_instances)\n",
   "        for test_instances in (self.instances, self.lazy_instances):\n",
   "            generator = SwagGenerator(num_examples=1)\n",
   "            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "            batches = list(generator(test_instances))\n",
   "            instances = [tuple(instance.detach().cpu().numpy())\n",
   "                         for batch in batches\n",
   "                         for instance in batch['text'][\"tokens\"]]\n",
   "            assert len(instances) == 5\n",
   "            self.assert_instances_are_correct(instances)\n"
  ]
 },
 "49": {
  "name": "instance",
  "type": "allennlp.data.Instance",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/swag_generator_test.py",
  "lineno": "47",
  "column": "8",
  "context": "ens = [Token(t) for t in second_sentence]\n        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n                             'second_sentence': Te",
  "context_lines": "        self.lazy_instances = LazyIterable()\n\n    def create_instance(self, first_sentence: List[str], second_sentence: List[str]):\n        first_tokens = [Token(t) for t in first_sentence]\n        second_tokens = [Token(t) for t in second_sentence]\n        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n        return instance\n\n    def assert_instances_are_correct(self, candidate_instances):\n        # First we need to remove padding tokens from the candidates.\n",
  "slicing": [
   "        instances = [\n",
   "                return (instance for instance in instances)\n",
   "        self.instances = instances\n",
   "        first_tokens = [Token(t) for t in first_sentence]\n",
   "        second_tokens = [Token(t) for t in second_sentence]\n",
   "        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n",
   "                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n",
   "        return instance\n",
   "        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n",
   "        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n",
   "                              for instance in self.instances]\n",
   "        assert set(candidate_instances) == set(expected_instances)\n",
   "        for test_instances in (self.instances, self.lazy_instances):\n",
   "            generator = SwagGenerator(num_examples=1)\n",
   "            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "            batches = list(generator(test_instances))\n",
   "            instances = [tuple(instance.detach().cpu().numpy())\n",
   "                         for batch in batches\n",
   "                         for instance in batch['text'][\"tokens\"]]\n",
   "            assert len(instances) == 5\n",
   "            self.assert_instances_are_correct(instances)\n"
  ]
 },
 "50": {
  "name": "candidate_instances",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/swag_generator_test.py",
  "lineno": "54",
  "column": "8",
  "context": "       # pylint: disable=protected-access\n        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n        expected_instances = [tuple(instance.field",
  "context_lines": "        return instance\n\n    def assert_instances_are_correct(self, candidate_instances):\n        # First we need to remove padding tokens from the candidates.\n        # pylint: disable=protected-access\n        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n                              for instance in self.instances]\n        assert set(candidate_instances) == set(expected_instances)\n\n\nclass TestSwagGenerator(GeneratorTest):\n",
  "slicing": [
   "        instances = [\n",
   "                return (instance for instance in instances)\n",
   "        self.instances = instances\n",
   "        first_tokens = [Token(t) for t in first_sentence]\n",
   "        second_tokens = [Token(t) for t in second_sentence]\n",
   "        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n",
   "                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n",
   "        return instance\n",
   "        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n",
   "        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n",
   "                              for instance in self.instances]\n",
   "        assert set(candidate_instances) == set(expected_instances)\n",
   "        for test_instances in (self.instances, self.lazy_instances):\n",
   "            generator = SwagGenerator(num_examples=1)\n",
   "            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "            batches = list(generator(test_instances))\n",
   "            instances = [tuple(instance.detach().cpu().numpy())\n",
   "                         for batch in batches\n",
   "                         for instance in batch['text'][\"tokens\"]]\n",
   "            assert len(instances) == 5\n",
   "            self.assert_instances_are_correct(instances)\n"
  ]
 },
 "51": {
  "name": "expected_instances",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/swag_generator_test.py",
  "lineno": "55",
  "column": "8",
  "context": "= 0) for instance in candidate_instances]\n        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n                              for instance in self",
  "context_lines": "    def assert_instances_are_correct(self, candidate_instances):\n        # First we need to remove padding tokens from the candidates.\n        # pylint: disable=protected-access\n        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n                              for instance in self.instances]\n        assert set(candidate_instances) == set(expected_instances)\n\n\nclass TestSwagGenerator(GeneratorTest):\n    # The Generator should work the same for lazy and non lazy datasets,\n",
  "slicing": [
   "        instances = [\n",
   "                return (instance for instance in instances)\n",
   "        self.instances = instances\n",
   "        first_tokens = [Token(t) for t in first_sentence]\n",
   "        second_tokens = [Token(t) for t in second_sentence]\n",
   "        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n",
   "                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n",
   "        return instance\n",
   "        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n",
   "        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n",
   "                              for instance in self.instances]\n",
   "        assert set(candidate_instances) == set(expected_instances)\n",
   "        for test_instances in (self.instances, self.lazy_instances):\n",
   "            generator = SwagGenerator(num_examples=1)\n",
   "            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "            batches = list(generator(test_instances))\n",
   "            instances = [tuple(instance.detach().cpu().numpy())\n",
   "                         for batch in batches\n",
   "                         for instance in batch['text'][\"tokens\"]]\n",
   "            assert len(instances) == 5\n",
   "            self.assert_instances_are_correct(instances)\n"
  ]
 },
 "52": {
  "name": "generator",
  "type": "adversarialnlp.generators.swag.swag_generator.SwagGenerator",
  "class": "customized",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/swag_generator_test.py",
  "lineno": "65",
  "column": "12",
  "context": "self.instances, self.lazy_instances):\n            generator = SwagGenerator(num_examples=1)\n            test_instances = ActivityNetCaptionsDa",
  "context_lines": "    # The Generator should work the same for lazy and non lazy datasets,\n    # so each remaining test runs over both.\n    def test_yield_one_epoch_generation_over_the_data_once(self):\n        for test_instances in (self.instances, self.lazy_instances):\n            generator = SwagGenerator(num_examples=1)\n            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n            batches = list(generator(test_instances))\n            # We just want to get the single-token array for the text field in the instance.\n            instances = [tuple(instance.detach().cpu().numpy())\n",
  "slicing": [
   "        instances = [\n",
   "                return (instance for instance in instances)\n",
   "        self.instances = instances\n",
   "        first_tokens = [Token(t) for t in first_sentence]\n",
   "        second_tokens = [Token(t) for t in second_sentence]\n",
   "        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n",
   "                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n",
   "        return instance\n",
   "        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n",
   "        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n",
   "                              for instance in self.instances]\n",
   "        assert set(candidate_instances) == set(expected_instances)\n",
   "        for test_instances in (self.instances, self.lazy_instances):\n",
   "            generator = SwagGenerator(num_examples=1)\n",
   "            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "            batches = list(generator(test_instances))\n",
   "            instances = [tuple(instance.detach().cpu().numpy())\n",
   "                         for batch in batches\n",
   "                         for instance in batch['text'][\"tokens\"]]\n",
   "            assert len(instances) == 5\n",
   "            self.assert_instances_are_correct(instances)\n"
  ]
 },
 "53": {
  "name": "test_instances",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/swag_generator_test.py",
  "lineno": "66",
  "column": "12",
  "context": "rator = SwagGenerator(num_examples=1)\n            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n            batches = list(generator(test_instance",
  "context_lines": "    # so each remaining test runs over both.\n    def test_yield_one_epoch_generation_over_the_data_once(self):\n        for test_instances in (self.instances, self.lazy_instances):\n            generator = SwagGenerator(num_examples=1)\n            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n            batches = list(generator(test_instances))\n            # We just want to get the single-token array for the text field in the instance.\n            instances = [tuple(instance.detach().cpu().numpy())\n                         for batch in batches\n",
  "slicing": [
   "        instances = [\n",
   "                return (instance for instance in instances)\n",
   "        self.instances = instances\n",
   "        first_tokens = [Token(t) for t in first_sentence]\n",
   "        second_tokens = [Token(t) for t in second_sentence]\n",
   "        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n",
   "                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n",
   "        return instance\n",
   "        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n",
   "        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n",
   "                              for instance in self.instances]\n",
   "        assert set(candidate_instances) == set(expected_instances)\n",
   "        for test_instances in (self.instances, self.lazy_instances):\n",
   "            generator = SwagGenerator(num_examples=1)\n",
   "            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "            batches = list(generator(test_instances))\n",
   "            instances = [tuple(instance.detach().cpu().numpy())\n",
   "                         for batch in batches\n",
   "                         for instance in batch['text'][\"tokens\"]]\n",
   "            assert len(instances) == 5\n",
   "            self.assert_instances_are_correct(instances)\n"
  ]
 },
 "54": {
  "name": "batches",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/swag_generator_test.py",
  "lineno": "67",
  "column": "12",
  "context": "S_ROOT / 'activitynet_captions.json')\n            batches = list(generator(test_instances))\n            # We just want to get the single-token",
  "context_lines": "    def test_yield_one_epoch_generation_over_the_data_once(self):\n        for test_instances in (self.instances, self.lazy_instances):\n            generator = SwagGenerator(num_examples=1)\n            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n            batches = list(generator(test_instances))\n            # We just want to get the single-token array for the text field in the instance.\n            instances = [tuple(instance.detach().cpu().numpy())\n                         for batch in batches\n                         for instance in batch['text'][\"tokens\"]]\n",
  "slicing": [
   "        instances = [\n",
   "                return (instance for instance in instances)\n",
   "        self.instances = instances\n",
   "        first_tokens = [Token(t) for t in first_sentence]\n",
   "        second_tokens = [Token(t) for t in second_sentence]\n",
   "        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n",
   "                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n",
   "        return instance\n",
   "        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n",
   "        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n",
   "                              for instance in self.instances]\n",
   "        assert set(candidate_instances) == set(expected_instances)\n",
   "        for test_instances in (self.instances, self.lazy_instances):\n",
   "            generator = SwagGenerator(num_examples=1)\n",
   "            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "            batches = list(generator(test_instances))\n",
   "            instances = [tuple(instance.detach().cpu().numpy())\n",
   "                         for batch in batches\n",
   "                         for instance in batch['text'][\"tokens\"]]\n",
   "            assert len(instances) == 5\n",
   "            self.assert_instances_are_correct(instances)\n"
  ]
 },
 "55": {
  "name": "instances",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/swag_generator_test.py",
  "lineno": "69",
  "column": "12",
  "context": "y for the text field in the instance.\n            instances = [tuple(instance.detach().cpu().numpy())\n                         for batch in batches\n    ",
  "context_lines": "            generator = SwagGenerator(num_examples=1)\n            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n            batches = list(generator(test_instances))\n            # We just want to get the single-token array for the text field in the instance.\n            instances = [tuple(instance.detach().cpu().numpy())\n                         for batch in batches\n                         for instance in batch['text'][\"tokens\"]]\n            assert len(instances) == 5\n",
  "slicing": [
   "        instances = [\n",
   "                return (instance for instance in instances)\n",
   "        self.instances = instances\n",
   "        first_tokens = [Token(t) for t in first_sentence]\n",
   "        second_tokens = [Token(t) for t in second_sentence]\n",
   "        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n",
   "                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n",
   "        return instance\n",
   "        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n",
   "        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n",
   "                              for instance in self.instances]\n",
   "        assert set(candidate_instances) == set(expected_instances)\n",
   "        for test_instances in (self.instances, self.lazy_instances):\n",
   "            generator = SwagGenerator(num_examples=1)\n",
   "            test_instances = ActivityNetCaptionsDatasetReader().read(FIXTURES_ROOT / 'activitynet_captions.json')\n",
   "            batches = list(generator(test_instances))\n",
   "            instances = [tuple(instance.detach().cpu().numpy())\n",
   "                         for batch in batches\n",
   "                         for instance in batch['text'][\"tokens\"]]\n",
   "            assert len(instances) == 5\n",
   "            self.assert_instances_are_correct(instances)\n"
  ]
 },
 "56": {
  "name": "logger",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/generator.py",
  "lineno": "6",
  "column": "0",
  "context": " collections import defaultdict\nimport itertools\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nclass Generator():\n    r\"\"\"An abstract ``Generato",
  "context_lines": "import logging\nfrom typing import Dict, Union, Iterable, List\nfrom collections import defaultdict\nimport itertools\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nclass Generator():\n    r\"\"\"An abstract ``Generator`` class.\n\n    A ``Generator`` takes as inputs an iterable of seeds (for examples\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n"
  ]
 },
 "57": {
  "name": "seeds",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/generator.py",
  "lineno": "85",
  "column": "16",
  "context": "f self.default_seeds is not None:\n                seeds = self.default_seeds\n            else:\n                return\n        #",
  "context_lines": "            from the seeds.\n        \"\"\"\n        if seeds is None:\n            if self.default_seeds is not None:\n                seeds = self.default_seeds\n            else:\n                return\n        # Instances is likely to be a list, which cannot be used as a key,\n        # so we take the object id instead.\n",
  "slicing": [
   "                seeds = self.default_seeds\n",
   "        key = id(seeds)\n",
   "        starting_epoch = self._epochs[key]\n",
   "            epochs: Iterable[int] = itertools.count(starting_epoch)\n",
   "            epochs = range(starting_epoch, starting_epoch + num_epochs)\n",
   "        for epoch in epochs:\n",
   "            self._epochs[key] = epoch\n",
   "            for seed in seeds:\n",
   "                yield from self.generate_from_seed(seed)\n"
  ]
 },
 "58": {
  "name": "key",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/generator.py",
  "lineno": "90",
  "column": "8",
  "context": "      # so we take the object id instead.\n        key = id(seeds)\n        starting_epoch = self._epochs[key]\n\n      ",
  "context_lines": "            else:\n                return\n        # Instances is likely to be a list, which cannot be used as a key,\n        # so we take the object id instead.\n        key = id(seeds)\n        starting_epoch = self._epochs[key]\n\n        if num_epochs is None:\n            epochs: Iterable[int] = itertools.count(starting_epoch)\n        else:\n",
  "slicing": [
   "                seeds = self.default_seeds\n",
   "        key = id(seeds)\n",
   "        starting_epoch = self._epochs[key]\n",
   "            epochs: Iterable[int] = itertools.count(starting_epoch)\n",
   "            epochs = range(starting_epoch, starting_epoch + num_epochs)\n",
   "        for epoch in epochs:\n",
   "            self._epochs[key] = epoch\n",
   "            for seed in seeds:\n",
   "                yield from self.generate_from_seed(seed)\n"
  ]
 },
 "59": {
  "name": "starting_epoch",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/generator.py",
  "lineno": "91",
  "column": "8",
  "context": "bject id instead.\n        key = id(seeds)\n        starting_epoch = self._epochs[key]\n\n        if num_epochs is None:\n            epochs",
  "context_lines": "                return\n        # Instances is likely to be a list, which cannot be used as a key,\n        # so we take the object id instead.\n        key = id(seeds)\n        starting_epoch = self._epochs[key]\n\n        if num_epochs is None:\n            epochs: Iterable[int] = itertools.count(starting_epoch)\n        else:\n",
  "slicing": [
   "                seeds = self.default_seeds\n",
   "        key = id(seeds)\n",
   "        starting_epoch = self._epochs[key]\n",
   "            epochs: Iterable[int] = itertools.count(starting_epoch)\n",
   "            epochs = range(starting_epoch, starting_epoch + num_epochs)\n",
   "        for epoch in epochs:\n",
   "            self._epochs[key] = epoch\n",
   "            for seed in seeds:\n",
   "                yield from self.generate_from_seed(seed)\n"
  ]
 },
 "60": {
  "name": "epochs",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/generator.py",
  "lineno": "94",
  "column": "12",
  "context": "[key]\n\n        if num_epochs is None:\n            epochs: Iterable[int] = itertools.count(starting_epoch)\n        else:\n            epochs = range(starting_",
  "context_lines": "        # so we take the object id instead.\n        key = id(seeds)\n        starting_epoch = self._epochs[key]\n\n        if num_epochs is None:\n            epochs: Iterable[int] = itertools.count(starting_epoch)\n        else:\n            epochs = range(starting_epoch, starting_epoch + num_epochs)\n\n        for epoch in epochs:\n            self._epochs[key] = epoch\n",
  "slicing": [
   "            epochs: Iterable[int] = itertools.count(starting_epoch)\n",
   "        for epoch in epochs:\n",
   "            self._epochs[key] = epoch\n"
  ]
 },
 "61": {
  "name": "epochs",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/generator.py",
  "lineno": "96",
  "column": "12",
  "context": "s.count(starting_epoch)\n        else:\n            epochs = range(starting_epoch, starting_epoch + num_epochs)\n\n        for epoch in epochs:\n            self._ep",
  "context_lines": "        starting_epoch = self._epochs[key]\n\n        if num_epochs is None:\n            epochs: Iterable[int] = itertools.count(starting_epoch)\n        else:\n            epochs = range(starting_epoch, starting_epoch + num_epochs)\n\n        for epoch in epochs:\n            self._epochs[key] = epoch\n            for seed in seeds:\n",
  "slicing": [
   "                seeds = self.default_seeds\n",
   "        key = id(seeds)\n",
   "        starting_epoch = self._epochs[key]\n",
   "            epochs: Iterable[int] = itertools.count(starting_epoch)\n",
   "            epochs = range(starting_epoch, starting_epoch + num_epochs)\n",
   "        for epoch in epochs:\n",
   "            self._epochs[key] = epoch\n",
   "            for seed in seeds:\n",
   "                yield from self.generate_from_seed(seed)\n"
  ]
 },
 "62": {
  "name": "logger",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/squad_reader.py",
  "lineno": "7",
  "column": "0",
  "context": "arialnlp.common.file_utils import download_files\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef squad_reader(file_path: str = None) -> Itera",
  "context_lines": "import json\nimport logging\nfrom typing import Iterator, List, Tuple\n\nfrom adversarialnlp.common.file_utils import download_files\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef squad_reader(file_path: str = None) -> Iterator[List[Tuple[str, str]]]:\n    r\"\"\" Reads a JSON-formatted SQuAD file and returns an Iterator.\n\n    Args:\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "    logger.info(\"Reading file at %s\", file_path)\n",
   "    logger.info(\"Reading the dataset\")\n"
  ]
 },
 "63": {
  "name": "file_path",
  "type": "adversarialnlp.common.file_utils.download_files",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/squad_reader.py",
  "lineno": "21",
  "column": "8",
  "context": "graph).\n    \"\"\"\n    if file_path is None:\n        file_path = download_files(fnames=['train-v1.1.json'],\n                                   paths='https://",
  "context_lines": "    Return:\n        list of tuple (question_answer, paragraph).\n    \"\"\"\n    if file_path is None:\n        file_path = download_files(fnames=['train-v1.1.json'],\n                                   paths='https://rajpurkar.github.io/SQuAD-explorer/dataset/',\n                                   local_folder='squad')\n        file_path = file_path[0]\n\n    logger.info(\"Reading file at %s\", file_path)\n",
  "slicing": [
   "        file_path = download_files(fnames=['train-v1.1.json'],\n",
   "        file_path = file_path[0]\n",
   "    logger.info(\"Reading file at %s\", file_path)\n",
   "    with open(file_path) as dataset_file:\n"
  ]
 },
 "64": {
  "name": "file_path",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/squad_reader.py",
  "lineno": "24",
  "column": "8",
  "context": "                    local_folder='squad')\n        file_path = file_path[0]\n\n    logger.info(\"Reading file at %s\", file_path)\n",
  "context_lines": "    if file_path is None:\n        file_path = download_files(fnames=['train-v1.1.json'],\n                                   paths='https://rajpurkar.github.io/SQuAD-explorer/dataset/',\n                                   local_folder='squad')\n        file_path = file_path[0]\n\n    logger.info(\"Reading file at %s\", file_path)\n    with open(file_path) as dataset_file:\n        dataset_json = json.load(dataset_file)\n",
  "slicing": [
   "        file_path = download_files(fnames=['train-v1.1.json'],\n",
   "        file_path = file_path[0]\n",
   "    logger.info(\"Reading file at %s\", file_path)\n",
   "    with open(file_path) as dataset_file:\n"
  ]
 },
 "65": {
  "name": "dataset_json",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/squad_reader.py",
  "lineno": "28",
  "column": "8",
  "context": "    with open(file_path) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        dataset = dataset_json['data']\n    logger.",
  "context_lines": "                                   local_folder='squad')\n        file_path = file_path[0]\n\n    logger.info(\"Reading file at %s\", file_path)\n    with open(file_path) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        dataset = dataset_json['data']\n    logger.info(\"Reading the dataset\")\n    out_data = []\n    for article in dataset:\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        file_path = download_files(fnames=['train-v1.1.json'],\n",
   "        file_path = file_path[0]\n",
   "    logger.info(\"Reading file at %s\", file_path)\n",
   "    with open(file_path) as dataset_file:\n",
   "        dataset_json = json.load(dataset_file)\n",
   "        dataset = dataset_json['data']\n",
   "    logger.info(\"Reading the dataset\")\n",
   "    for article in dataset:\n",
   "        for paragraph_json in article['paragraphs']:\n",
   "            paragraph = paragraph_json[\"context\"]\n",
   "            for question_answer in paragraph_json['qas']:\n",
   "                question_answer[\"question\"] = question_answer[\"question\"].strip().replace(\"\\n\", \"\")\n",
   "                out_data.append((question_answer, paragraph))\n"
  ]
 },
 "66": {
  "name": "dataset",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/squad_reader.py",
  "lineno": "29",
  "column": "8",
  "context": "   dataset_json = json.load(dataset_file)\n        dataset = dataset_json['data']\n    logger.info(\"Reading the dataset\")\n    out_dat",
  "context_lines": "        file_path = file_path[0]\n\n    logger.info(\"Reading file at %s\", file_path)\n    with open(file_path) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        dataset = dataset_json['data']\n    logger.info(\"Reading the dataset\")\n    out_data = []\n    for article in dataset:\n        for paragraph_json in article['paragraphs']:\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        file_path = download_files(fnames=['train-v1.1.json'],\n",
   "        file_path = file_path[0]\n",
   "    logger.info(\"Reading file at %s\", file_path)\n",
   "    with open(file_path) as dataset_file:\n",
   "        dataset_json = json.load(dataset_file)\n",
   "        dataset = dataset_json['data']\n",
   "    logger.info(\"Reading the dataset\")\n",
   "    for article in dataset:\n",
   "        for paragraph_json in article['paragraphs']:\n",
   "            paragraph = paragraph_json[\"context\"]\n",
   "            for question_answer in paragraph_json['qas']:\n",
   "                question_answer[\"question\"] = question_answer[\"question\"].strip().replace(\"\\n\", \"\")\n",
   "                out_data.append((question_answer, paragraph))\n"
  ]
 },
 "67": {
  "name": "out_data",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/squad_reader.py",
  "lineno": "31",
  "column": "4",
  "context": "data']\n    logger.info(\"Reading the dataset\")\n    out_data = []\n    for article in dataset:\n        for paragraph_",
  "context_lines": "    with open(file_path) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        dataset = dataset_json['data']\n    logger.info(\"Reading the dataset\")\n    out_data = []\n    for article in dataset:\n        for paragraph_json in article['paragraphs']:\n            paragraph = paragraph_json[\"context\"]\n            for question_answer in paragraph_json['qas']:\n",
  "slicing": [
   "    out_data = []\n",
   "                out_data.append((question_answer, paragraph))\n",
   "    return out_data\n"
  ]
 },
 "68": {
  "name": "paragraph",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/squad_reader.py",
  "lineno": "34",
  "column": "12",
  "context": "agraph_json in article['paragraphs']:\n            paragraph = paragraph_json[\"context\"]\n            for question_answer in paragraph_json[",
  "context_lines": "    logger.info(\"Reading the dataset\")\n    out_data = []\n    for article in dataset:\n        for paragraph_json in article['paragraphs']:\n            paragraph = paragraph_json[\"context\"]\n            for question_answer in paragraph_json['qas']:\n                question_answer[\"question\"] = question_answer[\"question\"].strip().replace(\"\\n\", \"\")\n                out_data.append((question_answer, paragraph))\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        file_path = download_files(fnames=['train-v1.1.json'],\n",
   "        file_path = file_path[0]\n",
   "    logger.info(\"Reading file at %s\", file_path)\n",
   "    with open(file_path) as dataset_file:\n",
   "        dataset_json = json.load(dataset_file)\n",
   "        dataset = dataset_json['data']\n",
   "    logger.info(\"Reading the dataset\")\n",
   "    for article in dataset:\n",
   "        for paragraph_json in article['paragraphs']:\n",
   "            paragraph = paragraph_json[\"context\"]\n",
   "            for question_answer in paragraph_json['qas']:\n",
   "                question_answer[\"question\"] = question_answer[\"question\"].strip().replace(\"\\n\", \"\")\n",
   "                out_data.append((question_answer, paragraph))\n"
  ]
 },
 "69": {
  "name": "logger",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "16",
  "column": "0",
  "context": "enerators.addsent.corenlp import StanfordCoreNLP\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nSQUAD_FILE = 'data/squad/train-v1.1.json'\nNEARBY_",
  "context_lines": "from adversarialnlp.generators.addsent.utils import (rejoin, ConstituencyParse, get_tokens_for_answers,\n                                                     get_determiner_for_answers, read_const_parse)\nfrom adversarialnlp.generators.addsent.squad_reader import squad_reader\nfrom adversarialnlp.generators.addsent.corenlp import StanfordCoreNLP\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nSQUAD_FILE = 'data/squad/train-v1.1.json'\nNEARBY_GLOVE_FILE = 'data/addsent/nearby_n100_glove_6B_100d.json'\nPOSTAG_FILE = 'data/addsent/postag_dict.json'\n\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n"
  ]
 },
 "70": {
  "name": "SQUAD_FILE",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "18",
  "column": "0",
  "context": "Logger(__name__)  # pylint: disable=invalid-name\n\nSQUAD_FILE = 'data/squad/train-v1.1.json'\nNEARBY_GLOVE_FILE = 'data/addsent/nearby_n100_glov",
  "context_lines": "                                                     get_determiner_for_answers, read_const_parse)\nfrom adversarialnlp.generators.addsent.squad_reader import squad_reader\nfrom adversarialnlp.generators.addsent.corenlp import StanfordCoreNLP\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nSQUAD_FILE = 'data/squad/train-v1.1.json'\nNEARBY_GLOVE_FILE = 'data/addsent/nearby_n100_glove_6B_100d.json'\nPOSTAG_FILE = 'data/addsent/postag_dict.json'\n\nclass AddSentGenerator(Generator):\n    r\"\"\"Adversarial examples generator based on AddSent.\n\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n"
  ]
 },
 "71": {
  "name": "NEARBY_GLOVE_FILE",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "19",
  "column": "0",
  "context": "d-name\n\nSQUAD_FILE = 'data/squad/train-v1.1.json'\nNEARBY_GLOVE_FILE = 'data/addsent/nearby_n100_glove_6B_100d.json'\nPOSTAG_FILE = 'data/addsent/postag_dict.json'\n\ncla",
  "context_lines": "from adversarialnlp.generators.addsent.squad_reader import squad_reader\nfrom adversarialnlp.generators.addsent.corenlp import StanfordCoreNLP\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nSQUAD_FILE = 'data/squad/train-v1.1.json'\nNEARBY_GLOVE_FILE = 'data/addsent/nearby_n100_glove_6B_100d.json'\nPOSTAG_FILE = 'data/addsent/postag_dict.json'\n\nclass AddSentGenerator(Generator):\n    r\"\"\"Adversarial examples generator based on AddSent.\n\n    AddSent is described in the paper `Adversarial Examples for\n",
  "slicing": "NEARBY_GLOVE_FILE = 'data/addsent/nearby_n100_glove_6B_100d.json'\n"
 },
 "72": {
  "name": "POSTAG_FILE",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "20",
  "column": "0",
  "context": "E = 'data/addsent/nearby_n100_glove_6B_100d.json'\nPOSTAG_FILE = 'data/addsent/postag_dict.json'\n\nclass AddSentGenerator(Generator):\n    r\"\"\"Advers",
  "context_lines": "from adversarialnlp.generators.addsent.corenlp import StanfordCoreNLP\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nSQUAD_FILE = 'data/squad/train-v1.1.json'\nNEARBY_GLOVE_FILE = 'data/addsent/nearby_n100_glove_6B_100d.json'\nPOSTAG_FILE = 'data/addsent/postag_dict.json'\n\nclass AddSentGenerator(Generator):\n    r\"\"\"Adversarial examples generator based on AddSent.\n\n    AddSent is described in the paper `Adversarial Examples for\n",
  "slicing": "POSTAG_FILE = 'data/addsent/postag_dict.json'\n"
 },
 "73": {
  "name": "model_files",
  "type": "adversarialnlp.common.file_utils.download_files",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "65",
  "column": "8",
  "context": "Generator).__init__(default_seeds, quiet)\n        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n                                             'post",
  "context_lines": "                 use_answer_placeholder: bool = False,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n        super(AddSentGenerator).__init__(default_seeds, quiet)\n        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n                                             'postag_dict.json'],\n                                     local_folder='addsent')\n        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n                                      paths='http://nlp.stanford.edu/software/',\n",
  "slicing": [
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "        with open(model_files[1], 'r') as data_file:\n"
  ]
 },
 "74": {
  "name": "corenlp_path",
  "type": "adversarialnlp.common.file_utils.download_files",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "68",
  "column": "8",
  "context": "                  local_folder='addsent')\n        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n                                      paths='http:",
  "context_lines": "        super(AddSentGenerator).__init__(default_seeds, quiet)\n        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n                                             'postag_dict.json'],\n                                     local_folder='addsent')\n        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n                                      paths='http://nlp.stanford.edu/software/',\n                                      local_folder='corenlp')\n\n        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n        with open(model_files[0], 'r') as data_file:\n",
  "slicing": [
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n"
  ]
 },
 "75": {
  "name": "props",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "91",
  "column": "8",
  "context": "\n        r\"\"\"Wrapper to call CoreNLP. \"\"\"\n        props = {'annotators': annotators,\n                 'ssplit.newlineIsSentenceBreak': ",
  "context_lines": "    def close(self):\n        self.nlp.close()\n\n    def _annotate(self, text: str, annotators: str):\n        r\"\"\"Wrapper to call CoreNLP. \"\"\"\n        props = {'annotators': annotators,\n                 'ssplit.newlineIsSentenceBreak': 'always',\n                 'outputFormat':'json'}\n        return json.loads(self.nlp.annotate(text, properties=props))\n\n    def _alter_question(self, question, tokens, const_parse):\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n"
  ]
 },
 "76": {
  "name": "used_words",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "98",
  "column": "8",
  "context": "estion to make it ask something else. \"\"\"\n        used_words = [tok['word'].lower() for tok in tokens]\n        new_qs = []\n        toks_all = []\n        ",
  "context_lines": "                 'outputFormat':'json'}\n        return json.loads(self.nlp.annotate(text, properties=props))\n\n    def _alter_question(self, question, tokens, const_parse):\n        r\"\"\"Alter the question to make it ask something else. \"\"\"\n        used_words = [tok['word'].lower() for tok in tokens]\n        new_qs = []\n        toks_all = []\n        if self.alteration_strategy.startswith('high-conf'):\n            rules = HIGH_CONF_ALTER_RULES\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "77": {
  "name": "new_qs",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "99",
  "column": "8",
  "context": "= [tok['word'].lower() for tok in tokens]\n        new_qs = []\n        toks_all = []\n        if self.alteration_s",
  "context_lines": "        return json.loads(self.nlp.annotate(text, properties=props))\n\n    def _alter_question(self, question, tokens, const_parse):\n        r\"\"\"Alter the question to make it ask something else. \"\"\"\n        used_words = [tok['word'].lower() for tok in tokens]\n        new_qs = []\n        toks_all = []\n        if self.alteration_strategy.startswith('high-conf'):\n            rules = HIGH_CONF_ALTER_RULES\n        else:\n",
  "slicing": [
   "        new_qs = []\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n"
  ]
 },
 "78": {
  "name": "toks_all",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "100",
  "column": "8",
  "context": "() for tok in tokens]\n        new_qs = []\n        toks_all = []\n        if self.alteration_strategy.startswith('hi",
  "context_lines": "    def _alter_question(self, question, tokens, const_parse):\n        r\"\"\"Alter the question to make it ask something else. \"\"\"\n        used_words = [tok['word'].lower() for tok in tokens]\n        new_qs = []\n        toks_all = []\n        if self.alteration_strategy.startswith('high-conf'):\n            rules = HIGH_CONF_ALTER_RULES\n        else:\n            rules = ALL_ALTER_RULES\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "79": {
  "name": "rules",
  "type": "adversarialnlp.generators.addsent.rules.HIGH_CONF_ALTER_RULES",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "102",
  "column": "12",
  "context": "ion_strategy.startswith('high-conf'):\n            rules = HIGH_CONF_ALTER_RULES\n        else:\n            rules = ALL_ALTER_RULES\n",
  "context_lines": "        used_words = [tok['word'].lower() for tok in tokens]\n        new_qs = []\n        toks_all = []\n        if self.alteration_strategy.startswith('high-conf'):\n            rules = HIGH_CONF_ALTER_RULES\n        else:\n            rules = ALL_ALTER_RULES\n        for i, tok in enumerate(tokens):\n            if tok['word'].lower() in DO_NOT_ALTER:\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "80": {
  "name": "begin",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "110",
  "column": "12",
  "context": ".append(tok)\n                continue\n            begin = tokens[:i]\n            end = tokens[i+1:]\n            found =",
  "context_lines": "            if tok['word'].lower() in DO_NOT_ALTER:\n                if self.alteration_strategy in ('high-conf', 'all'):\n                    toks_all.append(tok)\n                continue\n            begin = tokens[:i]\n            end = tokens[i+1:]\n            found = False\n            for rule_name in rules:\n                rule = rules[rule_name]\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "81": {
  "name": "end",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "111",
  "column": "12",
  "context": "ntinue\n            begin = tokens[:i]\n            end = tokens[i+1:]\n            found = False\n            for rule_nam",
  "context_lines": "                if self.alteration_strategy in ('high-conf', 'all'):\n                    toks_all.append(tok)\n                continue\n            begin = tokens[:i]\n            end = tokens[i+1:]\n            found = False\n            for rule_name in rules:\n                rule = rules[rule_name]\n                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "82": {
  "name": "found",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "112",
  "column": "12",
  "context": "ns[:i]\n            end = tokens[i+1:]\n            found = False\n            for rule_name in rules:\n              ",
  "context_lines": "                    toks_all.append(tok)\n                continue\n            begin = tokens[:i]\n            end = tokens[i+1:]\n            found = False\n            for rule_name in rules:\n                rule = rules[rule_name]\n                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n                if new_words:\n",
  "slicing": [
   "            found = False\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n"
  ]
 },
 "83": {
  "name": "rule",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "114",
  "column": "16",
  "context": "          for rule_name in rules:\n                rule = rules[rule_name]\n                new_words = rule(tok, nearby_word_",
  "context_lines": "            begin = tokens[:i]\n            end = tokens[i+1:]\n            found = False\n            for rule_name in rules:\n                rule = rules[rule_name]\n                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n                if new_words:\n                    for word in new_words:\n                        if word.lower() in used_words:\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "84": {
  "name": "new_words",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "115",
  "column": "16",
  "context": "          rule = rules[rule_name]\n                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n                if new_words:\n                    ",
  "context_lines": "            end = tokens[i+1:]\n            found = False\n            for rule_name in rules:\n                rule = rules[rule_name]\n                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n                if new_words:\n                    for word in new_words:\n                        if word.lower() in used_words:\n                            continue\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "85": {
  "name": "word",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "124",
  "column": "28",
  "context": " tok['word'].upper():\n                            word = word.upper()\n                        elif tok['word'] == tok['w",
  "context_lines": "                        if word.lower() in BAD_ALTERATIONS:\n                            continue\n                        # Match capitzliation\n                        if tok['word'] == tok['word'].upper():\n                            word = word.upper()\n                        elif tok['word'] == tok['word'].title():\n                            word = word.title()\n                        new_tok = dict(tok)\n                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "86": {
  "name": "new_tok",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "127",
  "column": "24",
  "context": "      word = word.title()\n                        new_tok = dict(tok)\n                        new_tok['word'] = new_tok[",
  "context_lines": "                        if tok['word'] == tok['word'].upper():\n                            word = word.upper()\n                        elif tok['word'] == tok['word'].title():\n                            word = word.title()\n                        new_tok = dict(tok)\n                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n                        new_tok['altered'] = True\n                        # NOTE: obviously this is approximate\n                        if self.alteration_strategy.endswith('separate'):\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "87": {
  "name": "new_tokens",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "132",
  "column": "28",
  "context": "endswith('separate'):\n                            new_tokens = begin + [new_tok] + end\n                            new_q = rejoin(new_tok",
  "context_lines": "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n                        new_tok['altered'] = True\n                        # NOTE: obviously this is approximate\n                        if self.alteration_strategy.endswith('separate'):\n                            new_tokens = begin + [new_tok] + end\n                            new_q = rejoin(new_tokens)\n                            tag = '%s-%d-%s' % (rule_name, i, word)\n                            new_const_parse = ConstituencyParse.replace_words(\n                                    const_parse, [tok['word'] for tok in new_tokens])\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "88": {
  "name": "new_q",
  "type": "adversarialnlp.generators.addsent.utils.rejoin",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "133",
  "column": "28",
  "context": "gin + [new_tok] + end\n                            new_q = rejoin(new_tokens)\n                            tag = '%s-%d-%s' % (ru",
  "context_lines": "                        new_tok['altered'] = True\n                        # NOTE: obviously this is approximate\n                        if self.alteration_strategy.endswith('separate'):\n                            new_tokens = begin + [new_tok] + end\n                            new_q = rejoin(new_tokens)\n                            tag = '%s-%d-%s' % (rule_name, i, word)\n                            new_const_parse = ConstituencyParse.replace_words(\n                                    const_parse, [tok['word'] for tok in new_tokens])\n                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "89": {
  "name": "tag",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "134",
  "column": "28",
  "context": " = rejoin(new_tokens)\n                            tag = '%s-%d-%s' % (rule_name, i, word)\n                            new_const_parse = Cons",
  "context_lines": "                        # NOTE: obviously this is approximate\n                        if self.alteration_strategy.endswith('separate'):\n                            new_tokens = begin + [new_tok] + end\n                            new_q = rejoin(new_tokens)\n                            tag = '%s-%d-%s' % (rule_name, i, word)\n                            new_const_parse = ConstituencyParse.replace_words(\n                                    const_parse, [tok['word'] for tok in new_tokens])\n                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n                            break\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "90": {
  "name": "new_const_parse",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "135",
  "column": "28",
  "context": " (rule_name, i, word)\n                            new_const_parse = ConstituencyParse.replace_words(\n                                    const_parse, [",
  "context_lines": "                        if self.alteration_strategy.endswith('separate'):\n                            new_tokens = begin + [new_tok] + end\n                            new_q = rejoin(new_tokens)\n                            tag = '%s-%d-%s' % (rule_name, i, word)\n                            new_const_parse = ConstituencyParse.replace_words(\n                                    const_parse, [tok['word'] for tok in new_tokens])\n                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n                            break\n                        elif self.alteration_strategy in ('high-conf', 'all'):\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "91": {
  "name": "new_q",
  "type": "adversarialnlp.generators.addsent.utils.rejoin",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "148",
  "column": "12",
  "context": "ion_strategy in ('high-conf', 'all'):\n            new_q = rejoin(toks_all)\n            new_const_parse = ConstituencyParse.re",
  "context_lines": "                    break\n            if self.alteration_strategy in ('high-conf', 'all') and not found:\n                toks_all.append(tok)\n        if self.alteration_strategy in ('high-conf', 'all'):\n            new_q = rejoin(toks_all)\n            new_const_parse = ConstituencyParse.replace_words(\n                    const_parse, [tok['word'] for tok in toks_all])\n            if new_q != question:\n                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "92": {
  "name": "new_const_parse",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "149",
  "column": "12",
  "context": "\n            new_q = rejoin(toks_all)\n            new_const_parse = ConstituencyParse.replace_words(\n                    const_parse, [tok['word'] for ",
  "context_lines": "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n                toks_all.append(tok)\n        if self.alteration_strategy in ('high-conf', 'all'):\n            new_q = rejoin(toks_all)\n            new_const_parse = ConstituencyParse.replace_words(\n                    const_parse, [tok['word'] for tok in toks_all])\n            if new_q != question:\n                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n        return new_qs\n\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "93": {
  "name": "rules",
  "type": "adversarialnlp.generators.addsent.rules.ALL_ALTER_RULES",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "104",
  "column": "12",
  "context": "= HIGH_CONF_ALTER_RULES\n        else:\n            rules = ALL_ALTER_RULES\n        for i, tok in enumerate(tokens):\n         ",
  "context_lines": "        toks_all = []\n        if self.alteration_strategy.startswith('high-conf'):\n            rules = HIGH_CONF_ALTER_RULES\n        else:\n            rules = ALL_ALTER_RULES\n        for i, tok in enumerate(tokens):\n            if tok['word'].lower() in DO_NOT_ALTER:\n                if self.alteration_strategy in ('high-conf', 'all'):\n                    toks_all.append(tok)\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "94": {
  "name": "word",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "126",
  "column": "28",
  "context": " tok['word'].title():\n                            word = word.title()\n                        new_tok = dict(tok)\n      ",
  "context_lines": "                        # Match capitzliation\n                        if tok['word'] == tok['word'].upper():\n                            word = word.upper()\n                        elif tok['word'] == tok['word'].title():\n                            word = word.title()\n                        new_tok = dict(tok)\n                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n                        new_tok['altered'] = True\n                        # NOTE: obviously this is approximate\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "95": {
  "name": "found",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "141",
  "column": "28",
  "context": "s_all.append(new_tok)\n                            found = True\n                            break\n                ",
  "context_lines": "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n                            break\n                        elif self.alteration_strategy in ('high-conf', 'all'):\n                            toks_all.append(new_tok)\n                            found = True\n                            break\n                if self.alteration_strategy in ('high-conf', 'all') and found:\n                    break\n            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
  "slicing": [
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n"
  ]
 },
 "96": {
  "name": "qas",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "157",
  "column": "8",
  "context": "r\"\"\"Edit a SQuAD example using rules. \"\"\"\n        qas, paragraph = seed\n        question = qas['question'].strip()\n       ",
  "context_lines": "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n        return new_qs\n\n    def generate_from_seed(self, seed: Tuple):\n        r\"\"\"Edit a SQuAD example using rules. \"\"\"\n        qas, paragraph = seed\n        question = qas['question'].strip()\n        if not self.quiet:\n            print(f\"Question: {question}\")\n        if self.use_answer_placeholder:\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "97": {
  "name": "paragraph",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "157",
  "column": "13",
  "context": "dit a SQuAD example using rules. \"\"\"\n        qas, paragraph = seed\n        question = qas['question'].strip()\n       ",
  "context_lines": "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n        return new_qs\n\n    def generate_from_seed(self, seed: Tuple):\n        r\"\"\"Edit a SQuAD example using rules. \"\"\"\n        qas, paragraph = seed\n        question = qas['question'].strip()\n        if not self.quiet:\n            print(f\"Question: {question}\")\n        if self.use_answer_placeholder:\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "98": {
  "name": "question",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "158",
  "column": "8",
  "context": " rules. \"\"\"\n        qas, paragraph = seed\n        question = qas['question'].strip()\n        if not self.quiet:\n            print(f\"Que",
  "context_lines": "        return new_qs\n\n    def generate_from_seed(self, seed: Tuple):\n        r\"\"\"Edit a SQuAD example using rules. \"\"\"\n        qas, paragraph = seed\n        question = qas['question'].strip()\n        if not self.quiet:\n            print(f\"Question: {question}\")\n        if self.use_answer_placeholder:\n            answer = 'ANSWER'\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "99": {
  "name": "answer",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "162",
  "column": "12",
  "context": "      if self.use_answer_placeholder:\n            answer = 'ANSWER'\n            determiner = ''\n        else:\n        ",
  "context_lines": "        question = qas['question'].strip()\n        if not self.quiet:\n            print(f\"Question: {question}\")\n        if self.use_answer_placeholder:\n            answer = 'ANSWER'\n            determiner = ''\n        else:\n            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "100": {
  "name": "determiner",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "163",
  "column": "12",
  "context": "holder:\n            answer = 'ANSWER'\n            determiner = ''\n        else:\n            p_parse = self._annotate",
  "context_lines": "        if not self.quiet:\n            print(f\"Question: {question}\")\n        if self.use_answer_placeholder:\n            answer = 'ANSWER'\n            determiner = ''\n        else:\n            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n            determiner = get_determiner_for_answers(qas['answers'])\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "101": {
  "name": "q_parse",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "175",
  "column": "8",
  "context": "       raise ValueError('Missing answer')\n        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n        q_parse = q_parse['sentences'][0]\n        ",
  "context_lines": "                if answer:\n                    break\n            else:\n                raise ValueError('Missing answer')\n        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n        q_parse = q_parse['sentences'][0]\n        q_tokens = q_parse['tokens']\n        q_const_parse = read_const_parse(q_parse['parse'])\n        if self.alteration_strategy:\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "102": {
  "name": "q_parse",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "176",
  "column": "8",
  "context": "uestion, 'tokenize,ssplit,pos,parse,ner')\n        q_parse = q_parse['sentences'][0]\n        q_tokens = q_parse['tokens']\n        q_con",
  "context_lines": "                    break\n            else:\n                raise ValueError('Missing answer')\n        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n        q_parse = q_parse['sentences'][0]\n        q_tokens = q_parse['tokens']\n        q_const_parse = read_const_parse(q_parse['parse'])\n        if self.alteration_strategy:\n            # Easiest to alter the question before converting\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "103": {
  "name": "q_tokens",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "177",
  "column": "8",
  "context": "        q_parse = q_parse['sentences'][0]\n        q_tokens = q_parse['tokens']\n        q_const_parse = read_const_parse(q_parse['",
  "context_lines": "            else:\n                raise ValueError('Missing answer')\n        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n        q_parse = q_parse['sentences'][0]\n        q_tokens = q_parse['tokens']\n        q_const_parse = read_const_parse(q_parse['parse'])\n        if self.alteration_strategy:\n            # Easiest to alter the question before converting\n            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "104": {
  "name": "q_const_parse",
  "type": "adversarialnlp.generators.addsent.utils.read_const_parse",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "178",
  "column": "8",
  "context": "][0]\n        q_tokens = q_parse['tokens']\n        q_const_parse = read_const_parse(q_parse['parse'])\n        if self.alteration_strategy:\n            #",
  "context_lines": "                raise ValueError('Missing answer')\n        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n        q_parse = q_parse['sentences'][0]\n        q_tokens = q_parse['tokens']\n        q_const_parse = read_const_parse(q_parse['parse'])\n        if self.alteration_strategy:\n            # Easiest to alter the question before converting\n            q_list = self._alter_question(question, q_tokens, q_const_parse)\n        else:\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "105": {
  "name": "q_list",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "181",
  "column": "12",
  "context": " alter the question before converting\n            q_list = self._alter_question(question, q_tokens, q_const_parse)\n        else:\n            q_list = [(question, q_t",
  "context_lines": "        q_tokens = q_parse['tokens']\n        q_const_parse = read_const_parse(q_parse['parse'])\n        if self.alteration_strategy:\n            # Easiest to alter the question before converting\n            q_list = self._alter_question(question, q_tokens, q_const_parse)\n        else:\n            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n        for q_str, q_tokens, q_const_parse, tag in q_list:\n            for rule in CONVERSION_RULES:\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "106": {
  "name": "sent",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "186",
  "column": "16",
  "context": "    for rule in CONVERSION_RULES:\n                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n                if sent:\n                    if no",
  "context_lines": "        else:\n            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n        for q_str, q_tokens, q_const_parse, tag in q_list:\n            for rule in CONVERSION_RULES:\n                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n                if sent:\n                    if not self.quiet:\n                        print(f\"  Sent ({tag}): {sent}'\")\n                    cur_qa = {\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "107": {
  "name": "cur_qa",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "190",
  "column": "20",
  "context": "t(f\"  Sent ({tag}): {sent}'\")\n                    cur_qa = {\n                            'question': qas['quest",
  "context_lines": "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n                if sent:\n                    if not self.quiet:\n                        print(f\"  Sent ({tag}): {sent}'\")\n                    cur_qa = {\n                            'question': qas['question'],\n                            'id': '%s-%s' % (qas['id'], tag),\n                            'answers': qas['answers']\n                    }\n",
  "slicing": [
   "                    cur_qa = {\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "108": {
  "name": "cur_text",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "196",
  "column": "24",
  "context": "         if self.prepend:\n                        cur_text = '%s %s' % (sent, paragraph)\n                        new_answers = []\n         ",
  "context_lines": "                            'id': '%s-%s' % (qas['id'], tag),\n                            'answers': qas['answers']\n                    }\n                    if self.prepend:\n                        cur_text = '%s %s' % (sent, paragraph)\n                        new_answers = []\n                        for ans in qas['answers']:\n                            new_answers.append({\n                                    'text': ans['text'],\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "109": {
  "name": "new_answers",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "197",
  "column": "24",
  "context": "s %s' % (sent, paragraph)\n                        new_answers = []\n                        for ans in qas['answers']:",
  "context_lines": "                            'answers': qas['answers']\n                    }\n                    if self.prepend:\n                        cur_text = '%s %s' % (sent, paragraph)\n                        new_answers = []\n                        for ans in qas['answers']:\n                            new_answers.append({\n                                    'text': ans['text'],\n                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
  "slicing": [
   "                        new_answers = []\n",
   "                            new_answers.append({\n",
   "                        cur_qa['answers'] = new_answers\n"
  ]
 },
 "110": {
  "name": "out_example",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "206",
  "column": "20",
  "context": "= '%s %s' % (paragraph, sent)\n                    out_example = {'title': title,\n                                   'seed_context':",
  "context_lines": "                            })\n                        cur_qa['answers'] = new_answers\n                    else:\n                        cur_text = '%s %s' % (paragraph, sent)\n                    out_example = {'title': title,\n                                   'seed_context': paragraph,\n                                   'seed_qas': qas,\n                                   'context': cur_text,\n                                   'qas': [cur_qa]}\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                    out_example = {'title': title,\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n",
   "                    yield out_example\n"
  ]
 },
 "111": {
  "name": "p_parse",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "165",
  "column": "12",
  "context": "        determiner = ''\n        else:\n            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n            ind, a_toks = get_tokens_for_answers(q",
  "context_lines": "        if self.use_answer_placeholder:\n            answer = 'ANSWER'\n            determiner = ''\n        else:\n            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n            determiner = get_determiner_for_answers(qas['answers'])\n            answer_obj = qas['answers'][ind]\n            for _, func in ANSWER_RULES:\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "112": {
  "name": "ind",
  "type": "adversarialnlp.generators.addsent.utils.get_tokens_for_answers",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "166",
  "column": "12",
  "context": "enize,ssplit,pos,ner,entitymentions')\n            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n            determiner = get_determiner_for_answer",
  "context_lines": "            answer = 'ANSWER'\n            determiner = ''\n        else:\n            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n            determiner = get_determiner_for_answers(qas['answers'])\n            answer_obj = qas['answers'][ind]\n            for _, func in ANSWER_RULES:\n                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "113": {
  "name": "a_toks",
  "type": "adversarialnlp.generators.addsent.utils.get_tokens_for_answers",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "166",
  "column": "17",
  "context": ",ssplit,pos,ner,entitymentions')\n            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n            determiner = get_determiner_for_answer",
  "context_lines": "            answer = 'ANSWER'\n            determiner = ''\n        else:\n            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n            determiner = get_determiner_for_answers(qas['answers'])\n            answer_obj = qas['answers'][ind]\n            for _, func in ANSWER_RULES:\n                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "114": {
  "name": "determiner",
  "type": "adversarialnlp.generators.addsent.utils.get_determiner_for_answers",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "167",
  "column": "12",
  "context": "_for_answers(qas['answers'], p_parse)\n            determiner = get_determiner_for_answers(qas['answers'])\n            answer_obj = qas['answers'][ind]\n     ",
  "context_lines": "            determiner = ''\n        else:\n            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n            determiner = get_determiner_for_answers(qas['answers'])\n            answer_obj = qas['answers'][ind]\n            for _, func in ANSWER_RULES:\n                answer = func(answer_obj, a_toks, question, determiner=determiner)\n                if answer:\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "115": {
  "name": "answer_obj",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "168",
  "column": "12",
  "context": "eterminer_for_answers(qas['answers'])\n            answer_obj = qas['answers'][ind]\n            for _, func in ANSWER_RULES:\n         ",
  "context_lines": "        else:\n            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n            determiner = get_determiner_for_answers(qas['answers'])\n            answer_obj = qas['answers'][ind]\n            for _, func in ANSWER_RULES:\n                answer = func(answer_obj, a_toks, question, determiner=determiner)\n                if answer:\n                    break\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "116": {
  "name": "answer",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "170",
  "column": "16",
  "context": "     for _, func in ANSWER_RULES:\n                answer = func(answer_obj, a_toks, question, determiner=determiner)\n                if answer:\n                    bre",
  "context_lines": "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n            determiner = get_determiner_for_answers(qas['answers'])\n            answer_obj = qas['answers'][ind]\n            for _, func in ANSWER_RULES:\n                answer = func(answer_obj, a_toks, question, determiner=determiner)\n                if answer:\n                    break\n            else:\n                raise ValueError('Missing answer')\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "117": {
  "name": "q_list",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "183",
  "column": "12",
  "context": "_tokens, q_const_parse)\n        else:\n            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n        for q_str, q_tokens, q_const_parse, tag in",
  "context_lines": "        if self.alteration_strategy:\n            # Easiest to alter the question before converting\n            q_list = self._alter_question(question, q_tokens, q_const_parse)\n        else:\n            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n        for q_str, q_tokens, q_const_parse, tag in q_list:\n            for rule in CONVERSION_RULES:\n                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n                if sent:\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "118": {
  "name": "cur_text",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "205",
  "column": "24",
  "context": "                    else:\n                        cur_text = '%s %s' % (paragraph, sent)\n                    out_example = {'title': title,",
  "context_lines": "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n                            })\n                        cur_qa['answers'] = new_answers\n                    else:\n                        cur_text = '%s %s' % (paragraph, sent)\n                    out_example = {'title': title,\n                                   'seed_context': paragraph,\n                                   'seed_qas': qas,\n                                   'context': cur_text,\n",
  "slicing": [
   "SQUAD_FILE = 'data/squad/train-v1.1.json'\n",
   "        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
   "        corenlp_path = download_files(fnames=['stanford-corenlp-full-2018-02-27.zip'],\n",
   "        self.nlp: StanfordCoreNLP = StanfordCoreNLP(corenlp_path[0])\n",
   "        with open(model_files[0], 'r') as data_file:\n",
   "            self.nearby_word_dict: Dict = json.load(data_file)\n",
   "        with open(model_files[1], 'r') as data_file:\n",
   "            self.postag_dict: Dict = json.load(data_file)\n",
   "            self.default_seeds = squad_reader(SQUAD_FILE)\n",
   "        props = {'annotators': annotators,\n",
   "        return json.loads(self.nlp.annotate(text, properties=props))\n",
   "        used_words = [tok['word'].lower() for tok in tokens]\n",
   "        new_qs = []\n",
   "        toks_all = []\n",
   "            rules = HIGH_CONF_ALTER_RULES\n",
   "            rules = ALL_ALTER_RULES\n",
   "        for i, tok in enumerate(tokens):\n",
   "            if tok['word'].lower() in DO_NOT_ALTER:\n",
   "                    toks_all.append(tok)\n",
   "            begin = tokens[:i]\n",
   "            end = tokens[i+1:]\n",
   "            found = False\n",
   "            for rule_name in rules:\n",
   "                rule = rules[rule_name]\n",
   "                new_words = rule(tok, nearby_word_dict=self.nearby_word_dict, postag_dict=self.postag_dict)\n",
   "                if new_words:\n",
   "                    for word in new_words:\n",
   "                        if word.lower() in used_words:\n",
   "                        if word.lower() in BAD_ALTERATIONS:\n",
   "                        if tok['word'] == tok['word'].upper():\n",
   "                            word = word.upper()\n",
   "                        elif tok['word'] == tok['word'].title():\n",
   "                            word = word.title()\n",
   "                        new_tok = dict(tok)\n",
   "                        new_tok['word'] = new_tok['lemma'] = new_tok['originalText'] = word\n",
   "                        new_tok['altered'] = True\n",
   "                            new_tokens = begin + [new_tok] + end\n",
   "                            new_q = rejoin(new_tokens)\n",
   "                            tag = '%s-%d-%s' % (rule_name, i, word)\n",
   "                            new_const_parse = ConstituencyParse.replace_words(\n",
   "                                    const_parse, [tok['word'] for tok in new_tokens])\n",
   "                            new_qs.append((new_q, new_tokens, new_const_parse, tag))\n",
   "                            toks_all.append(new_tok)\n",
   "                            found = True\n",
   "                if self.alteration_strategy in ('high-conf', 'all') and found:\n",
   "            if self.alteration_strategy in ('high-conf', 'all') and not found:\n",
   "                toks_all.append(tok)\n",
   "            new_q = rejoin(toks_all)\n",
   "            new_const_parse = ConstituencyParse.replace_words(\n",
   "                    const_parse, [tok['word'] for tok in toks_all])\n",
   "            if new_q != question:\n",
   "                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n",
   "        return new_qs\n",
   "        qas, paragraph = seed\n",
   "        question = qas['question'].strip()\n",
   "            print(f\"Question: {question}\")\n",
   "            answer = 'ANSWER'\n",
   "            determiner = ''\n",
   "            p_parse = self._annotate(paragraph, 'tokenize,ssplit,pos,ner,entitymentions')\n",
   "            ind, a_toks = get_tokens_for_answers(qas['answers'], p_parse)\n",
   "            determiner = get_determiner_for_answers(qas['answers'])\n",
   "            answer_obj = qas['answers'][ind]\n",
   "            for _, func in ANSWER_RULES:\n",
   "                answer = func(answer_obj, a_toks, question, determiner=determiner)\n",
   "                if answer:\n",
   "        q_parse = self._annotate(question, 'tokenize,ssplit,pos,parse,ner')\n",
   "        q_parse = q_parse['sentences'][0]\n",
   "        q_tokens = q_parse['tokens']\n",
   "        q_const_parse = read_const_parse(q_parse['parse'])\n",
   "            q_list = self._alter_question(question, q_tokens, q_const_parse)\n",
   "            q_list = [(question, q_tokens, q_const_parse, 'unaltered')]\n",
   "        for q_str, q_tokens, q_const_parse, tag in q_list:\n",
   "            for rule in CONVERSION_RULES:\n",
   "                sent = rule.convert(q_str, answer, q_tokens, q_const_parse)\n",
   "                if sent:\n",
   "                        print(f\"  Sent ({tag}): {sent}'\")\n",
   "                    cur_qa = {\n",
   "                            'question': qas['question'],\n",
   "                            'id': '%s-%s' % (qas['id'], tag),\n",
   "                            'answers': qas['answers']\n",
   "                        cur_text = '%s %s' % (sent, paragraph)\n",
   "                        new_answers = []\n",
   "                        for ans in qas['answers']:\n",
   "                            new_answers.append({\n",
   "                                    'text': ans['text'],\n",
   "                                    'answer_start': ans['answer_start'] + len(sent) + 1\n",
   "                        cur_qa['answers'] = new_answers\n",
   "                        cur_text = '%s %s' % (paragraph, sent)\n",
   "                                   'seed_context': paragraph,\n",
   "                                   'seed_qas': qas,\n",
   "                                   'context': cur_text,\n",
   "                                   'qas': [cur_qa]}\n"
  ]
 },
 "119": {
  "name": "sock",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "118",
  "column": "8",
  "context": "port)\n\n        # Wait until server starts\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        host_name = urlparse(self.url).hostname\n  ",
  "context_lines": "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n                logging.info('Server shell PID: {}'.format(self.p.pid))\n\n            self.url = 'http://localhost:' + str(self.port)\n\n        # Wait until server starts\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        host_name = urlparse(self.url).hostname\n        time.sleep(1)  # OSX, not tested\n        trial = 1\n        while sock.connect_ex((host_name, self.port)):\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        while sock.connect_ex((host_name, self.port)):\n"
  ]
 },
 "120": {
  "name": "host_name",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "119",
  "column": "8",
  "context": "ocket(socket.AF_INET, socket.SOCK_STREAM)\n        host_name = urlparse(self.url).hostname\n        time.sleep(1)  # OSX, not tested\n        t",
  "context_lines": "                logging.info('Server shell PID: {}'.format(self.p.pid))\n\n            self.url = 'http://localhost:' + str(self.port)\n\n        # Wait until server starts\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        host_name = urlparse(self.url).hostname\n        time.sleep(1)  # OSX, not tested\n        trial = 1\n        while sock.connect_ex((host_name, self.port)):\n            if trial > max_retries:\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        while sock.connect_ex((host_name, self.port)):\n"
  ]
 },
 "121": {
  "name": "trial",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "121",
  "column": "8",
  "context": "\n        time.sleep(1)  # OSX, not tested\n        trial = 1\n        while sock.connect_ex((host_name, self.por",
  "context_lines": "        # Wait until server starts\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        host_name = urlparse(self.url).hostname\n        time.sleep(1)  # OSX, not tested\n        trial = 1\n        while sock.connect_ex((host_name, self.port)):\n            if trial > max_retries:\n                raise ValueError('Corenlp server is not available')\n            logging.info('Waiting until the server is available.')\n",
  "slicing": [
   "        trial = 1\n",
   "            if trial > max_retries:\n"
  ]
 },
 "122": {
  "name": "trial",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "126",
  "column": "12",
  "context": "ting until the server is available.')\n            trial += 1\n            time.sleep(1)\n        logging.info('Th",
  "context_lines": "        while sock.connect_ex((host_name, self.port)):\n            if trial > max_retries:\n                raise ValueError('Corenlp server is not available')\n            logging.info('Waiting until the server is available.')\n            trial += 1\n            time.sleep(1)\n        logging.info('The server is available.')\n\n    def __enter__(self):\n        return self\n\n",
  "slicing": "            trial += 1\n"
 },
 "123": {
  "name": "directory",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "55",
  "column": "12",
  "context": "th_or_host) + ' is not a directory.')\n            directory = os.path.normpath(self.path_or_host) + os.sep\n            self.class_path_dir = directory\n\n     ",
  "context_lines": "                raise RuntimeError('Java not found.')\n\n            # Check if the dir exists\n            if not os.path.isdir(self.path_or_host):\n                raise IOError(str(self.path_or_host) + ' is not a directory.')\n            directory = os.path.normpath(self.path_or_host) + os.sep\n            self.class_path_dir = directory\n\n            # Check if the language specific model file exists\n            switcher = {\n                'en': 'stanford-corenlp-[0-9].[0-9].[0-9]-models.jar',\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n"
  ]
 },
 "124": {
  "name": "switcher",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "59",
  "column": "12",
  "context": "e language specific model file exists\n            switcher = {\n                'en': 'stanford-corenlp-[0-9].[0-9",
  "context_lines": "                raise IOError(str(self.path_or_host) + ' is not a directory.')\n            directory = os.path.normpath(self.path_or_host) + os.sep\n            self.class_path_dir = directory\n\n            # Check if the language specific model file exists\n            switcher = {\n                'en': 'stanford-corenlp-[0-9].[0-9].[0-9]-models.jar',\n                'zh': 'stanford-chinese-corenlp-[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]-models.jar',\n                'ar': 'stanford-arabic-corenlp-[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]-models.jar',\n                'fr': 'stanford-french-corenlp-[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]-models.jar',\n",
  "slicing": [
   "            switcher = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n"
  ]
 },
 "125": {
  "name": "jars",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "67",
  "column": "12",
  "context": "-[0-9][0-9]-models.jar'\n            }\n            jars = {\n                'en': 'stanford-corenlp-x.x.x-mode",
  "context_lines": "                'fr': 'stanford-french-corenlp-[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]-models.jar',\n                'de': 'stanford-german-corenlp-[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]-models.jar',\n                'es': 'stanford-spanish-corenlp-[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]-models.jar'\n            }\n            jars = {\n                'en': 'stanford-corenlp-x.x.x-models.jar',\n                'zh': 'stanford-chinese-corenlp-yyyy-MM-dd-models.jar',\n                'ar': 'stanford-arabic-corenlp-yyyy-MM-dd-models.jar',\n                'fr': 'stanford-french-corenlp-yyyy-MM-dd-models.jar',\n",
  "slicing": [
   "            jars = {\n",
   "                raise IOError(jars.get(\n"
  ]
 },
 "126": {
  "name": "cmd",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "95",
  "column": "12",
  "context": "info('Initializing native server...')\n            cmd = \"java\"\n            java_args = \"-Xmx{}\".format(self.memor",
  "context_lines": "            # if self.port in [conn.laddr[1] for conn in psutil.net_connections()]:\n            #     raise IOError('Port ' + str(self.port) + ' is already in use.')\n\n            # Start native server\n            logging.info('Initializing native server...')\n            cmd = \"java\"\n            java_args = \"-Xmx{}\".format(self.memory)\n            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n            class_path = '\"{}*\"'.format(directory)\n\n            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n"
  ]
 },
 "127": {
  "name": "java_args",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "96",
  "column": "12",
  "context": " server...')\n            cmd = \"java\"\n            java_args = \"-Xmx{}\".format(self.memory)\n            java_class = \"edu.stanford.nlp.pipelin",
  "context_lines": "            #     raise IOError('Port ' + str(self.port) + ' is already in use.')\n\n            # Start native server\n            logging.info('Initializing native server...')\n            cmd = \"java\"\n            java_args = \"-Xmx{}\".format(self.memory)\n            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n            class_path = '\"{}*\"'.format(directory)\n\n            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n\n            args = ' '.join(args)\n\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n"
  ]
 },
 "128": {
  "name": "java_class",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "97",
  "column": "12",
  "context": "a_args = \"-Xmx{}\".format(self.memory)\n            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n            class_path = '\"{}*\"'.format(directory)",
  "context_lines": "            # Start native server\n            logging.info('Initializing native server...')\n            cmd = \"java\"\n            java_args = \"-Xmx{}\".format(self.memory)\n            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n            class_path = '\"{}*\"'.format(directory)\n\n            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n\n            args = ' '.join(args)\n\n            logging.info(args)\n\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n"
  ]
 },
 "129": {
  "name": "class_path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "98",
  "column": "12",
  "context": "d.nlp.pipeline.StanfordCoreNLPServer\"\n            class_path = '\"{}*\"'.format(directory)\n\n            args = [cmd, java_args, '-cp', class_",
  "context_lines": "            logging.info('Initializing native server...')\n            cmd = \"java\"\n            java_args = \"-Xmx{}\".format(self.memory)\n            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n            class_path = '\"{}*\"'.format(directory)\n\n            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n\n            args = ' '.join(args)\n\n            logging.info(args)\n\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n"
  ]
 },
 "130": {
  "name": "args",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "100",
  "column": "12",
  "context": "ass_path = '\"{}*\"'.format(directory)\n\n            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n\n            args = ' '.join(args)\n\n            lo",
  "context_lines": "            cmd = \"java\"\n            java_args = \"-Xmx{}\".format(self.memory)\n            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n            class_path = '\"{}*\"'.format(directory)\n\n            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n\n            args = ' '.join(args)\n\n            logging.info(args)\n\n            # Silence\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n"
  ]
 },
 "131": {
  "name": "args",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "102",
  "column": "12",
  "context": "java_class, '-port', str(self.port)]\n\n            args = ' '.join(args)\n\n            logging.info(args)\n\n            # Sil",
  "context_lines": "            java_args = \"-Xmx{}\".format(self.memory)\n            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n            class_path = '\"{}*\"'.format(directory)\n\n            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n\n            args = ' '.join(args)\n\n            logging.info(args)\n\n            # Silence\n            with open(os.devnull, 'w') as null_file:\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n"
  ]
 },
 "132": {
  "name": "out_file",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "108",
  "column": "16",
  "context": "en(os.devnull, 'w') as null_file:\n                out_file = None\n                if self.quiet:\n                   ",
  "context_lines": "            args = ' '.join(args)\n\n            logging.info(args)\n\n            # Silence\n            with open(os.devnull, 'w') as null_file:\n                out_file = None\n                if self.quiet:\n                    out_file = null_file\n\n                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n                logging.info('Server shell PID: {}'.format(self.p.pid))\n\n",
  "slicing": [
   "                out_file = None\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n"
  ]
 },
 "133": {
  "name": "out_file",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "110",
  "column": "20",
  "context": "               if self.quiet:\n                    out_file = null_file\n\n                self.p = subprocess.Popen(args, s",
  "context_lines": "            # Silence\n            with open(os.devnull, 'w') as null_file:\n                out_file = None\n                if self.quiet:\n                    out_file = null_file\n\n                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n                logging.info('Server shell PID: {}'.format(self.p.pid))\n\n            self.url = 'http://localhost:' + str(self.port)\n\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n"
  ]
 },
 "134": {
  "name": "children",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "149",
  "column": "12",
  "context": "t.cmdline()))\n                return\n\n            children = parent.children(recursive=True)\n            for process in children:\n             ",
  "context_lines": "                return\n\n            if self.class_path_dir not in ' '.join(parent.cmdline()):\n                logging.info('Process not in: {}'.format(parent.cmdline()))\n                return\n\n            children = parent.children(recursive=True)\n            for process in children:\n                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n                # process.send_signal(signal.SIGTERM)\n                process.kill()\n\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n"
  ]
 },
 "135": {
  "name": "parent",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "140",
  "column": "16",
  "context": "attr(self, 'p'):\n            try:\n                parent = psutil.Process(self.p.pid)\n            except psutil.NoSuchProcess:\n         ",
  "context_lines": "    def close(self):\n        logging.info('Cleanup...')\n        if hasattr(self, 'p'):\n            try:\n                parent = psutil.Process(self.p.pid)\n            except psutil.NoSuchProcess:\n                logging.info('No process: {}'.format(self.p.pid))\n                return\n\n            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n"
  ]
 },
 "136": {
  "name": "text",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "161",
  "column": "12",
  "context": "      if sys.version_info.major >= 3:\n            text = text.encode('utf-8')\n\n        r = requests.post(self.url, params={'prop",
  "context_lines": "            # parent.send_signal(signal.SIGTERM)\n            parent.kill()\n\n    def annotate(self, text, properties=None):\n        if sys.version_info.major >= 3:\n            text = text.encode('utf-8')\n\n        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n                          headers={'Connection': 'close'})\n        return r.text\n\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "137": {
  "name": "r",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "163",
  "column": "8",
  "context": "\n            text = text.encode('utf-8')\n\n        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n                          headers={'Connection': '",
  "context_lines": "            parent.kill()\n\n    def annotate(self, text, properties=None):\n        if sys.version_info.major >= 3:\n            text = text.encode('utf-8')\n\n        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n                          headers={'Connection': 'close'})\n        return r.text\n\n    def tregex(self, sentence, pattern):\n        tregex_url = self.url + '/tregex'\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "138": {
  "name": "tregex_url",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "168",
  "column": "8",
  "context": "\n    def tregex(self, sentence, pattern):\n        tregex_url = self.url + '/tregex'\n        r_dict = self._request(tregex_url, \"tokeni",
  "context_lines": "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n                          headers={'Connection': 'close'})\n        return r.text\n\n    def tregex(self, sentence, pattern):\n        tregex_url = self.url + '/tregex'\n        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n        return r_dict\n\n    def tokensregex(self, sentence, pattern):\n        tokensregex_url = self.url + '/tokensregex'\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "139": {
  "name": "r_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "169",
  "column": "8",
  "context": "        tregex_url = self.url + '/tregex'\n        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n        return r_dict\n\n    def tokensregex(self, s",
  "context_lines": "                          headers={'Connection': 'close'})\n        return r.text\n\n    def tregex(self, sentence, pattern):\n        tregex_url = self.url + '/tregex'\n        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n        return r_dict\n\n    def tokensregex(self, sentence, pattern):\n        tokensregex_url = self.url + '/tokensregex'\n        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "140": {
  "name": "tokensregex_url",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "173",
  "column": "8",
  "context": "def tokensregex(self, sentence, pattern):\n        tokensregex_url = self.url + '/tokensregex'\n        r_dict = self._request(tokensregex_url, \"t",
  "context_lines": "        tregex_url = self.url + '/tregex'\n        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n        return r_dict\n\n    def tokensregex(self, sentence, pattern):\n        tokensregex_url = self.url + '/tokensregex'\n        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def semgrex(self, sentence, pattern):\n        semgrex_url = self.url + '/semgrex'\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "141": {
  "name": "r_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "174",
  "column": "8",
  "context": "kensregex_url = self.url + '/tokensregex'\n        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def semgrex(self, sente",
  "context_lines": "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n        return r_dict\n\n    def tokensregex(self, sentence, pattern):\n        tokensregex_url = self.url + '/tokensregex'\n        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def semgrex(self, sentence, pattern):\n        semgrex_url = self.url + '/semgrex'\n        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "142": {
  "name": "semgrex_url",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "178",
  "column": "8",
  "context": "    def semgrex(self, sentence, pattern):\n        semgrex_url = self.url + '/semgrex'\n        r_dict = self._request(semgrex_url, \"token",
  "context_lines": "        tokensregex_url = self.url + '/tokensregex'\n        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def semgrex(self, sentence, pattern):\n        semgrex_url = self.url + '/semgrex'\n        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def word_tokenize(self, sentence, span=False):\n        r_dict = self._request('ssplit,tokenize', sentence)\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "143": {
  "name": "r_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "179",
  "column": "8",
  "context": "      semgrex_url = self.url + '/semgrex'\n        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def word_tokenize(self,",
  "context_lines": "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def semgrex(self, sentence, pattern):\n        semgrex_url = self.url + '/semgrex'\n        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def word_tokenize(self, sentence, span=False):\n        r_dict = self._request('ssplit,tokenize', sentence)\n        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "144": {
  "name": "r_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "183",
  "column": "8",
  "context": "ord_tokenize(self, sentence, span=False):\n        r_dict = self._request('ssplit,tokenize', sentence)\n        tokens = [token['originalText'] for s in r",
  "context_lines": "        semgrex_url = self.url + '/semgrex'\n        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def word_tokenize(self, sentence, span=False):\n        r_dict = self._request('ssplit,tokenize', sentence)\n        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n\n        # Whether return token span\n        if span:\n            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "145": {
  "name": "tokens",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "184",
  "column": "8",
  "context": "elf._request('ssplit,tokenize', sentence)\n        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n\n        # Whether return token span\n        if sp",
  "context_lines": "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def word_tokenize(self, sentence, span=False):\n        r_dict = self._request('ssplit,tokenize', sentence)\n        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n\n        # Whether return token span\n        if span:\n            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "146": {
  "name": "spans",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "188",
  "column": "12",
  "context": "er return token span\n        if span:\n            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n                     in s['tokens']]\n            r",
  "context_lines": "        r_dict = self._request('ssplit,tokenize', sentence)\n        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n\n        # Whether return token span\n        if span:\n            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n                     in s['tokens']]\n            return tokens, spans\n        else:\n            return tokens\n\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "147": {
  "name": "r_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "195",
  "column": "8",
  "context": " tokens\n\n    def pos_tag(self, sentence):\n        r_dict = self._request(self.url, 'pos', sentence)\n        words = []\n        tags = []\n        for s",
  "context_lines": "            return tokens, spans\n        else:\n            return tokens\n\n    def pos_tag(self, sentence):\n        r_dict = self._request(self.url, 'pos', sentence)\n        words = []\n        tags = []\n        for s in r_dict['sentences']:\n            for token in s['tokens']:\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "148": {
  "name": "words",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "196",
  "column": "8",
  "context": " self._request(self.url, 'pos', sentence)\n        words = []\n        tags = []\n        for s in r_dict['sentenc",
  "context_lines": "        else:\n            return tokens\n\n    def pos_tag(self, sentence):\n        r_dict = self._request(self.url, 'pos', sentence)\n        words = []\n        tags = []\n        for s in r_dict['sentences']:\n            for token in s['tokens']:\n                words.append(token['originalText'])\n",
  "slicing": [
   "        words = []\n",
   "                words.append(token['originalText'])\n",
   "        return list(zip(words, tags))\n",
   "                words.append(token['originalText'])\n",
   "        return list(zip(words, ner_tags))\n"
  ]
 },
 "149": {
  "name": "tags",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "197",
  "column": "8",
  "context": ".url, 'pos', sentence)\n        words = []\n        tags = []\n        for s in r_dict['sentences']:\n            ",
  "context_lines": "            return tokens\n\n    def pos_tag(self, sentence):\n        r_dict = self._request(self.url, 'pos', sentence)\n        words = []\n        tags = []\n        for s in r_dict['sentences']:\n            for token in s['tokens']:\n                words.append(token['originalText'])\n                tags.append(token['pos'])\n",
  "slicing": [
   "        tags = []\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n"
  ]
 },
 "150": {
  "name": "r_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "205",
  "column": "8",
  "context": "rds, tags))\n\n    def ner(self, sentence):\n        r_dict = self._request(self.url, 'ner', sentence)\n        words = []\n        ner_tags = []\n        f",
  "context_lines": "                words.append(token['originalText'])\n                tags.append(token['pos'])\n        return list(zip(words, tags))\n\n    def ner(self, sentence):\n        r_dict = self._request(self.url, 'ner', sentence)\n        words = []\n        ner_tags = []\n        for s in r_dict['sentences']:\n            for token in s['tokens']:\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "151": {
  "name": "words",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "206",
  "column": "8",
  "context": " self._request(self.url, 'ner', sentence)\n        words = []\n        ner_tags = []\n        for s in r_dict['sen",
  "context_lines": "                tags.append(token['pos'])\n        return list(zip(words, tags))\n\n    def ner(self, sentence):\n        r_dict = self._request(self.url, 'ner', sentence)\n        words = []\n        ner_tags = []\n        for s in r_dict['sentences']:\n            for token in s['tokens']:\n                words.append(token['originalText'])\n",
  "slicing": [
   "        words = []\n",
   "                words.append(token['originalText'])\n",
   "        return list(zip(words, ner_tags))\n"
  ]
 },
 "152": {
  "name": "ner_tags",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "207",
  "column": "8",
  "context": ".url, 'ner', sentence)\n        words = []\n        ner_tags = []\n        for s in r_dict['sentences']:\n            ",
  "context_lines": "        return list(zip(words, tags))\n\n    def ner(self, sentence):\n        r_dict = self._request(self.url, 'ner', sentence)\n        words = []\n        ner_tags = []\n        for s in r_dict['sentences']:\n            for token in s['tokens']:\n                words.append(token['originalText'])\n                ner_tags.append(token['ner'])\n",
  "slicing": [
   "        ner_tags = []\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n"
  ]
 },
 "153": {
  "name": "r_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "215",
  "column": "8",
  "context": "er_tags))\n\n    def parse(self, sentence):\n        r_dict = self._request(self.url, 'pos,parse', sentence)\n        return [s['parse'] for s in r_dict['senten",
  "context_lines": "                words.append(token['originalText'])\n                ner_tags.append(token['ner'])\n        return list(zip(words, ner_tags))\n\n    def parse(self, sentence):\n        r_dict = self._request(self.url, 'pos,parse', sentence)\n        return [s['parse'] for s in r_dict['sentences']][0]\n\n    def dependency_parse(self, sentence):\n        r_dict = self._request(self.url, 'depparse', sentence)\n        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "154": {
  "name": "r_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "219",
  "column": "8",
  "context": "    def dependency_parse(self, sentence):\n        r_dict = self._request(self.url, 'depparse', sentence)\n        return [(dep['dep'], dep['governor'], dep[",
  "context_lines": "    def parse(self, sentence):\n        r_dict = self._request(self.url, 'pos,parse', sentence)\n        return [s['parse'] for s in r_dict['sentences']][0]\n\n    def dependency_parse(self, sentence):\n        r_dict = self._request(self.url, 'depparse', sentence)\n        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n                s['basicDependencies']]\n\n    def coref(self, text):\n        r_dict = self._request('coref', text)\n\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "155": {
  "name": "r_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "224",
  "column": "8",
  "context": "pendencies']]\n\n    def coref(self, text):\n        r_dict = self._request('coref', text)\n\n        corefs = []\n        for k, mentions in r_",
  "context_lines": "        r_dict = self._request(self.url, 'depparse', sentence)\n        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n                s['basicDependencies']]\n\n    def coref(self, text):\n        r_dict = self._request('coref', text)\n\n        corefs = []\n        for k, mentions in r_dict['corefs'].items():\n            simplified_mentions = []\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "156": {
  "name": "corefs",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "226",
  "column": "8",
  "context": "   r_dict = self._request('coref', text)\n\n        corefs = []\n        for k, mentions in r_dict['corefs'].items(",
  "context_lines": "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n                s['basicDependencies']]\n\n    def coref(self, text):\n        r_dict = self._request('coref', text)\n\n        corefs = []\n        for k, mentions in r_dict['corefs'].items():\n            simplified_mentions = []\n            for m in mentions:\n                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
  "slicing": [
   "        corefs = []\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n"
  ]
 },
 "157": {
  "name": "simplified_mentions",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "228",
  "column": "12",
  "context": "mentions in r_dict['corefs'].items():\n            simplified_mentions = []\n            for m in mentions:\n                sim",
  "context_lines": "    def coref(self, text):\n        r_dict = self._request('coref', text)\n\n        corefs = []\n        for k, mentions in r_dict['corefs'].items():\n            simplified_mentions = []\n            for m in mentions:\n                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n            corefs.append(simplified_mentions)\n        return corefs\n\n",
  "slicing": [
   "            simplified_mentions = []\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n"
  ]
 },
 "158": {
  "name": "data",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "240",
  "column": "12",
  "context": "      if sys.version_info.major >= 3:\n            data = data.encode('utf-8')\n\n        properties = {'annotators': annotators, '",
  "context_lines": "        self._check_language(language)\n        self.lang = language\n\n    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n        if sys.version_info.major >= 3:\n            data = data.encode('utf-8')\n\n        properties = {'annotators': annotators, 'outputFormat': 'json'}\n        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n        if 'pattern' in kwargs:\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "159": {
  "name": "properties",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "242",
  "column": "8",
  "context": "\n            data = data.encode('utf-8')\n\n        properties = {'annotators': annotators, 'outputFormat': 'json'}\n        params = {'properties': str(properties), '",
  "context_lines": "        self.lang = language\n\n    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n        if sys.version_info.major >= 3:\n            data = data.encode('utf-8')\n\n        properties = {'annotators': annotators, 'outputFormat': 'json'}\n        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n        if 'pattern' in kwargs:\n            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n\n        logging.info(params)\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "160": {
  "name": "params",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "243",
  "column": "8",
  "context": "ors': annotators, 'outputFormat': 'json'}\n        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n        if 'pattern' in kwargs:\n            params",
  "context_lines": "    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n        if sys.version_info.major >= 3:\n            data = data.encode('utf-8')\n\n        properties = {'annotators': annotators, 'outputFormat': 'json'}\n        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n        if 'pattern' in kwargs:\n            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n\n        logging.info(params)\n        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "161": {
  "name": "params",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "245",
  "column": "12",
  "context": "lang}\n        if 'pattern' in kwargs:\n            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n\n        logging.info(params)\n        r = requests",
  "context_lines": "            data = data.encode('utf-8')\n\n        properties = {'annotators': annotators, 'outputFormat': 'json'}\n        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n        if 'pattern' in kwargs:\n            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n\n        logging.info(params)\n        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n        r_dict = json.loads(r.text)\n\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "162": {
  "name": "r",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "248",
  "column": "8",
  "context": " self.lang}\n\n        logging.info(params)\n        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n        r_dict = json.loads(r.text)\n\n        retur",
  "context_lines": "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n        if 'pattern' in kwargs:\n            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n\n        logging.info(params)\n        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n        r_dict = json.loads(r.text)\n\n        return r_dict\n\n    def _check_args(self):\n        self._check_language(self.lang)\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "163": {
  "name": "r_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "249",
  "column": "8",
  "context": "ta=data, headers={'Connection': 'close'})\n        r_dict = json.loads(r.text)\n\n        return r_dict\n\n    def _check_args(self):",
  "context_lines": "        if 'pattern' in kwargs:\n            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n\n        logging.info(params)\n        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n        r_dict = json.loads(r.text)\n\n        return r_dict\n\n    def _check_args(self):\n        self._check_language(self.lang)\n",
  "slicing": [
   "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
   "            self.class_path_dir = directory\n",
   "            switcher = {\n",
   "            jars = {\n",
   "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
   "                raise IOError(jars.get(\n",
   "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
   "            cmd = \"java\"\n",
   "            java_args = \"-Xmx{}\".format(self.memory)\n",
   "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
   "            class_path = '\"{}*\"'.format(directory)\n",
   "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
   "            args = ' '.join(args)\n",
   "            logging.info(args)\n",
   "            with open(os.devnull, 'w') as null_file:\n",
   "                out_file = None\n",
   "                    out_file = null_file\n",
   "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
   "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
   "        host_name = urlparse(self.url).hostname\n",
   "        trial = 1\n",
   "        while sock.connect_ex((host_name, self.port)):\n",
   "            if trial > max_retries:\n",
   "            trial += 1\n",
   "                parent = psutil.Process(self.p.pid)\n",
   "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
   "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
   "            children = parent.children(recursive=True)\n",
   "            for process in children:\n",
   "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
   "                process.kill()\n",
   "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
   "            parent.kill()\n",
   "            text = text.encode('utf-8')\n",
   "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
   "        return r.text\n",
   "        tregex_url = self.url + '/tregex'\n",
   "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        tokensregex_url = self.url + '/tokensregex'\n",
   "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        semgrex_url = self.url + '/semgrex'\n",
   "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
   "        return r_dict\n",
   "        r_dict = self._request('ssplit,tokenize', sentence)\n",
   "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
   "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
   "                     in s['tokens']]\n",
   "            return tokens, spans\n",
   "            return tokens\n",
   "        r_dict = self._request(self.url, 'pos', sentence)\n",
   "        words = []\n",
   "        tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                tags.append(token['pos'])\n",
   "        return list(zip(words, tags))\n",
   "        r_dict = self._request(self.url, 'ner', sentence)\n",
   "        words = []\n",
   "        ner_tags = []\n",
   "        for s in r_dict['sentences']:\n",
   "            for token in s['tokens']:\n",
   "                words.append(token['originalText'])\n",
   "                ner_tags.append(token['ner'])\n",
   "        return list(zip(words, ner_tags))\n",
   "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
   "        return [s['parse'] for s in r_dict['sentences']][0]\n",
   "        r_dict = self._request(self.url, 'depparse', sentence)\n",
   "        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n",
   "                s['basicDependencies']]\n",
   "        r_dict = self._request('coref', text)\n",
   "        corefs = []\n",
   "        for k, mentions in r_dict['corefs'].items():\n",
   "            simplified_mentions = []\n",
   "            for m in mentions:\n",
   "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
   "            corefs.append(simplified_mentions)\n",
   "        return corefs\n",
   "            data = data.encode('utf-8')\n",
   "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
   "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
   "        logging.info(params)\n",
   "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
   "        r_dict = json.loads(r.text)\n",
   "        return r_dict\n"
  ]
 },
 "164": {
  "name": "orig_i",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "21",
  "column": "8",
  "context": "cursive_parse_corenlp(cls, tokens, i, j):\n        orig_i = i\n        if tokens[i] == '(':\n            tag = tok",
  "context_lines": "        self.word = word\n        self.index = index\n\n    @classmethod\n    def _recursive_parse_corenlp(cls, tokens, i, j):\n        orig_i = i\n        if tokens[i] == '(':\n            tag = tokens[i + 1]\n            children = []\n            i = i + 2\n",
  "slicing": [
   "        orig_i = i\n"
  ]
 },
 "165": {
  "name": "tag",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "23",
  "column": "12",
  "context": "ig_i = i\n        if tokens[i] == '(':\n            tag = tokens[i + 1]\n            children = []\n            i = i + 2\n  ",
  "context_lines": "    @classmethod\n    def _recursive_parse_corenlp(cls, tokens, i, j):\n        orig_i = i\n        if tokens[i] == '(':\n            tag = tokens[i + 1]\n            children = []\n            i = i + 2\n            while True:\n                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n"
  ]
 },
 "166": {
  "name": "children",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "24",
  "column": "12",
  "context": " '(':\n            tag = tokens[i + 1]\n            children = []\n            i = i + 2\n            while True:\n    ",
  "context_lines": "    def _recursive_parse_corenlp(cls, tokens, i, j):\n        orig_i = i\n        if tokens[i] == '(':\n            tag = tokens[i + 1]\n            children = []\n            i = i + 2\n            while True:\n                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n                if isinstance(child, cls):\n",
  "slicing": [
   "            children = []\n",
   "                    children.append(child)\n",
   "                        return cls(tag, children), i + 1, j\n"
  ]
 },
 "167": {
  "name": "i",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "25",
  "column": "12",
  "context": "kens[i + 1]\n            children = []\n            i = i + 2\n            while True:\n                child, i, ",
  "context_lines": "        orig_i = i\n        if tokens[i] == '(':\n            tag = tokens[i + 1]\n            children = []\n            i = i + 2\n            while True:\n                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n                if isinstance(child, cls):\n                    children.append(child)\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "168": {
  "name": "child",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "27",
  "column": "16",
  "context": "i = i + 2\n            while True:\n                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n                if isinstance(child, cls):\n       ",
  "context_lines": "            tag = tokens[i + 1]\n            children = []\n            i = i + 2\n            while True:\n                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n                if isinstance(child, cls):\n                    children.append(child)\n                    if tokens[i] == ')': \n                        return cls(tag, children), i + 1, j\n",
  "slicing": [
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n"
  ]
 },
 "169": {
  "name": "i",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "27",
  "column": "23",
  "context": " 2\n            while True:\n                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n                if isinstance(child, cls):\n       ",
  "context_lines": "            tag = tokens[i + 1]\n            children = []\n            i = i + 2\n            while True:\n                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n                if isinstance(child, cls):\n                    children.append(child)\n                    if tokens[i] == ')': \n                        return cls(tag, children), i + 1, j\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "170": {
  "name": "j",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "27",
  "column": "26",
  "context": "            while True:\n                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n                if isinstance(child, cls):\n       ",
  "context_lines": "            tag = tokens[i + 1]\n            children = []\n            i = i + 2\n            while True:\n                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n                if isinstance(child, cls):\n                    children.append(child)\n                    if tokens[i] == ')': \n                        return cls(tag, children), i + 1, j\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "171": {
  "name": "s_spaced",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "44",
  "column": "8",
  "context": "ne) (NN person))))\\n    (. ?        )))\",\n        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n        tokens = [t for t in s_spaced.split(' ') i",
  "context_lines": "    @classmethod\n    def from_corenlp(cls, s):\n        \"\"\"Parses the \"parse\" attribute returned by CoreNLP parse annotator.\"\"\"\n        # \"parse\": \"(ROOT\\n  (SBARQ\\n    (WHNP (WDT What)\\n      (NP (NN portion)\\n        (PP (IN                       of)\\n          (NP\\n            (NP (NNS households))\\n            (PP (IN in)\\n              (NP (NNP             Jacksonville)))))))\\n    (SQ\\n      (VP (VBP have)\\n        (NP (RB only) (CD one) (NN person))))\\n    (. ?        )))\",\n        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n        tokens = [t for t in s_spaced.split(' ') if t]\n        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n        if index != len(tokens):\n            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "172": {
  "name": "tokens",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "45",
  "column": "8",
  "context": ").replace('(', ' ( ').replace(')', ' ) ')\n        tokens = [t for t in s_spaced.split(' ') if t]\n        tree, index, num_words = cls._recursive_pa",
  "context_lines": "    def from_corenlp(cls, s):\n        \"\"\"Parses the \"parse\" attribute returned by CoreNLP parse annotator.\"\"\"\n        # \"parse\": \"(ROOT\\n  (SBARQ\\n    (WHNP (WDT What)\\n      (NP (NN portion)\\n        (PP (IN                       of)\\n          (NP\\n            (NP (NNS households))\\n            (PP (IN in)\\n              (NP (NNP             Jacksonville)))))))\\n    (SQ\\n      (VP (VBP have)\\n        (NP (RB only) (CD one) (NN person))))\\n    (. ?        )))\",\n        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n        tokens = [t for t in s_spaced.split(' ') if t]\n        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n        if index != len(tokens):\n            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n        return tree\n\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "173": {
  "name": "tree",
  "type": "adversarialnlp.generators.addsent.utils.ConstituencyParse",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "46",
  "column": "8",
  "context": "s = [t for t in s_spaced.split(' ') if t]\n        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n        if index != len(tokens):\n            raise",
  "context_lines": "        \"\"\"Parses the \"parse\" attribute returned by CoreNLP parse annotator.\"\"\"\n        # \"parse\": \"(ROOT\\n  (SBARQ\\n    (WHNP (WDT What)\\n      (NP (NN portion)\\n        (PP (IN                       of)\\n          (NP\\n            (NP (NNS households))\\n            (PP (IN in)\\n              (NP (NNP             Jacksonville)))))))\\n    (SQ\\n      (VP (VBP have)\\n        (NP (RB only) (CD one) (NN person))))\\n    (. ?        )))\",\n        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n        tokens = [t for t in s_spaced.split(' ') if t]\n        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n        if index != len(tokens):\n            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n        return tree\n\n    def is_singleton(self):\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "174": {
  "name": "index",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "46",
  "column": "14",
  "context": " for t in s_spaced.split(' ') if t]\n        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n        if index != len(tokens):\n            raise",
  "context_lines": "        \"\"\"Parses the \"parse\" attribute returned by CoreNLP parse annotator.\"\"\"\n        # \"parse\": \"(ROOT\\n  (SBARQ\\n    (WHNP (WDT What)\\n      (NP (NN portion)\\n        (PP (IN                       of)\\n          (NP\\n            (NP (NNS households))\\n            (PP (IN in)\\n              (NP (NNP             Jacksonville)))))))\\n    (SQ\\n      (VP (VBP have)\\n        (NP (RB only) (CD one) (NN person))))\\n    (. ?        )))\",\n        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n        tokens = [t for t in s_spaced.split(' ') if t]\n        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n        if index != len(tokens):\n            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n        return tree\n\n    def is_singleton(self):\n",
  "slicing": [
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n"
  ]
 },
 "175": {
  "name": "num_words",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "46",
  "column": "21",
  "context": "in s_spaced.split(' ') if t]\n        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n        if index != len(tokens):\n            raise",
  "context_lines": "        \"\"\"Parses the \"parse\" attribute returned by CoreNLP parse annotator.\"\"\"\n        # \"parse\": \"(ROOT\\n  (SBARQ\\n    (WHNP (WDT What)\\n      (NP (NN portion)\\n        (PP (IN                       of)\\n          (NP\\n            (NP (NNS households))\\n            (PP (IN in)\\n              (NP (NNP             Jacksonville)))))))\\n    (SQ\\n      (VP (VBP have)\\n        (NP (RB only) (CD one) (NN person))))\\n    (. ?        )))\",\n        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n        tokens = [t for t in s_spaced.split(' ') if t]\n        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n        if index != len(tokens):\n            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n        return tree\n\n    def is_singleton(self):\n",
  "slicing": "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n"
 },
 "176": {
  "name": "spaces",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "59",
  "column": "8",
  "context": "     \n    def print_tree(self, indent=0):\n        spaces = '  ' * indent\n        if self.word:\n            print(f\"{spaces}",
  "context_lines": "        if len(self.children) > 1:\n            return False\n        return self.children[0].is_singleton()\n        \n    def print_tree(self, indent=0):\n        spaces = '  ' * indent\n        if self.word:\n            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n        else:\n            print(f\"{spaces}{self.tag}\")\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "177": {
  "name": "toks",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "70",
  "column": "8",
  "context": "f self.word:\n            return self.word\n        toks = []\n        for i, c in enumerate(self.children):\n    ",
  "context_lines": "                c.print_tree(indent=indent + 1)\n\n    def get_phrase(self):\n        if self.word:\n            return self.word\n        toks = []\n        for i, c in enumerate(self.children):\n            p = c.get_phrase()\n            if i == 0 or p.startswith(\"'\"):\n                toks.append(p)\n",
  "slicing": [
   "        toks = []\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n"
  ]
 },
 "178": {
  "name": "p",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "72",
  "column": "12",
  "context": "for i, c in enumerate(self.children):\n            p = c.get_phrase()\n            if i == 0 or p.startswith(\"'\"):\n      ",
  "context_lines": "        if self.word:\n            return self.word\n        toks = []\n        for i, c in enumerate(self.children):\n            p = c.get_phrase()\n            if i == 0 or p.startswith(\"'\"):\n                toks.append(p)\n            else:\n                toks.append(' ' + p)\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "179": {
  "name": "new_word",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "92",
  "column": "12",
  "context": " new_words, i):\n        if tree.word:\n            new_word = new_words[i]\n            return (cls(tree.tag, word=new_word, i",
  "context_lines": "        return self.children[-1].get_end_index()\n\n    @classmethod\n    def _recursive_replace_words(cls, tree, new_words, i):\n        if tree.word:\n            new_word = new_words[i]\n            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n        new_children = []\n        for c in tree.children:\n            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "180": {
  "name": "new_children",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "94",
  "column": "8",
  "context": " word=new_word, index=tree.index), i + 1)\n        new_children = []\n        for c in tree.children:\n            new_ch",
  "context_lines": "    def _recursive_replace_words(cls, tree, new_words, i):\n        if tree.word:\n            new_word = new_words[i]\n            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n        new_children = []\n        for c in tree.children:\n            new_child, i = cls._recursive_replace_words(c, new_words, i)\n            new_children.append(new_child)\n        return cls(tree.tag, children=new_children), i\n\n",
  "slicing": [
   "        new_children = []\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n"
  ]
 },
 "181": {
  "name": "new_child",
  "type": "adversarialnlp.generators.addsent.utils.ConstituencyParse",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "96",
  "column": "12",
  "context": " = []\n        for c in tree.children:\n            new_child, i = cls._recursive_replace_words(c, new_words, i)\n            new_children.append(new_child)\n       ",
  "context_lines": "            new_word = new_words[i]\n            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n        new_children = []\n        for c in tree.children:\n            new_child, i = cls._recursive_replace_words(c, new_words, i)\n            new_children.append(new_child)\n        return cls(tree.tag, children=new_children), i\n\n    @classmethod\n    def replace_words(cls, tree, new_words):\n",
  "slicing": [
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n"
  ]
 },
 "182": {
  "name": "i",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "96",
  "column": "23",
  "context": "   for c in tree.children:\n            new_child, i = cls._recursive_replace_words(c, new_words, i)\n            new_children.append(new_child)\n       ",
  "context_lines": "            new_word = new_words[i]\n            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n        new_children = []\n        for c in tree.children:\n            new_child, i = cls._recursive_replace_words(c, new_words, i)\n            new_children.append(new_child)\n        return cls(tree.tag, children=new_children), i\n\n    @classmethod\n    def replace_words(cls, tree, new_words):\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "183": {
  "name": "new_tree",
  "type": "adversarialnlp.generators.addsent.utils.ConstituencyParse",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "103",
  "column": "8",
  "context": "ee, with new words replacing old ones.\"\"\"\n        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n        if i != len(new_words):\n            raise ",
  "context_lines": "        return cls(tree.tag, children=new_children), i\n\n    @classmethod\n    def replace_words(cls, tree, new_words):\n        \"\"\"Return a new tree, with new words replacing old ones.\"\"\"\n        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n        if i != len(new_words):\n            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n        return new_tree\n\ndef rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "184": {
  "name": "i",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "103",
  "column": "18",
  "context": "ew words replacing old ones.\"\"\"\n        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n        if i != len(new_words):\n            raise ",
  "context_lines": "        return cls(tree.tag, children=new_children), i\n\n    @classmethod\n    def replace_words(cls, tree, new_words):\n        \"\"\"Return a new tree, with new words replacing old ones.\"\"\"\n        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n        if i != len(new_words):\n            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n        return new_tree\n\ndef rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "185": {
  "name": "first_a_toks",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "127",
  "column": "4",
  "context": "ns corresponding to a SQuAD answer object.\"\"\"\n    first_a_toks = None\n    for i, a_obj in enumerate(answer_objs):\n      ",
  "context_lines": "        # Use the given separator instead\n        return sep.join(t['originalText'] for t in tokens)\n\n\ndef get_tokens_for_answers(answer_objs: List[Tuple[int, Dict]], corenlp_obj: Dict) -> Tuple[int, List]:\n    \"\"\"Get CoreNLP tokens corresponding to a SQuAD answer object.\"\"\"\n    first_a_toks = None\n    for i, a_obj in enumerate(answer_objs):\n        a_toks = []\n        answer_start = a_obj['answer_start']\n        answer_end = answer_start + len(a_obj['text'])\n",
  "slicing": [
   "    first_a_toks = None\n",
   "    return 0, first_a_toks\n"
  ]
 },
 "186": {
  "name": "a_toks",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "129",
  "column": "8",
  "context": "  for i, a_obj in enumerate(answer_objs):\n        a_toks = []\n        answer_start = a_obj['answer_start']\n     ",
  "context_lines": "def get_tokens_for_answers(answer_objs: List[Tuple[int, Dict]], corenlp_obj: Dict) -> Tuple[int, List]:\n    \"\"\"Get CoreNLP tokens corresponding to a SQuAD answer object.\"\"\"\n    first_a_toks = None\n    for i, a_obj in enumerate(answer_objs):\n        a_toks = []\n        answer_start = a_obj['answer_start']\n        answer_end = answer_start + len(a_obj['text'])\n        for sent in corenlp_obj['sentences']:\n            for tok in sent['tokens']:\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "187": {
  "name": "answer_start",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "130",
  "column": "8",
  "context": "umerate(answer_objs):\n        a_toks = []\n        answer_start = a_obj['answer_start']\n        answer_end = answer_start + len(a_obj['tex",
  "context_lines": "    \"\"\"Get CoreNLP tokens corresponding to a SQuAD answer object.\"\"\"\n    first_a_toks = None\n    for i, a_obj in enumerate(answer_objs):\n        a_toks = []\n        answer_start = a_obj['answer_start']\n        answer_end = answer_start + len(a_obj['text'])\n        for sent in corenlp_obj['sentences']:\n            for tok in sent['tokens']:\n                if tok['characterOffsetBegin'] >= answer_end:\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "188": {
  "name": "answer_end",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "131",
  "column": "8",
  "context": "     answer_start = a_obj['answer_start']\n        answer_end = answer_start + len(a_obj['text'])\n        for sent in corenlp_obj['sentences']:\n    ",
  "context_lines": "    first_a_toks = None\n    for i, a_obj in enumerate(answer_objs):\n        a_toks = []\n        answer_start = a_obj['answer_start']\n        answer_end = answer_start + len(a_obj['text'])\n        for sent in corenlp_obj['sentences']:\n            for tok in sent['tokens']:\n                if tok['characterOffsetBegin'] >= answer_end:\n                    continue\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "189": {
  "name": "first_a_toks",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "143",
  "column": "12",
  "context": "  return i, a_toks\n        if i == 0:\n            first_a_toks = a_toks\n    # None of the extracted token lists reconstruc",
  "context_lines": "        if rejoin(a_toks).strip() == a_obj['text']:\n            # Make sure that the tokens reconstruct the answer\n            return i, a_toks\n        if i == 0:\n            first_a_toks = a_toks\n    # None of the extracted token lists reconstruct the answer\n    # Default to the first\n    return 0, first_a_toks\n\ndef get_determiner_for_answers(answer_objs: List[Dict]) -> Optional[str]:\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "190": {
  "name": "words",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "150",
  "column": "8",
  "context": "ptional[str]:\n    for ans in answer_objs:\n        words = ans['text'].split(' ')\n        if words[0].lower() == 'the':\n            ",
  "context_lines": "    # Default to the first\n    return 0, first_a_toks\n\ndef get_determiner_for_answers(answer_objs: List[Dict]) -> Optional[str]:\n    for ans in answer_objs:\n        words = ans['text'].split(' ')\n        if words[0].lower() == 'the':\n            return 'the'\n        if words[0].lower() in ('a', 'an'):\n            return 'a'\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "191": {
  "name": "wh_word",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "167",
  "column": "4",
  "context": "se('NP', children=[tree])\n        return tree\n    wh_word = None\n    new_np_children = []\n    new_siblings = []\n   ",
  "context_lines": "        if inside_whnp:\n            # Wrap everything in an NP\n            return ConstituencyParse('NP', children=[tree])\n        return tree\n    wh_word = None\n    new_np_children = []\n    new_siblings = []\n    for i, c in enumerate(tree.children):\n        if i == 0:\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "192": {
  "name": "new_np_children",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "168",
  "column": "4",
  "context": "tree])\n        return tree\n    wh_word = None\n    new_np_children = []\n    new_siblings = []\n    for i, c in enumerate(tr",
  "context_lines": "            # Wrap everything in an NP\n            return ConstituencyParse('NP', children=[tree])\n        return tree\n    wh_word = None\n    new_np_children = []\n    new_siblings = []\n    for i, c in enumerate(tree.children):\n        if i == 0:\n            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "193": {
  "name": "new_siblings",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "169",
  "column": "4",
  "context": "e\n    wh_word = None\n    new_np_children = []\n    new_siblings = []\n    for i, c in enumerate(tree.children):\n        ",
  "context_lines": "            return ConstituencyParse('NP', children=[tree])\n        return tree\n    wh_word = None\n    new_np_children = []\n    new_siblings = []\n    for i, c in enumerate(tree.children):\n        if i == 0:\n            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n                wh_word = c.children[0]\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "194": {
  "name": "wh_word",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "173",
  "column": "16",
  "context": "HNP', 'WHADJP', 'WHAVP', 'WHPP'):\n                wh_word = c.children[0]\n                new_np_children.extend(c.children[",
  "context_lines": "    new_siblings = []\n    for i, c in enumerate(tree.children):\n        if i == 0:\n            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n                wh_word = c.children[0]\n                new_np_children.extend(c.children[1:])\n            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n                wh_word = c\n            else:\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "195": {
  "name": "new_np",
  "type": "adversarialnlp.generators.addsent.utils.ConstituencyParse",
  "class": "customized",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "187",
  "column": "8",
  "context": "', children=[c]))\n    if new_np_children:\n        new_np = ConstituencyParse('NP', children=new_np_children)\n        new_tree = ConstituencyParse('WHNP', child",
  "context_lines": "                break\n            # Wrap everything in an NP\n            new_np_children.append(ConstituencyParse('NP', children=[c]))\n    if new_np_children:\n        new_np = ConstituencyParse('NP', children=new_np_children)\n        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n    else:\n        new_tree = tree\n    if new_siblings:\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "196": {
  "name": "new_tree",
  "type": "adversarialnlp.generators.addsent.utils.ConstituencyParse",
  "class": "customized",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "188",
  "column": "8",
  "context": "encyParse('NP', children=new_np_children)\n        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n    else:\n        new_tree = tree\n    if new_sibli",
  "context_lines": "            # Wrap everything in an NP\n            new_np_children.append(ConstituencyParse('NP', children=[c]))\n    if new_np_children:\n        new_np = ConstituencyParse('NP', children=new_np_children)\n        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n    else:\n        new_tree = tree\n    if new_siblings:\n        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "197": {
  "name": "new_tree",
  "type": "adversarialnlp.generators.addsent.utils.ConstituencyParse",
  "class": "customized",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "192",
  "column": "8",
  "context": "     new_tree = tree\n    if new_siblings:\n        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n    return new_tree\n\ndef read_const_parse(parse_st",
  "context_lines": "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n    else:\n        new_tree = tree\n    if new_siblings:\n        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n    return new_tree\n\ndef read_const_parse(parse_str):\n    tree = ConstituencyParse.from_corenlp(parse_str)\n    new_tree = compress_whnp(tree)\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "198": {
  "name": "wh_word",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "176",
  "column": "16",
  "context": "g in ('WDT', 'WP', 'WP$', 'WRB'):\n                wh_word = c\n            else:\n                # No WH-word at ",
  "context_lines": "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n                wh_word = c.children[0]\n                new_np_children.extend(c.children[1:])\n            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n                wh_word = c\n            else:\n                # No WH-word at start of WHNP\n                return tree\n        else:\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "199": {
  "name": "new_siblings",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "182",
  "column": "16",
  "context": "to bad parse, SQ may show up here\n                new_siblings = tree.children[i:]\n                break\n            # Wrap everythin",
  "context_lines": "                # No WH-word at start of WHNP\n                return tree\n        else:\n            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n                new_siblings = tree.children[i:]\n                break\n            # Wrap everything in an NP\n            new_np_children.append(ConstituencyParse('NP', children=[c]))\n    if new_np_children:\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "200": {
  "name": "new_tree",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "190",
  "column": "8",
  "context": "P', children=[wh_word, new_np])\n    else:\n        new_tree = tree\n    if new_siblings:\n        new_tree = Constituen",
  "context_lines": "    if new_np_children:\n        new_np = ConstituencyParse('NP', children=new_np_children)\n        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n    else:\n        new_tree = tree\n    if new_siblings:\n        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n    return new_tree\n\ndef read_const_parse(parse_str):\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "201": {
  "name": "tree",
  "type": "adversarialnlp.generators.addsent.utils.ConstituencyParse",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "196",
  "column": "4",
  "context": "rn new_tree\n\ndef read_const_parse(parse_str):\n    tree = ConstituencyParse.from_corenlp(parse_str)\n    new_tree = compress_whnp(tree)\n    return new_",
  "context_lines": "    if new_siblings:\n        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n    return new_tree\n\ndef read_const_parse(parse_str):\n    tree = ConstituencyParse.from_corenlp(parse_str)\n    new_tree = compress_whnp(tree)\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "202": {
  "name": "new_tree",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "197",
  "column": "4",
  "context": "e = ConstituencyParse.from_corenlp(parse_str)\n    new_tree = compress_whnp(tree)\n    return new_tree\n",
  "context_lines": "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n    return new_tree\n\ndef read_const_parse(parse_str):\n    tree = ConstituencyParse.from_corenlp(parse_str)\n    new_tree = compress_whnp(tree)\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "203": {
  "name": "MONTHS",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "5",
  "column": "0",
  "context": "sarialnlp.generators.addsent.utils import rejoin\n\nMONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n                    'august', 'september', 'octobe",
  "context_lines": "import math\n\nfrom adversarialnlp.generators.addsent.utils import rejoin\n\nMONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n                    'august', 'september', 'october', 'november', 'december']\n\ndef ans_number(a, tokens, q, **kwargs):\n    out_toks = []\n    seen_num = False\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "204": {
  "name": "out_toks",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "9",
  "column": "4",
  "context": "er']\n\ndef ans_number(a, tokens, q, **kwargs):\n    out_toks = []\n    seen_num = False\n    for t in tokens:\n        ",
  "context_lines": "from adversarialnlp.generators.addsent.utils import rejoin\n\nMONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n                    'august', 'september', 'october', 'november', 'december']\n\ndef ans_number(a, tokens, q, **kwargs):\n    out_toks = []\n    seen_num = False\n    for t in tokens:\n        ner = t['ner']\n        pos = t['pos']\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "205": {
  "name": "seen_num",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "10",
  "column": "4",
  "context": "er(a, tokens, q, **kwargs):\n    out_toks = []\n    seen_num = False\n    for t in tokens:\n        ner = t['ner']\n      ",
  "context_lines": "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n                    'august', 'september', 'october', 'november', 'december']\n\ndef ans_number(a, tokens, q, **kwargs):\n    out_toks = []\n    seen_num = False\n    for t in tokens:\n        ner = t['ner']\n        pos = t['pos']\n        w = t['word']\n",
  "slicing": [
   "    seen_num = False\n",
   "    if seen_num:\n"
  ]
 },
 "206": {
  "name": "ner",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "12",
  "column": "8",
  "context": "    seen_num = False\n    for t in tokens:\n        ner = t['ner']\n        pos = t['pos']\n        w = t['word']\n     ",
  "context_lines": "def ans_number(a, tokens, q, **kwargs):\n    out_toks = []\n    seen_num = False\n    for t in tokens:\n        ner = t['ner']\n        pos = t['pos']\n        w = t['word']\n        out_tok = {'before': t['before']}\n\n        # Split on dashes\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "207": {
  "name": "pos",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "13",
  "column": "8",
  "context": "  for t in tokens:\n        ner = t['ner']\n        pos = t['pos']\n        w = t['word']\n        out_tok = {'before':",
  "context_lines": "    out_toks = []\n    seen_num = False\n    for t in tokens:\n        ner = t['ner']\n        pos = t['pos']\n        w = t['word']\n        out_tok = {'before': t['before']}\n\n        # Split on dashes\n        leftover = ''\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "208": {
  "name": "w",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "14",
  "column": "8",
  "context": "    ner = t['ner']\n        pos = t['pos']\n        w = t['word']\n        out_tok = {'before': t['before']}\n\n       ",
  "context_lines": "    seen_num = False\n    for t in tokens:\n        ner = t['ner']\n        pos = t['pos']\n        w = t['word']\n        out_tok = {'before': t['before']}\n\n        # Split on dashes\n        leftover = ''\n        dash_toks = w.split('-')\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "209": {
  "name": "out_tok",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "15",
  "column": "8",
  "context": "     pos = t['pos']\n        w = t['word']\n        out_tok = {'before': t['before']}\n\n        # Split on dashes\n        leftover = ''\n ",
  "context_lines": "    for t in tokens:\n        ner = t['ner']\n        pos = t['pos']\n        w = t['word']\n        out_tok = {'before': t['before']}\n\n        # Split on dashes\n        leftover = ''\n        dash_toks = w.split('-')\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "210": {
  "name": "leftover",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "18",
  "column": "8",
  "context": ": t['before']}\n\n        # Split on dashes\n        leftover = ''\n        dash_toks = w.split('-')\n        if len(da",
  "context_lines": "        pos = t['pos']\n        w = t['word']\n        out_tok = {'before': t['before']}\n\n        # Split on dashes\n        leftover = ''\n        dash_toks = w.split('-')\n        if len(dash_toks) > 1:\n            w = dash_toks[0]\n            leftover = '-'.join(dash_toks[1:])\n\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "211": {
  "name": "dash_toks",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "19",
  "column": "8",
  "context": "  # Split on dashes\n        leftover = ''\n        dash_toks = w.split('-')\n        if len(dash_toks) > 1:\n            w = das",
  "context_lines": "        w = t['word']\n        out_tok = {'before': t['before']}\n\n        # Split on dashes\n        leftover = ''\n        dash_toks = w.split('-')\n        if len(dash_toks) > 1:\n            w = dash_toks[0]\n            leftover = '-'.join(dash_toks[1:])\n\n        # Try to get a number out\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "212": {
  "name": "w",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "21",
  "column": "12",
  "context": "t('-')\n        if len(dash_toks) > 1:\n            w = dash_toks[0]\n            leftover = '-'.join(dash_toks[1:])\n\n  ",
  "context_lines": "        # Split on dashes\n        leftover = ''\n        dash_toks = w.split('-')\n        if len(dash_toks) > 1:\n            w = dash_toks[0]\n            leftover = '-'.join(dash_toks[1:])\n\n        # Try to get a number out\n        value = None\n        if w != '%': \n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "213": {
  "name": "leftover",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "22",
  "column": "12",
  "context": "ks) > 1:\n            w = dash_toks[0]\n            leftover = '-'.join(dash_toks[1:])\n\n        # Try to get a number out\n        value =",
  "context_lines": "        leftover = ''\n        dash_toks = w.split('-')\n        if len(dash_toks) > 1:\n            w = dash_toks[0]\n            leftover = '-'.join(dash_toks[1:])\n\n        # Try to get a number out\n        value = None\n        if w != '%': \n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "214": {
  "name": "value",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "25",
  "column": "8",
  "context": "s[1:])\n\n        # Try to get a number out\n        value = None\n        if w != '%': \n            # Percent sign s",
  "context_lines": "        if len(dash_toks) > 1:\n            w = dash_toks[0]\n            leftover = '-'.join(dash_toks[1:])\n\n        # Try to get a number out\n        value = None\n        if w != '%': \n            # Percent sign should just pass through\n            try:\n                value = float(w.replace(',', ''))\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "215": {
  "name": "value",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "42",
  "column": "12",
  "context": "  # Force this to be a number anyways\n            value = 10\n        if value:\n            if math.isinf(value)",
  "context_lines": "        if not value and (\n                ner == 'NUMBER' or \n                (ner == 'PERCENT' and pos == 'CD')):\n            # Force this to be a number anyways\n            value = 10\n        if value:\n            if math.isinf(value) or math.isnan(value): value = 9001\n            seen_num = True\n            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "216": {
  "name": "value",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "44",
  "column": "55",
  "context": "       if math.isinf(value) or math.isnan(value): value = 9001\n            seen_num = True\n            if w in ('",
  "context_lines": "                (ner == 'PERCENT' and pos == 'CD')):\n            # Force this to be a number anyways\n            value = 10\n        if value:\n            if math.isinf(value) or math.isnan(value): value = 9001\n            seen_num = True\n            if w in ('thousand', 'million', 'billion', 'trillion'):\n                if w == 'thousand':\n                    new_val = 'million'\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "217": {
  "name": "seen_num",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "45",
  "column": "12",
  "context": "e) or math.isnan(value): value = 9001\n            seen_num = True\n            if w in ('thousand', 'million', 'billi",
  "context_lines": "            # Force this to be a number anyways\n            value = 10\n        if value:\n            if math.isinf(value) or math.isnan(value): value = 9001\n            seen_num = True\n            if w in ('thousand', 'million', 'billion', 'trillion'):\n                if w == 'thousand':\n                    new_val = 'million'\n                else:\n",
  "slicing": [
   "            seen_num = True\n",
   "    if seen_num:\n"
  ]
 },
 "218": {
  "name": "new_val",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "48",
  "column": "20",
  "context": "          if w == 'thousand':\n                    new_val = 'million'\n                else:\n                    new_val ",
  "context_lines": "            if math.isinf(value) or math.isnan(value): value = 9001\n            seen_num = True\n            if w in ('thousand', 'million', 'billion', 'trillion'):\n                if w == 'thousand':\n                    new_val = 'million'\n                else:\n                    new_val = 'thousand'\n            else:\n                if value < 2500 and value > 1000:\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "219": {
  "name": "new_val",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "68",
  "column": "16",
  "context": "l_chars)\n            if leftover:\n                new_val = '%s-%s' % (new_val, leftover)\n            out_tok['originalText'] = new_val\n    ",
  "context_lines": "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n                            break\n                    new_val = ''.join(val_chars)\n            if leftover:\n                new_val = '%s-%s' % (new_val, leftover)\n            out_tok['originalText'] = new_val\n        else:\n            out_tok['originalText'] = t['originalText']\n        out_toks.append(out_tok)\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "220": {
  "name": "value",
  "type": "float",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "29",
  "column": "16",
  "context": "ust pass through\n            try:\n                value = float(w.replace(',', ''))\n            except:\n                try:\n         ",
  "context_lines": "        value = None\n        if w != '%': \n            # Percent sign should just pass through\n            try:\n                value = float(w.replace(',', ''))\n            except:\n                try:\n                    norm_ner = t['normalizedNER']\n                    if norm_ner[0] in ('%', '>', '<'):\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "221": {
  "name": "norm_ner",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "32",
  "column": "20",
  "context": " except:\n                try:\n                    norm_ner = t['normalizedNER']\n                    if norm_ner[0] in ('%', '>', '",
  "context_lines": "            try:\n                value = float(w.replace(',', ''))\n            except:\n                try:\n                    norm_ner = t['normalizedNER']\n                    if norm_ner[0] in ('%', '>', '<'):\n                        norm_ner = norm_ner[1:]\n                    value = float(norm_ner)\n                except:\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "222": {
  "name": "norm_ner",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "34",
  "column": "24",
  "context": "er[0] in ('%', '>', '<'):\n                        norm_ner = norm_ner[1:]\n                    value = float(norm_ner)\n      ",
  "context_lines": "            except:\n                try:\n                    norm_ner = t['normalizedNER']\n                    if norm_ner[0] in ('%', '>', '<'):\n                        norm_ner = norm_ner[1:]\n                    value = float(norm_ner)\n                except:\n                    pass\n        if not value and (\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "223": {
  "name": "value",
  "type": "float",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "35",
  "column": "20",
  "context": "      norm_ner = norm_ner[1:]\n                    value = float(norm_ner)\n                except:\n                    pass\n ",
  "context_lines": "                try:\n                    norm_ner = t['normalizedNER']\n                    if norm_ner[0] in ('%', '>', '<'):\n                        norm_ner = norm_ner[1:]\n                    value = float(norm_ner)\n                except:\n                    pass\n        if not value and (\n                ner == 'NUMBER' or \n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "224": {
  "name": "new_val",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "50",
  "column": "20",
  "context": "illion'\n                else:\n                    new_val = 'thousand'\n            else:\n                if value < 2500 ",
  "context_lines": "            if w in ('thousand', 'million', 'billion', 'trillion'):\n                if w == 'thousand':\n                    new_val = 'million'\n                else:\n                    new_val = 'thousand'\n            else:\n                if value < 2500 and value > 1000:\n                    new_val = str(value - 75)\n                else:\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "225": {
  "name": "new_val",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "53",
  "column": "20",
  "context": "alue < 2500 and value > 1000:\n                    new_val = str(value - 75)\n                else:\n                    # Change",
  "context_lines": "                else:\n                    new_val = 'thousand'\n            else:\n                if value < 2500 and value > 1000:\n                    new_val = str(value - 75)\n                else:\n                    # Change leading digit\n                    if value == int(value):\n                        val_chars = list('%d' % value)\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "226": {
  "name": "val_chars",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "57",
  "column": "24",
  "context": "  if value == int(value):\n                        val_chars = list('%d' % value)\n                    else:\n                        ",
  "context_lines": "                    new_val = str(value - 75)\n                else:\n                    # Change leading digit\n                    if value == int(value):\n                        val_chars = list('%d' % value)\n                    else:\n                        val_chars = list('%g' % value)\n                    c = val_chars[0]\n                    for i in range(len(val_chars)):\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "227": {
  "name": "val_chars",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "59",
  "column": "24",
  "context": "                    else:\n                        val_chars = list('%g' % value)\n                    c = val_chars[0]\n             ",
  "context_lines": "                    # Change leading digit\n                    if value == int(value):\n                        val_chars = list('%d' % value)\n                    else:\n                        val_chars = list('%g' % value)\n                    c = val_chars[0]\n                    for i in range(len(val_chars)):\n                        c = val_chars[i]\n                        if c >= '0' and c <= '9':\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "228": {
  "name": "c",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "60",
  "column": "20",
  "context": "al_chars = list('%g' % value)\n                    c = val_chars[0]\n                    for i in range(len(val_chars))",
  "context_lines": "                    if value == int(value):\n                        val_chars = list('%d' % value)\n                    else:\n                        val_chars = list('%g' % value)\n                    c = val_chars[0]\n                    for i in range(len(val_chars)):\n                        c = val_chars[i]\n                        if c >= '0' and c <= '9':\n                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "229": {
  "name": "c",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "62",
  "column": "24",
  "context": "in range(len(val_chars)):\n                        c = val_chars[i]\n                        if c >= '0' and c <= '9':\n",
  "context_lines": "                    else:\n                        val_chars = list('%g' % value)\n                    c = val_chars[0]\n                    for i in range(len(val_chars)):\n                        c = val_chars[i]\n                        if c >= '0' and c <= '9':\n                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n                            break\n                    new_val = ''.join(val_chars)\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "230": {
  "name": "new_val",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "66",
  "column": "20",
  "context": "                        break\n                    new_val = ''.join(val_chars)\n            if leftover:\n                new_val =",
  "context_lines": "                        c = val_chars[i]\n                        if c >= '0' and c <= '9':\n                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n                            break\n                    new_val = ''.join(val_chars)\n            if leftover:\n                new_val = '%s-%s' % (new_val, leftover)\n            out_tok['originalText'] = new_val\n        else:\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "231": {
  "name": "out_toks",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "79",
  "column": "4",
  "context": "n None\n\ndef ans_date(a, tokens, q, **kwargs):\n    out_toks = []\n    if not all(t['ner'] == 'DATE' for t in tokens)",
  "context_lines": "        return rejoin(out_toks).strip()\n    else:\n        return None\n\ndef ans_date(a, tokens, q, **kwargs):\n    out_toks = []\n    if not all(t['ner'] == 'DATE' for t in tokens):\n        return None\n    for t in tokens:\n        if t['pos'] == 'CD' or t['word'].isdigit():\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "232": {
  "name": "new_val",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "88",
  "column": "28",
  "context": "alue = 10  # fallback\n            if value > 50:  new_val = str(value - 25)  # Year\n            else:  # Day of month\n                ",
  "context_lines": "            try:\n                value = int(t['word'])\n            except:\n                value = 10  # fallback\n            if value > 50:  new_val = str(value - 25)  # Year\n            else:  # Day of month\n                if value > 15: new_val = str(value - 11)\n                else: new_val = str(value + 11)\n        else:\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "233": {
  "name": "new_ans",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "100",
  "column": "4",
  "context": "fore': t['before'], 'originalText': new_val})\n    new_ans = rejoin(out_toks).strip()\n    if new_ans == a['text']: return None\n    retur",
  "context_lines": "            else:\n                # Give up\n                new_val = t['originalText']\n        out_toks.append({'before': t['before'], 'originalText': new_val})\n    new_ans = rejoin(out_toks).strip()\n    if new_ans == a['text']: return None\n    return new_ans\n\ndef ans_entity_full(ner_tag, new_ans):\n    \"\"\"Returns a function that yields new_ans iff every token has |ner_tag|.\"\"\"\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "234": {
  "name": "value",
  "type": "int",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "85",
  "column": "16",
  "context": "ord'].isdigit():\n            try:\n                value = int(t['word'])\n            except:\n                value = 10  # ",
  "context_lines": "        return None\n    for t in tokens:\n        if t['pos'] == 'CD' or t['word'].isdigit():\n            try:\n                value = int(t['word'])\n            except:\n                value = 10  # fallback\n            if value > 50:  new_val = str(value - 25)  # Year\n            else:  # Day of month\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "235": {
  "name": "value",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "87",
  "column": "16",
  "context": "nt(t['word'])\n            except:\n                value = 10  # fallback\n            if value > 50:  new_val = str(value - ",
  "context_lines": "        if t['pos'] == 'CD' or t['word'].isdigit():\n            try:\n                value = int(t['word'])\n            except:\n                value = 10  # fallback\n            if value > 50:  new_val = str(value - 25)  # Year\n            else:  # Day of month\n                if value > 15: new_val = str(value - 11)\n                else: new_val = str(value + 11)\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "236": {
  "name": "new_val",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "90",
  "column": "31",
  "context": "e:  # Day of month\n                if value > 15: new_val = str(value - 11)\n                else: new_val = str(value + 11)\n  ",
  "context_lines": "            except:\n                value = 10  # fallback\n            if value > 50:  new_val = str(value - 25)  # Year\n            else:  # Day of month\n                if value > 15: new_val = str(value - 11)\n                else: new_val = str(value + 11)\n        else:\n            if t['word'].lower() in MONTHS:\n                m_ind = MONTHS.index(t['word'].lower())\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "237": {
  "name": "new_val",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "91",
  "column": "22",
  "context": ": new_val = str(value - 11)\n                else: new_val = str(value + 11)\n        else:\n            if t['word'].lower() in ",
  "context_lines": "                value = 10  # fallback\n            if value > 50:  new_val = str(value - 25)  # Year\n            else:  # Day of month\n                if value > 15: new_val = str(value - 11)\n                else: new_val = str(value + 11)\n        else:\n            if t['word'].lower() in MONTHS:\n                m_ind = MONTHS.index(t['word'].lower())\n                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "238": {
  "name": "m_ind",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "94",
  "column": "16",
  "context": "  if t['word'].lower() in MONTHS:\n                m_ind = MONTHS.index(t['word'].lower())\n                new_val = MONTHS[(m_ind + 6) % 12]",
  "context_lines": "                if value > 15: new_val = str(value - 11)\n                else: new_val = str(value + 11)\n        else:\n            if t['word'].lower() in MONTHS:\n                m_ind = MONTHS.index(t['word'].lower())\n                new_val = MONTHS[(m_ind + 6) % 12].title()\n            else:\n                # Give up\n                new_val = t['originalText']\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "239": {
  "name": "new_val",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "95",
  "column": "16",
  "context": "= MONTHS.index(t['word'].lower())\n                new_val = MONTHS[(m_ind + 6) % 12].title()\n            else:\n                # Give up\n      ",
  "context_lines": "                else: new_val = str(value + 11)\n        else:\n            if t['word'].lower() in MONTHS:\n                m_ind = MONTHS.index(t['word'].lower())\n                new_val = MONTHS[(m_ind + 6) % 12].title()\n            else:\n                # Give up\n                new_val = t['originalText']\n        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "240": {
  "name": "new_val",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "98",
  "column": "16",
  "context": "  else:\n                # Give up\n                new_val = t['originalText']\n        out_toks.append({'before': t['before'], 'o",
  "context_lines": "                m_ind = MONTHS.index(t['word'].lower())\n                new_val = MONTHS[(m_ind + 6) % 12].title()\n            else:\n                # Give up\n                new_val = t['originalText']\n        out_toks.append({'before': t['before'], 'originalText': new_val})\n    new_ans = rejoin(out_toks).strip()\n    if new_ans == a['text']: return None\n    return new_ans\n\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "241": {
  "name": "s",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "114",
  "column": "8",
  "context": "s):\n    def func(a, tokens, q, **kwargs):\n        s = a['text']\n        if s == s.upper() and s != s.lower():\n    ",
  "context_lines": "        return new_ans\n    return func\n\ndef ans_abbrev(new_ans):\n    def func(a, tokens, q, **kwargs):\n        s = a['text']\n        if s == s.upper() and s != s.lower():\n            return new_ans\n        return None\n    return func\n\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "        for t in tokens:\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "        s = a['text']\n",
   "        if s == s.upper() and s != s.lower():\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "242": {
  "name": "t",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "132",
  "column": "12",
  "context": "eterminer, **kwargs):\n        if end:\n            t = tokens[-1]\n        else:\n            t = tokens[0]\n        if",
  "context_lines": "def ans_pos(pos, new_ans, end=False, add_dt=False):\n    \"\"\"Returns a function that yields new_ans if the first/last token has |pos|.\"\"\"\n    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n        else:\n            t = tokens[0]\n        if t['pos'] != pos: return None\n        if add_dt and determiner:\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "        for t in tokens:\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "        s = a['text']\n",
   "        if s == s.upper() and s != s.lower():\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "            t = tokens[-1]\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "243": {
  "name": "t",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "134",
  "column": "12",
  "context": "         t = tokens[-1]\n        else:\n            t = tokens[0]\n        if t['pos'] != pos: return None\n        if",
  "context_lines": "    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n        else:\n            t = tokens[0]\n        if t['pos'] != pos: return None\n        if add_dt and determiner:\n            return '%s %s' % (determiner, new_ans)\n        return new_ans\n",
  "slicing": [
   "MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n",
   "    out_toks = []\n",
   "    seen_num = False\n",
   "    for t in tokens:\n",
   "        ner = t['ner']\n",
   "        pos = t['pos']\n",
   "        w = t['word']\n",
   "        out_tok = {'before': t['before']}\n",
   "        leftover = ''\n",
   "        dash_toks = w.split('-')\n",
   "        if len(dash_toks) > 1:\n",
   "            w = dash_toks[0]\n",
   "            leftover = '-'.join(dash_toks[1:])\n",
   "        value = None\n",
   "        if w != '%': \n",
   "                value = float(w.replace(',', ''))\n",
   "                    norm_ner = t['normalizedNER']\n",
   "                    if norm_ner[0] in ('%', '>', '<'):\n",
   "                        norm_ner = norm_ner[1:]\n",
   "                    value = float(norm_ner)\n",
   "        if not value and (\n",
   "                ner == 'NUMBER' or \n",
   "                (ner == 'PERCENT' and pos == 'CD')):\n",
   "            value = 10\n",
   "        if value:\n",
   "            if math.isinf(value) or math.isnan(value): value = 9001\n",
   "            seen_num = True\n",
   "            if w in ('thousand', 'million', 'billion', 'trillion'):\n",
   "                if w == 'thousand':\n",
   "                    new_val = 'million'\n",
   "                    new_val = 'thousand'\n",
   "                if value < 2500 and value > 1000:\n",
   "                    new_val = str(value - 75)\n",
   "                    if value == int(value):\n",
   "                        val_chars = list('%d' % value)\n",
   "                        val_chars = list('%g' % value)\n",
   "                    c = val_chars[0]\n",
   "                    for i in range(len(val_chars)):\n",
   "                        c = val_chars[i]\n",
   "                        if c >= '0' and c <= '9':\n",
   "                            val_chars[i] = str(max((int(c) + 5) % 10, 1))\n",
   "                    new_val = ''.join(val_chars)\n",
   "            if leftover:\n",
   "                new_val = '%s-%s' % (new_val, leftover)\n",
   "            out_tok['originalText'] = new_val\n",
   "            out_tok['originalText'] = t['originalText']\n",
   "        out_toks.append(out_tok)\n",
   "    if seen_num:\n",
   "        return rejoin(out_toks).strip()\n",
   "    out_toks = []\n",
   "    if not all(t['ner'] == 'DATE' for t in tokens):\n",
   "    for t in tokens:\n",
   "        if t['pos'] == 'CD' or t['word'].isdigit():\n",
   "                value = int(t['word'])\n",
   "                value = 10  # fallback\n",
   "            if value > 50:  new_val = str(value - 25)  # Year\n",
   "                if value > 15: new_val = str(value - 11)\n",
   "                else: new_val = str(value + 11)\n",
   "            if t['word'].lower() in MONTHS:\n",
   "                m_ind = MONTHS.index(t['word'].lower())\n",
   "                new_val = MONTHS[(m_ind + 6) % 12].title()\n",
   "                new_val = t['originalText']\n",
   "        out_toks.append({'before': t['before'], 'originalText': new_val})\n",
   "    new_ans = rejoin(out_toks).strip()\n",
   "    if new_ans == a['text']: return None\n",
   "    return new_ans\n",
   "        for t in tokens:\n",
   "            if t['ner'] != ner_tag: return None\n",
   "        return new_ans\n",
   "        s = a['text']\n",
   "        if s == s.upper() and s != s.lower():\n",
   "            return new_ans\n",
   "            return new_ans\n",
   "            t = tokens[-1]\n",
   "            t = tokens[0]\n",
   "        if t['pos'] != pos: return None\n",
   "            return '%s %s' % (determiner, new_ans)\n",
   "        return new_ans\n",
   "        return new_ans\n"
  ]
 },
 "244": {
  "name": "ANSWER_RULES",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "147",
  "column": "0",
  "context": "*kwargs):\n        return new_ans\n    return func\n\nANSWER_RULES = [\n        ('date', ans_date),\n        ('number', ans",
  "context_lines": "def ans_catch_all(new_ans):\n    def func(a, tokens, q, **kwargs):\n        return new_ans\n    return func\n\nANSWER_RULES = [\n        ('date', ans_date),\n        ('number', ans_number),\n        ('ner_person', ans_entity_full('PERSON', 'Jeff Dean')),\n        ('ner_location', ans_entity_full('LOCATION', 'Chicago')),\n",
  "slicing": "ANSWER_RULES = [\n"
 },
 "245": {
  "name": "STEMMER",
  "type": "nltk.stem.lancaster.LancasterStemmer",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "9",
  "column": "0",
  "context": "from nltk.stem.lancaster import LancasterStemmer\n\nSTEMMER = LancasterStemmer()\n\nPOS_TO_WORDNET = {\n        'NN': wn.NOUN,\n       ",
  "context_lines": "import nltk\nnltk.download('wordnet')\n\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.lancaster import LancasterStemmer\n\nSTEMMER = LancasterStemmer()\n\nPOS_TO_WORDNET = {\n        'NN': wn.NOUN,\n        'JJ': wn.ADJ,\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "246": {
  "name": "POS_TO_WORDNET",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "11",
  "column": "0",
  "context": "t LancasterStemmer\n\nSTEMMER = LancasterStemmer()\n\nPOS_TO_WORDNET = {\n        'NN': wn.NOUN,\n        'JJ': wn.ADJ,\n     ",
  "context_lines": "nltk.download('wordnet')\n\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.lancaster import LancasterStemmer\n\nSTEMMER = LancasterStemmer()\n\nPOS_TO_WORDNET = {\n        'NN': wn.NOUN,\n        'JJ': wn.ADJ,\n        'JJR': wn.ADJ,\n        'JJS': wn.ADJ,\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "247": {
  "name": "w",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "19",
  "column": "4",
  "context": "n.ADJ,\n}\n\ndef alter_special(token, **kwargs):\n    w = token['originalText']\n    if w in SPECIAL_ALTERATIONS:\n        return [S",
  "context_lines": "        'JJR': wn.ADJ,\n        'JJS': wn.ADJ,\n}\n\ndef alter_special(token, **kwargs):\n    w = token['originalText']\n    if w in SPECIAL_ALTERATIONS:\n        return [SPECIAL_ALTERATIONS[w]]\n    return None\n\ndef alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "248": {
  "name": "w",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "29",
  "column": "8",
  "context": "ZATION', 'MISC'):\n            return None\n        w = token['word'].lower()\n        if w in ('war'): return None\n        if w ",
  "context_lines": "    def func(token, nearby_word_dict=None, postag_dict=None, **kwargs):\n        if token['pos'] not in pos_list: return None\n        if is_ner and token['ner'] not in ('PERSON', 'LOCATION', 'ORGANIZATION', 'MISC'):\n            return None\n        w = token['word'].lower()\n        if w in ('war'): return None\n        if w not in nearby_word_dict: return None\n        new_words = []\n        w_stem = STEMMER.stem(w.replace('.', ''))\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "249": {
  "name": "new_words",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "32",
  "column": "8",
  "context": "if w not in nearby_word_dict: return None\n        new_words = []\n        w_stem = STEMMER.stem(w.replace('.', ''))\n",
  "context_lines": "            return None\n        w = token['word'].lower()\n        if w in ('war'): return None\n        if w not in nearby_word_dict: return None\n        new_words = []\n        w_stem = STEMMER.stem(w.replace('.', ''))\n        for x in nearby_word_dict[w][1:]:\n            new_word = x['word']\n            # Make sure words aren't too similar (e.g. same stem)\n",
  "slicing": [
   "        new_words = []\n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n"
  ]
 },
 "250": {
  "name": "w_stem",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "33",
  "column": "8",
  "context": "_dict: return None\n        new_words = []\n        w_stem = STEMMER.stem(w.replace('.', ''))\n        for x in nearby_word_dict[w][1:]:\n        ",
  "context_lines": "        w = token['word'].lower()\n        if w in ('war'): return None\n        if w not in nearby_word_dict: return None\n        new_words = []\n        w_stem = STEMMER.stem(w.replace('.', ''))\n        for x in nearby_word_dict[w][1:]:\n            new_word = x['word']\n            # Make sure words aren't too similar (e.g. same stem)\n            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "251": {
  "name": "new_word",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "35",
  "column": "12",
  "context": "    for x in nearby_word_dict[w][1:]:\n            new_word = x['word']\n            # Make sure words aren't too similar (",
  "context_lines": "        if w not in nearby_word_dict: return None\n        new_words = []\n        w_stem = STEMMER.stem(w.replace('.', ''))\n        for x in nearby_word_dict[w][1:]:\n            new_word = x['word']\n            # Make sure words aren't too similar (e.g. same stem)\n            new_stem = STEMMER.stem(new_word.replace('.', ''))\n            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n            if not ignore_pos:\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "252": {
  "name": "new_stem",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "37",
  "column": "12",
  "context": "s aren't too similar (e.g. same stem)\n            new_stem = STEMMER.stem(new_word.replace('.', ''))\n            if w_stem.startswith(new_stem) or new_",
  "context_lines": "        w_stem = STEMMER.stem(w.replace('.', ''))\n        for x in nearby_word_dict[w][1:]:\n            new_word = x['word']\n            # Make sure words aren't too similar (e.g. same stem)\n            new_stem = STEMMER.stem(new_word.replace('.', ''))\n            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n            if not ignore_pos:\n                # Check for POS tag match\n                if new_word not in postag_dict: continue\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "253": {
  "name": "new_postag",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "42",
  "column": "16",
  "context": "word not in postag_dict: continue\n                new_postag = postag_dict[new_word]\n                if new_postag != token['pos']: con",
  "context_lines": "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n            if not ignore_pos:\n                # Check for POS tag match\n                if new_word not in postag_dict: continue\n                new_postag = postag_dict[new_word]\n                if new_postag != token['pos']: continue \n            new_words.append(new_word)\n        return new_words\n    return func\n\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "254": {
  "name": "w",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "51",
  "column": "4",
  "context": "CATION', 'ORGANIZATION', 'MISC'): return None\n    w = token['word'].lower()\n    if w == token['word']: return None  # Only do ",
  "context_lines": "    return func\n\ndef alter_entity_glove(token, nearby_word_dict=None, **kwargs):\n    # NOTE: Deprecated\n    if token['ner'] not in ('PERSON', 'LOCATION', 'ORGANIZATION', 'MISC'): return None\n    w = token['word'].lower()\n    if w == token['word']: return None  # Only do capitalized words\n    if w not in nearby_word_dict: return None\n    new_words = []\n    for x in nearby_word_dict[w][1:3]:\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "255": {
  "name": "new_words",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "54",
  "column": "4",
  "context": "    if w not in nearby_word_dict: return None\n    new_words = []\n    for x in nearby_word_dict[w][1:3]:\n        if ",
  "context_lines": "    if token['ner'] not in ('PERSON', 'LOCATION', 'ORGANIZATION', 'MISC'): return None\n    w = token['word'].lower()\n    if w == token['word']: return None  # Only do capitalized words\n    if w not in nearby_word_dict: return None\n    new_words = []\n    for x in nearby_word_dict[w][1:3]:\n        if token['word'] == w.upper():\n            new_words.append(x['word'].upper())\n        else:\n",
  "slicing": [
   "    new_words = []\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n"
  ]
 },
 "256": {
  "name": "pos",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "63",
  "column": "4",
  "context": "ords\n\ndef alter_entity_type(token, **kwargs):\n    pos = token['pos']\n    ner = token['ner']\n    word = token['word']\n  ",
  "context_lines": "        else:\n            new_words.append(x['word'].title())\n    return new_words\n\ndef alter_entity_type(token, **kwargs):\n    pos = token['pos']\n    ner = token['ner']\n    word = token['word']\n    is_abbrev = word == word.upper() and not word == word.lower()\n    if token['pos'] not in (\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "257": {
  "name": "ner",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "64",
  "column": "4",
  "context": "type(token, **kwargs):\n    pos = token['pos']\n    ner = token['ner']\n    word = token['word']\n    is_abbrev = word == w",
  "context_lines": "            new_words.append(x['word'].title())\n    return new_words\n\ndef alter_entity_type(token, **kwargs):\n    pos = token['pos']\n    ner = token['ner']\n    word = token['word']\n    is_abbrev = word == word.upper() and not word == word.lower()\n    if token['pos'] not in (\n            'JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS', 'RB', 'RBR', 'RBS',\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "258": {
  "name": "word",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "65",
  "column": "4",
  "context": "    pos = token['pos']\n    ner = token['ner']\n    word = token['word']\n    is_abbrev = word == word.upper() and not word ",
  "context_lines": "    return new_words\n\ndef alter_entity_type(token, **kwargs):\n    pos = token['pos']\n    ner = token['ner']\n    word = token['word']\n    is_abbrev = word == word.upper() and not word == word.lower()\n    if token['pos'] not in (\n            'JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS', 'RB', 'RBR', 'RBS',\n            'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'):\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "259": {
  "name": "is_abbrev",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "66",
  "column": "4",
  "context": "  ner = token['ner']\n    word = token['word']\n    is_abbrev = word == word.upper() and not word == word.lower()\n    if token['pos'] not in (\n            'JJ', 'JJ",
  "context_lines": "def alter_entity_type(token, **kwargs):\n    pos = token['pos']\n    ner = token['ner']\n    word = token['word']\n    is_abbrev = word == word.upper() and not word == word.lower()\n    if token['pos'] not in (\n            'JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS', 'RB', 'RBR', 'RBS',\n            'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'):\n        # Don't alter non-content words\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "260": {
  "name": "w",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "90",
  "column": "4",
  "context": "ken['pos'] not in POS_TO_WORDNET: return None\n    w = token['word'].lower()\n    wn_pos = POS_TO_WORDNET[token['pos']]\n    syns",
  "context_lines": "        return ['Daleks']\n    return None\n\ndef alter_wordnet_antonyms(token, **kwargs):\n    if token['pos'] not in POS_TO_WORDNET: return None\n    w = token['word'].lower()\n    wn_pos = POS_TO_WORDNET[token['pos']]\n    synsets = wn.synsets(w, wn_pos)\n    if not synsets: return None\n    synset = synsets[0]\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "261": {
  "name": "wn_pos",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "91",
  "column": "4",
  "context": "ET: return None\n    w = token['word'].lower()\n    wn_pos = POS_TO_WORDNET[token['pos']]\n    synsets = wn.synsets(w, wn_pos)\n    if not syn",
  "context_lines": "    return None\n\ndef alter_wordnet_antonyms(token, **kwargs):\n    if token['pos'] not in POS_TO_WORDNET: return None\n    w = token['word'].lower()\n    wn_pos = POS_TO_WORDNET[token['pos']]\n    synsets = wn.synsets(w, wn_pos)\n    if not synsets: return None\n    synset = synsets[0]\n    antonyms = []\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "262": {
  "name": "synsets",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "92",
  "column": "4",
  "context": "r()\n    wn_pos = POS_TO_WORDNET[token['pos']]\n    synsets = wn.synsets(w, wn_pos)\n    if not synsets: return None\n    synset = synse",
  "context_lines": "def alter_wordnet_antonyms(token, **kwargs):\n    if token['pos'] not in POS_TO_WORDNET: return None\n    w = token['word'].lower()\n    wn_pos = POS_TO_WORDNET[token['pos']]\n    synsets = wn.synsets(w, wn_pos)\n    if not synsets: return None\n    synset = synsets[0]\n    antonyms = []\n    for lem in synset.lemmas():\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "263": {
  "name": "synset",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "94",
  "column": "4",
  "context": "ts(w, wn_pos)\n    if not synsets: return None\n    synset = synsets[0]\n    antonyms = []\n    for lem in synset.lemmas():\n",
  "context_lines": "    w = token['word'].lower()\n    wn_pos = POS_TO_WORDNET[token['pos']]\n    synsets = wn.synsets(w, wn_pos)\n    if not synsets: return None\n    synset = synsets[0]\n    antonyms = []\n    for lem in synset.lemmas():\n        if lem.antonyms():\n            for a in lem.antonyms():\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "264": {
  "name": "antonyms",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "95",
  "column": "4",
  "context": " synsets: return None\n    synset = synsets[0]\n    antonyms = []\n    for lem in synset.lemmas():\n        if lem.ant",
  "context_lines": "    wn_pos = POS_TO_WORDNET[token['pos']]\n    synsets = wn.synsets(w, wn_pos)\n    if not synsets: return None\n    synset = synsets[0]\n    antonyms = []\n    for lem in synset.lemmas():\n        if lem.antonyms():\n            for a in lem.antonyms():\n                new_word = a.name()\n",
  "slicing": [
   "    antonyms = []\n",
   "                antonyms.append(new_word)\n",
   "    return antonyms\n"
  ]
 },
 "265": {
  "name": "new_word",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "99",
  "column": "16",
  "context": "         for a in lem.antonyms():\n                new_word = a.name()\n                if '_' in a.name(): continue\n     ",
  "context_lines": "    antonyms = []\n    for lem in synset.lemmas():\n        if lem.antonyms():\n            for a in lem.antonyms():\n                new_word = a.name()\n                if '_' in a.name(): continue\n                antonyms.append(new_word)\n    return antonyms\n\nSPECIAL_ALTERATIONS = {\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n"
  ]
 },
 "266": {
  "name": "SPECIAL_ALTERATIONS",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "104",
  "column": "0",
  "context": "   antonyms.append(new_word)\n    return antonyms\n\nSPECIAL_ALTERATIONS = {\n        'States': 'Kingdom',\n        'US': 'UK',\n ",
  "context_lines": "                new_word = a.name()\n                if '_' in a.name(): continue\n                antonyms.append(new_word)\n    return antonyms\n\nSPECIAL_ALTERATIONS = {\n        'States': 'Kingdom',\n        'US': 'UK',\n        'U.S': 'U.K.',\n        'U.S.': 'U.K.',\n",
  "slicing": "SPECIAL_ALTERATIONS = {\n"
 },
 "267": {
  "name": "DO_NOT_ALTER",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "120",
  "column": "0",
  "context": "   'May': 'April',\n        'Peyton': 'Trevor',\n}\n\nDO_NOT_ALTER = ['many', 'such', 'few', 'much', 'other', 'same', 'general',\n                                'type', 'record', ",
  "context_lines": "        'lowest': 'highest',\n        'May': 'April',\n        'Peyton': 'Trevor',\n}\n\nDO_NOT_ALTER = ['many', 'such', 'few', 'much', 'other', 'same', 'general',\n                                'type', 'record', 'kind', 'sort', 'part', 'form', 'terms', 'use',\n                                'place', 'way', 'old', 'young', 'bowl', 'united', 'one',\n                                'likely', 'different', 'square', 'war', 'republic', 'doctor', 'color']\n\nBAD_ALTERATIONS = ['mx2004', 'planet', 'u.s.', 'Http://Www.Co.Mo.Md.Us']\n\n",
  "slicing": "DO_NOT_ALTER = ['many', 'such', 'few', 'much', 'other', 'same', 'general',\n"
 },
 "268": {
  "name": "BAD_ALTERATIONS",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "125",
  "column": "0",
  "context": " 'square', 'war', 'republic', 'doctor', 'color']\n\nBAD_ALTERATIONS = ['mx2004', 'planet', 'u.s.', 'Http://Www.Co.Mo.Md.Us']\n\nHIGH_CONF_ALTER_RULES = collections.OrderedDict([",
  "context_lines": "DO_NOT_ALTER = ['many', 'such', 'few', 'much', 'other', 'same', 'general',\n                                'type', 'record', 'kind', 'sort', 'part', 'form', 'terms', 'use',\n                                'place', 'way', 'old', 'young', 'bowl', 'united', 'one',\n                                'likely', 'different', 'square', 'war', 'republic', 'doctor', 'color']\n\nBAD_ALTERATIONS = ['mx2004', 'planet', 'u.s.', 'Http://Www.Co.Mo.Md.Us']\n\nHIGH_CONF_ALTER_RULES = collections.OrderedDict([\n        ('special', alter_special),\n        ('wn_antonyms', alter_wordnet_antonyms),\n",
  "slicing": "BAD_ALTERATIONS = ['mx2004', 'planet', 'u.s.', 'Http://Www.Co.Mo.Md.Us']\n"
 },
 "269": {
  "name": "HIGH_CONF_ALTER_RULES",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "127",
  "column": "0",
  "context": "04', 'planet', 'u.s.', 'Http://Www.Co.Mo.Md.Us']\n\nHIGH_CONF_ALTER_RULES = collections.OrderedDict([\n        ('special', alter_special),\n        ('wn_a",
  "context_lines": "                                'type', 'record', 'kind', 'sort', 'part', 'form', 'terms', 'use',\n                                'place', 'way', 'old', 'young', 'bowl', 'united', 'one',\n                                'likely', 'different', 'square', 'war', 'republic', 'doctor', 'color']\n\nBAD_ALTERATIONS = ['mx2004', 'planet', 'u.s.', 'Http://Www.Co.Mo.Md.Us']\n\nHIGH_CONF_ALTER_RULES = collections.OrderedDict([\n        ('special', alter_special),\n        ('wn_antonyms', alter_wordnet_antonyms),\n        ('nearbyNum', alter_nearby(['CD'], ignore_pos=True)),\n        ('nearbyProperNoun', alter_nearby(['NNP', 'NNPS'])),\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "def alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n",
   "        if token['pos'] not in pos_list: return None\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    antonyms = []\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n",
   "    return antonyms\n",
   "SPECIAL_ALTERATIONS = {\n",
   "HIGH_CONF_ALTER_RULES = collections.OrderedDict([\n",
   "ALL_ALTER_RULES = collections.OrderedDict(list(HIGH_CONF_ALTER_RULES.items()) + [\n"
  ]
 },
 "270": {
  "name": "ALL_ALTER_RULES",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "138",
  "column": "0",
  "context": "        #('entity_glove', alter_entity_glove),\n])\nALL_ALTER_RULES = collections.OrderedDict(list(HIGH_CONF_ALTER_RULES.items()) + [\n        ('nearbyAdj', alter_nearby(['JJ', 'JJR', '",
  "context_lines": "        ('nearbyEntityJJ', alter_nearby(['JJ', 'JJR', 'JJS'], is_ner=True)),\n        ('entityType', alter_entity_type),\n        #('entity_glove', alter_entity_glove),\n])\nALL_ALTER_RULES = collections.OrderedDict(list(HIGH_CONF_ALTER_RULES.items()) + [\n        ('nearbyAdj', alter_nearby(['JJ', 'JJR', 'JJS'])),\n        ('nearbyNoun', alter_nearby(['NN', 'NNS'])),\n        #('nearbyNoun', alter_nearby(['NN', 'NNS'], ignore_pos=True)),\n",
  "slicing": [
   "STEMMER = LancasterStemmer()\n",
   "POS_TO_WORDNET = {\n",
   "    w = token['originalText']\n",
   "    if w in SPECIAL_ALTERATIONS:\n",
   "        return [SPECIAL_ALTERATIONS[w]]\n",
   "def alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n",
   "        if token['pos'] not in pos_list: return None\n",
   "        w = token['word'].lower()\n",
   "        if w in ('war'): return None\n",
   "        if w not in nearby_word_dict: return None\n",
   "        new_words = []\n",
   "        w_stem = STEMMER.stem(w.replace('.', ''))\n",
   "        for x in nearby_word_dict[w][1:]:\n",
   "            new_word = x['word']\n",
   "            new_stem = STEMMER.stem(new_word.replace('.', ''))\n",
   "            if w_stem.startswith(new_stem) or new_stem.startswith(w_stem): continue\n",
   "                if new_word not in postag_dict: continue\n",
   "                new_postag = postag_dict[new_word]\n",
   "                if new_postag != token['pos']: continue \n",
   "            new_words.append(new_word)\n",
   "        return new_words\n",
   "    w = token['word'].lower()\n",
   "    if w == token['word']: return None  # Only do capitalized words\n",
   "    if w not in nearby_word_dict: return None\n",
   "    new_words = []\n",
   "    for x in nearby_word_dict[w][1:3]:\n",
   "        if token['word'] == w.upper():\n",
   "            new_words.append(x['word'].upper())\n",
   "            new_words.append(x['word'].title())\n",
   "    return new_words\n",
   "    pos = token['pos']\n",
   "    ner = token['ner']\n",
   "    word = token['word']\n",
   "    is_abbrev = word == word.upper() and not word == word.lower()\n",
   "    if ner == 'PERSON':\n",
   "    elif ner == 'LOCATION':\n",
   "    elif ner == 'ORGANIZATION':\n",
   "        if is_abbrev: return ['UNICEF']\n",
   "    elif ner == 'MISC':\n",
   "    elif ner == 'NNP':\n",
   "        if is_abbrev: return ['XKCD']\n",
   "    elif pos == 'NNPS':\n",
   "    if token['pos'] not in POS_TO_WORDNET: return None\n",
   "    w = token['word'].lower()\n",
   "    wn_pos = POS_TO_WORDNET[token['pos']]\n",
   "    synsets = wn.synsets(w, wn_pos)\n",
   "    if not synsets: return None\n",
   "    synset = synsets[0]\n",
   "    antonyms = []\n",
   "    for lem in synset.lemmas():\n",
   "        if lem.antonyms():\n",
   "            for a in lem.antonyms():\n",
   "                new_word = a.name()\n",
   "                if '_' in a.name(): continue\n",
   "                antonyms.append(new_word)\n",
   "    return antonyms\n",
   "SPECIAL_ALTERATIONS = {\n",
   "HIGH_CONF_ALTER_RULES = collections.OrderedDict([\n",
   "ALL_ALTER_RULES = collections.OrderedDict(list(HIGH_CONF_ALTER_RULES.items()) + [\n"
  ]
 },
 "271": {
  "name": "CONST_PARSE_MACROS",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "3",
  "column": "0",
  "context": "from pattern import en as patten\n\nCONST_PARSE_MACROS = {\n        '$Noun': '$NP/$NN/$NNS/$NNP/$NNPS',\n      ",
  "context_lines": "from pattern import en as patten\n\nCONST_PARSE_MACROS = {\n        '$Noun': '$NP/$NN/$NNS/$NNP/$NNPS',\n        '$Verb': '$VB/$VBD/$VBP/$VBZ',\n        '$Part': '$VBN/$VG',\n        '$Be': 'is/are/was/were',\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                if not phrase:\n",
   "                fmt_args.append(phrase)\n"
  ]
 },
 "272": {
  "name": "POS_TO_PATTERN",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "14",
  "column": "0",
  "context": "//www.clips.ua.ac.be/pages/pattern-en#conjugation\nPOS_TO_PATTERN = {\n        'vb': 'inf',  # Infinitive\n        'vbp': ",
  "context_lines": "        '$WHP': '$WHADJP/$WHADVP/$WHNP/$WHPP',\n}\n\n# Map to pattern.en aliases\n# http://www.clips.ua.ac.be/pages/pattern-en#conjugation\nPOS_TO_PATTERN = {\n        'vb': 'inf',  # Infinitive\n        'vbp': '1sg',  # non-3rd-person singular present\n        'vbz': '3sg',  # 3rd-person singular present\n        'vbg': 'part',  # gerund or present participle\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "273": {
  "name": "PATTERN_TENSES",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "23",
  "column": "0",
  "context": "e\n}\n# Tenses prioritized by likelihood of arising\nPATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n\ndef _check_match(node, pattern_tok):\n    if patte",
  "context_lines": "        'vbd': 'p',  # past\n        'vbn': 'ppart',  # past participle\n}\n# Tenses prioritized by likelihood of arising\nPATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n\ndef _check_match(node, pattern_tok):\n    if pattern_tok in CONST_PARSE_MACROS:\n        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "274": {
  "name": "pattern_tok",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "27",
  "column": "8",
  "context": "    if pattern_tok in CONST_PARSE_MACROS:\n        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n    if ':' in pattern_tok:\n        # ':' means you",
  "context_lines": "# Tenses prioritized by likelihood of arising\nPATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n\ndef _check_match(node, pattern_tok):\n    if pattern_tok in CONST_PARSE_MACROS:\n        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n    if ':' in pattern_tok:\n        # ':' means you match the LHS category and start with something on the right\n        lhs, rhs = pattern_tok.split(':')\n        match_lhs = _check_match(node, lhs)\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                if not phrase:\n",
   "                fmt_args.append(phrase)\n"
  ]
 },
 "275": {
  "name": "lhs",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "30",
  "column": "8",
  "context": "ory and start with something on the right\n        lhs, rhs = pattern_tok.split(':')\n        match_lhs = _check_match(node, lhs)\n      ",
  "context_lines": "    if pattern_tok in CONST_PARSE_MACROS:\n        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n    if ':' in pattern_tok:\n        # ':' means you match the LHS category and start with something on the right\n        lhs, rhs = pattern_tok.split(':')\n        match_lhs = _check_match(node, lhs)\n        if not match_lhs: return False\n        phrase = node.get_phrase().lower()\n        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                if not phrase:\n",
   "                fmt_args.append(phrase)\n"
  ]
 },
 "276": {
  "name": "rhs",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "30",
  "column": "13",
  "context": "nd start with something on the right\n        lhs, rhs = pattern_tok.split(':')\n        match_lhs = _check_match(node, lhs)\n      ",
  "context_lines": "    if pattern_tok in CONST_PARSE_MACROS:\n        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n    if ':' in pattern_tok:\n        # ':' means you match the LHS category and start with something on the right\n        lhs, rhs = pattern_tok.split(':')\n        match_lhs = _check_match(node, lhs)\n        if not match_lhs: return False\n        phrase = node.get_phrase().lower()\n        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                if not phrase:\n",
   "                fmt_args.append(phrase)\n"
  ]
 },
 "277": {
  "name": "match_lhs",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "31",
  "column": "8",
  "context": "        lhs, rhs = pattern_tok.split(':')\n        match_lhs = _check_match(node, lhs)\n        if not match_lhs: return False\n        phr",
  "context_lines": "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n    if ':' in pattern_tok:\n        # ':' means you match the LHS category and start with something on the right\n        lhs, rhs = pattern_tok.split(':')\n        match_lhs = _check_match(node, lhs)\n        if not match_lhs: return False\n        phrase = node.get_phrase().lower()\n        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n        return retval\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                if not phrase:\n",
   "                fmt_args.append(phrase)\n"
  ]
 },
 "278": {
  "name": "phrase",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "33",
  "column": "8",
  "context": "s)\n        if not match_lhs: return False\n        phrase = node.get_phrase().lower()\n        retval = any(phrase.startswith(w) for w in",
  "context_lines": "        # ':' means you match the LHS category and start with something on the right\n        lhs, rhs = pattern_tok.split(':')\n        match_lhs = _check_match(node, lhs)\n        if not match_lhs: return False\n        phrase = node.get_phrase().lower()\n        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n        return retval\n    elif '/' in pattern_tok:\n        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                if not phrase:\n",
   "                fmt_args.append(phrase)\n"
  ]
 },
 "279": {
  "name": "retval",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "34",
  "column": "8",
  "context": "       phrase = node.get_phrase().lower()\n        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n        return retval\n    elif '/' in pattern_tok:",
  "context_lines": "        lhs, rhs = pattern_tok.split(':')\n        match_lhs = _check_match(node, lhs)\n        if not match_lhs: return False\n        phrase = node.get_phrase().lower()\n        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n        return retval\n    elif '/' in pattern_tok:\n        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                if not phrase:\n",
   "                fmt_args.append(phrase)\n"
  ]
 },
 "280": {
  "name": "cur_tok",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "47",
  "column": "4",
  "context": "ck) == 0\n    if len(stack) == 0: return False\n    cur_tok = pattern_toks[len(matches)]\n    node = stack.pop()\n    # See if we match the c",
  "context_lines": "    if len(matches) == len(pattern_toks):\n        # We matched everything in the pattern; also need stack to be empty\n        return len(stack) == 0\n    if len(stack) == 0: return False\n    cur_tok = pattern_toks[len(matches)]\n    node = stack.pop()\n    # See if we match the current token at this level\n    is_match = _check_match(node, cur_tok)\n    if is_match:\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "281": {
  "name": "node",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "48",
  "column": "4",
  "context": "alse\n    cur_tok = pattern_toks[len(matches)]\n    node = stack.pop()\n    # See if we match the current token at this le",
  "context_lines": "        # We matched everything in the pattern; also need stack to be empty\n        return len(stack) == 0\n    if len(stack) == 0: return False\n    cur_tok = pattern_toks[len(matches)]\n    node = stack.pop()\n    # See if we match the current token at this level\n    is_match = _check_match(node, cur_tok)\n    if is_match:\n        cur_num_matches = len(matches)\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "282": {
  "name": "is_match",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "50",
  "column": "4",
  "context": "e if we match the current token at this level\n    is_match = _check_match(node, cur_tok)\n    if is_match:\n        cur_num_matches = len(mat",
  "context_lines": "    if len(stack) == 0: return False\n    cur_tok = pattern_toks[len(matches)]\n    node = stack.pop()\n    # See if we match the current token at this level\n    is_match = _check_match(node, cur_tok)\n    if is_match:\n        cur_num_matches = len(matches)\n        matches.append(node)\n        new_stack = list(stack)\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "283": {
  "name": "cur_num_matches",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "52",
  "column": "8",
  "context": "eck_match(node, cur_tok)\n    if is_match:\n        cur_num_matches = len(matches)\n        matches.append(node)\n        new_stack = l",
  "context_lines": "    node = stack.pop()\n    # See if we match the current token at this level\n    is_match = _check_match(node, cur_tok)\n    if is_match:\n        cur_num_matches = len(matches)\n        matches.append(node)\n        new_stack = list(stack)\n        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n        if success: return True\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "284": {
  "name": "new_stack",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "54",
  "column": "8",
  "context": "len(matches)\n        matches.append(node)\n        new_stack = list(stack)\n        success = _recursive_match_pattern(pattern",
  "context_lines": "    is_match = _check_match(node, cur_tok)\n    if is_match:\n        cur_num_matches = len(matches)\n        matches.append(node)\n        new_stack = list(stack)\n        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n        if success: return True\n        # Backtrack\n        while len(matches) > cur_num_matches:\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "285": {
  "name": "success",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "55",
  "column": "8",
  "context": "end(node)\n        new_stack = list(stack)\n        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n        if success: return True\n        # Backtrac",
  "context_lines": "    if is_match:\n        cur_num_matches = len(matches)\n        matches.append(node)\n        new_stack = list(stack)\n        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n        if success: return True\n        # Backtrack\n        while len(matches) > cur_num_matches:\n            matches.pop()\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "286": {
  "name": "pattern_toks",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "66",
  "column": "4",
  "context": "es)\n\ndef match_pattern(pattern, const_parse):\n    pattern_toks = pattern.split(' ')\n    whole_phrase = const_parse.get_phrase()\n    if",
  "context_lines": "    if not node.children: return False  # No children to recurse on, we failed\n    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n    return _recursive_match_pattern(pattern_toks, stack, matches)\n\ndef match_pattern(pattern, const_parse):\n    pattern_toks = pattern.split(' ')\n    whole_phrase = const_parse.get_phrase()\n    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n        # Match trailing punctuation as needed\n        pattern_toks.append(whole_phrase[-1])\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "287": {
  "name": "whole_phrase",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "67",
  "column": "4",
  "context": "parse):\n    pattern_toks = pattern.split(' ')\n    whole_phrase = const_parse.get_phrase()\n    if whole_phrase.endswith('?') or whole_phrase.",
  "context_lines": "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n    return _recursive_match_pattern(pattern_toks, stack, matches)\n\ndef match_pattern(pattern, const_parse):\n    pattern_toks = pattern.split(' ')\n    whole_phrase = const_parse.get_phrase()\n    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n        # Match trailing punctuation as needed\n        pattern_toks.append(whole_phrase[-1])\n    matches = []\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "288": {
  "name": "matches",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "71",
  "column": "4",
  "context": "        pattern_toks.append(whole_phrase[-1])\n    matches = []\n    success = _recursive_match_pattern(pattern_tok",
  "context_lines": "    whole_phrase = const_parse.get_phrase()\n    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n        # Match trailing punctuation as needed\n        pattern_toks.append(whole_phrase[-1])\n    matches = []\n    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n    if success:\n        return matches\n    else:\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "289": {
  "name": "success",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "72",
  "column": "4",
  "context": "oks.append(whole_phrase[-1])\n    matches = []\n    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n    if success:\n        return matches\n    else:\n ",
  "context_lines": "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n        # Match trailing punctuation as needed\n        pattern_toks.append(whole_phrase[-1])\n    matches = []\n    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n    if success:\n        return matches\n    else:\n        return None\n\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "290": {
  "name": "rule_list",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "79",
  "column": "4",
  "context": "\n\ndef run_postprocessing(s, rules, all_args):\n    rule_list = rules.split(',')\n    for rule in rule_list:\n        if rule == 'low",
  "context_lines": "        return matches\n    else:\n        return None\n\ndef run_postprocessing(s, rules, all_args):\n    rule_list = rules.split(',')\n    for rule in rule_list:\n        if rule == 'lower':\n            s = s.lower()\n        elif rule.startswith('tense-'):\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "291": {
  "name": "s",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "82",
  "column": "12",
  "context": "ule_list:\n        if rule == 'lower':\n            s = s.lower()\n        elif rule.startswith('tense-'):\n          ",
  "context_lines": "def run_postprocessing(s, rules, all_args):\n    rule_list = rules.split(',')\n    for rule in rule_list:\n        if rule == 'lower':\n            s = s.lower()\n        elif rule.startswith('tense-'):\n            ind = int(rule[6:])\n            orig_vb = all_args[ind]\n            tenses = patten.tenses(orig_vb)\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "292": {
  "name": "ind",
  "type": "int",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "84",
  "column": "12",
  "context": "      elif rule.startswith('tense-'):\n            ind = int(rule[6:])\n            orig_vb = all_args[ind]\n            te",
  "context_lines": "    for rule in rule_list:\n        if rule == 'lower':\n            s = s.lower()\n        elif rule.startswith('tense-'):\n            ind = int(rule[6:])\n            orig_vb = all_args[ind]\n            tenses = patten.tenses(orig_vb)\n            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n                if tense in tenses:\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "293": {
  "name": "orig_vb",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "85",
  "column": "12",
  "context": "e-'):\n            ind = int(rule[6:])\n            orig_vb = all_args[ind]\n            tenses = patten.tenses(orig_vb)\n      ",
  "context_lines": "        if rule == 'lower':\n            s = s.lower()\n        elif rule.startswith('tense-'):\n            ind = int(rule[6:])\n            orig_vb = all_args[ind]\n            tenses = patten.tenses(orig_vb)\n            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n                if tense in tenses:\n                    break\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "294": {
  "name": "tenses",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "86",
  "column": "12",
  "context": ")\n            orig_vb = all_args[ind]\n            tenses = patten.tenses(orig_vb)\n            for tense in PATTERN_TENSES:  # Priori",
  "context_lines": "            s = s.lower()\n        elif rule.startswith('tense-'):\n            ind = int(rule[6:])\n            orig_vb = all_args[ind]\n            tenses = patten.tenses(orig_vb)\n            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n                if tense in tenses:\n                    break\n            else:  # Default to first tense\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "295": {
  "name": "tense",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "91",
  "column": "16",
  "context": "  else:  # Default to first tense\n                tense = PATTERN_TENSES[0]\n            s = patten.conjugate(s, tense)\n       ",
  "context_lines": "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n                if tense in tenses:\n                    break\n            else:  # Default to first tense\n                tense = PATTERN_TENSES[0]\n            s = patten.conjugate(s, tense)\n        elif rule in POS_TO_PATTERN:\n            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n    return s\n\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "296": {
  "name": "s",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "92",
  "column": "12",
  "context": "            tense = PATTERN_TENSES[0]\n            s = patten.conjugate(s, tense)\n        elif rule in POS_TO_PATTERN:\n            s",
  "context_lines": "                if tense in tenses:\n                    break\n            else:  # Default to first tense\n                tense = PATTERN_TENSES[0]\n            s = patten.conjugate(s, tense)\n        elif rule in POS_TO_PATTERN:\n            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n    return s\n\ndef convert_whp(node, q, a, tokens, quiet=False):\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "297": {
  "name": "s",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "94",
  "column": "12",
  "context": "\n        elif rule in POS_TO_PATTERN:\n            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n    return s\n\ndef convert_whp(node, q, a, tokens, ",
  "context_lines": "            else:  # Default to first tense\n                tense = PATTERN_TENSES[0]\n            s = patten.conjugate(s, tense)\n        elif rule in POS_TO_PATTERN:\n            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n    return s\n\ndef convert_whp(node, q, a, tokens, quiet=False):\n    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n        # Apply WHP rules\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "298": {
  "name": "cur_phrase",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "100",
  "column": "8",
  "context": "ADVP', 'WHPP'):\n        # Apply WHP rules\n        cur_phrase = node.get_phrase()\n        cur_tokens = tokens[node.get_start_index()",
  "context_lines": "    return s\n\ndef convert_whp(node, q, a, tokens, quiet=False):\n    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n        # Apply WHP rules\n        cur_phrase = node.get_phrase()\n        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n        for r in WHP_RULES:\n            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n            if phrase:\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "299": {
  "name": "cur_tokens",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "101",
  "column": "8",
  "context": "es\n        cur_phrase = node.get_phrase()\n        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n        for r in WHP_RULES:\n            phrase = r",
  "context_lines": "def convert_whp(node, q, a, tokens, quiet=False):\n    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n        # Apply WHP rules\n        cur_phrase = node.get_phrase()\n        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n        for r in WHP_RULES:\n            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n            if phrase:\n                if not quiet:\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "300": {
  "name": "phrase",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "103",
  "column": "12",
  "context": "_index()]\n        for r in WHP_RULES:\n            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n            if phrase:\n                if not quie",
  "context_lines": "        # Apply WHP rules\n        cur_phrase = node.get_phrase()\n        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n        for r in WHP_RULES:\n            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n            if phrase:\n                if not quiet:\n                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n                return phrase\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "301": {
  "name": "s",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "113",
  "column": "4",
  "context": "\"Minor, general style fixes for questions.\"\"\"\n    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n    s = s.strip(' .')\n    if s[0] == s[0].lower():",
  "context_lines": "    return None\n\n### Rules for converting questions into declarative sentences\ndef fix_style(s):\n    \"\"\"Minor, general style fixes for questions.\"\"\"\n    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n    s = s.strip(' .')\n    if s[0] == s[0].lower():\n        s = s[0].upper() + s[1:]\n    return s + '.'\n\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "302": {
  "name": "s",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "114",
  "column": "4",
  "context": "# Delete question marks anywhere in sentence.\n    s = s.strip(' .')\n    if s[0] == s[0].lower():\n        s = s[0].uppe",
  "context_lines": "### Rules for converting questions into declarative sentences\ndef fix_style(s):\n    \"\"\"Minor, general style fixes for questions.\"\"\"\n    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n    s = s.strip(' .')\n    if s[0] == s[0].lower():\n        s = s[0].upper() + s[1:]\n    return s + '.'\n\nclass ConversionRule(object):\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "303": {
  "name": "s",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "116",
  "column": "8",
  "context": ".strip(' .')\n    if s[0] == s[0].lower():\n        s = s[0].upper() + s[1:]\n    return s + '.'\n\nclass ConversionRule(object):\n",
  "context_lines": "    \"\"\"Minor, general style fixes for questions.\"\"\"\n    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n    s = s.strip(' .')\n    if s[0] == s[0].lower():\n        s = s[0].upper() + s[1:]\n    return s + '.'\n\nclass ConversionRule(object):\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "304": {
  "name": "pattern_toks",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "136",
  "column": "8",
  "context": " const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n        match = match_pattern(self.in_pattern, con",
  "context_lines": "            self.postproc = postproc\n        else:\n            self.postproc = {}\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n        match = match_pattern(self.in_pattern, const_parse)\n        appended_clause = False\n        if not match:\n            # Try adding a PP at the beginning\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "305": {
  "name": "match",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "137",
  "column": "8",
  "context": "  # Don't care about trailing punctuation\n        match = match_pattern(self.in_pattern, const_parse)\n        appended_clause = False\n        if not mat",
  "context_lines": "        else:\n            self.postproc = {}\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n        match = match_pattern(self.in_pattern, const_parse)\n        appended_clause = False\n        if not match:\n            # Try adding a PP at the beginning\n            appended_clause = True\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "306": {
  "name": "appended_clause",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "138",
  "column": "8",
  "context": "tch_pattern(self.in_pattern, const_parse)\n        appended_clause = False\n        if not match:\n            # Try adding a P",
  "context_lines": "            self.postproc = {}\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n        match = match_pattern(self.in_pattern, const_parse)\n        appended_clause = False\n        if not match:\n            # Try adding a PP at the beginning\n            appended_clause = True\n            new_pattern = '$PP , ' + self.in_pattern\n",
  "slicing": [
   "        appended_clause = False\n",
   "        if appended_clause:\n",
   "        if appended_clause:\n"
  ]
 },
 "307": {
  "name": "appended_clause",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "141",
  "column": "12",
  "context": "   # Try adding a PP at the beginning\n            appended_clause = True\n            new_pattern = '$PP , ' + self.in_patte",
  "context_lines": "        match = match_pattern(self.in_pattern, const_parse)\n        appended_clause = False\n        if not match:\n            # Try adding a PP at the beginning\n            appended_clause = True\n            new_pattern = '$PP , ' + self.in_pattern\n            pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, const_parse)\n        if not match:\n",
  "slicing": [
   "            appended_clause = True\n",
   "        if appended_clause:\n",
   "        if appended_clause:\n"
  ]
 },
 "308": {
  "name": "new_pattern",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "142",
  "column": "12",
  "context": "ng\n            appended_clause = True\n            new_pattern = '$PP , ' + self.in_pattern\n            pattern_toks = new_pattern.split(' ')\n",
  "context_lines": "        appended_clause = False\n        if not match:\n            # Try adding a PP at the beginning\n            appended_clause = True\n            new_pattern = '$PP , ' + self.in_pattern\n            pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, const_parse)\n        if not match:\n            # Try adding an SBAR at the beginning\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "309": {
  "name": "pattern_toks",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "143",
  "column": "12",
  "context": "_pattern = '$PP , ' + self.in_pattern\n            pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, con",
  "context_lines": "        if not match:\n            # Try adding a PP at the beginning\n            appended_clause = True\n            new_pattern = '$PP , ' + self.in_pattern\n            pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, const_parse)\n        if not match:\n            # Try adding an SBAR at the beginning\n            new_pattern = '$SBAR , ' + self.in_pattern\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "310": {
  "name": "match",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "144",
  "column": "12",
  "context": "pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, const_parse)\n        if not match:\n            # Try adding an ",
  "context_lines": "            # Try adding a PP at the beginning\n            appended_clause = True\n            new_pattern = '$PP , ' + self.in_pattern\n            pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, const_parse)\n        if not match:\n            # Try adding an SBAR at the beginning\n            new_pattern = '$SBAR , ' + self.in_pattern\n            pattern_toks = new_pattern.split(' ')\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "311": {
  "name": "new_pattern",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "147",
  "column": "12",
  "context": "# Try adding an SBAR at the beginning\n            new_pattern = '$SBAR , ' + self.in_pattern\n            pattern_toks = new_pattern.split(' ')\n",
  "context_lines": "            pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, const_parse)\n        if not match:\n            # Try adding an SBAR at the beginning\n            new_pattern = '$SBAR , ' + self.in_pattern\n            pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, const_parse)\n        if not match: return None\n        appended_clause_match = None\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "312": {
  "name": "pattern_toks",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "148",
  "column": "12",
  "context": "attern = '$SBAR , ' + self.in_pattern\n            pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, con",
  "context_lines": "            match = match_pattern(new_pattern, const_parse)\n        if not match:\n            # Try adding an SBAR at the beginning\n            new_pattern = '$SBAR , ' + self.in_pattern\n            pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, const_parse)\n        if not match: return None\n        appended_clause_match = None\n        fmt_args = [a]\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "313": {
  "name": "match",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "149",
  "column": "12",
  "context": "pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, const_parse)\n        if not match: return None\n        appended",
  "context_lines": "        if not match:\n            # Try adding an SBAR at the beginning\n            new_pattern = '$SBAR , ' + self.in_pattern\n            pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, const_parse)\n        if not match: return None\n        appended_clause_match = None\n        fmt_args = [a]\n        for t, m in zip(pattern_toks, match):\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "314": {
  "name": "appended_clause_match",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "151",
  "column": "8",
  "context": "_parse)\n        if not match: return None\n        appended_clause_match = None\n        fmt_args = [a]\n        for t, m in zip(pat",
  "context_lines": "            new_pattern = '$SBAR , ' + self.in_pattern\n            pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, const_parse)\n        if not match: return None\n        appended_clause_match = None\n        fmt_args = [a]\n        for t, m in zip(pattern_toks, match):\n            if t.startswith('$') or '/' in t:\n                # First check if it's a WHP\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "315": {
  "name": "fmt_args",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "152",
  "column": "8",
  "context": "None\n        appended_clause_match = None\n        fmt_args = [a]\n        for t, m in zip(pattern_toks, match):\n    ",
  "context_lines": "            pattern_toks = new_pattern.split(' ')\n            match = match_pattern(new_pattern, const_parse)\n        if not match: return None\n        appended_clause_match = None\n        fmt_args = [a]\n        for t, m in zip(pattern_toks, match):\n            if t.startswith('$') or '/' in t:\n                # First check if it's a WHP\n                phrase = convert_whp(m, q, a, tokens)\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "316": {
  "name": "phrase",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "156",
  "column": "16",
  "context": "      # First check if it's a WHP\n                phrase = convert_whp(m, q, a, tokens)\n                if not phrase:\n                   ",
  "context_lines": "        fmt_args = [a]\n        for t, m in zip(pattern_toks, match):\n            if t.startswith('$') or '/' in t:\n                # First check if it's a WHP\n                phrase = convert_whp(m, q, a, tokens)\n                if not phrase:\n                    phrase = m.get_phrase()\n                fmt_args.append(phrase)\n        if appended_clause:\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "317": {
  "name": "phrase",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "158",
  "column": "20",
  "context": "               if not phrase:\n                    phrase = m.get_phrase()\n                fmt_args.append(phrase)\n        if",
  "context_lines": "            if t.startswith('$') or '/' in t:\n                # First check if it's a WHP\n                phrase = convert_whp(m, q, a, tokens)\n                if not phrase:\n                    phrase = m.get_phrase()\n                fmt_args.append(phrase)\n        if appended_clause:\n            appended_clause_match = fmt_args[1]\n            fmt_args = [a] + fmt_args[2:]\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "318": {
  "name": "appended_clause_match",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "161",
  "column": "12",
  "context": "d(phrase)\n        if appended_clause:\n            appended_clause_match = fmt_args[1]\n            fmt_args = [a] + fmt_args[2:]\n        ",
  "context_lines": "                if not phrase:\n                    phrase = m.get_phrase()\n                fmt_args.append(phrase)\n        if appended_clause:\n            appended_clause_match = fmt_args[1]\n            fmt_args = [a] + fmt_args[2:]\n        for i in range(len(fmt_args)):\n            if i in self.postproc:\n                # Run postprocessing filters\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "319": {
  "name": "fmt_args",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "162",
  "column": "12",
  "context": "  appended_clause_match = fmt_args[1]\n            fmt_args = [a] + fmt_args[2:]\n        for i in range(len(fmt_args)):\n           ",
  "context_lines": "                    phrase = m.get_phrase()\n                fmt_args.append(phrase)\n        if appended_clause:\n            appended_clause_match = fmt_args[1]\n            fmt_args = [a] + fmt_args[2:]\n        for i in range(len(fmt_args)):\n            if i in self.postproc:\n                # Run postprocessing filters\n                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "320": {
  "name": "output",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "167",
  "column": "8",
  "context": "(fmt_args[i], self.postproc[i], fmt_args)\n        output = self.gen_output(fmt_args)\n        if appended_clause:\n            output = a",
  "context_lines": "        for i in range(len(fmt_args)):\n            if i in self.postproc:\n                # Run postprocessing filters\n                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n        output = self.gen_output(fmt_args)\n        if appended_clause:\n            output = appended_clause_match + ', ' + output\n        if run_fix_style:\n            output = fix_style(output)\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "321": {
  "name": "output",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "169",
  "column": "12",
  "context": "fmt_args)\n        if appended_clause:\n            output = appended_clause_match + ', ' + output\n        if run_fix_style:\n            output = fix",
  "context_lines": "                # Run postprocessing filters\n                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n        output = self.gen_output(fmt_args)\n        if appended_clause:\n            output = appended_clause_match + ', ' + output\n        if run_fix_style:\n            output = fix_style(output)\n        return output\n\n\n    def gen_output(self, fmt_args):\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "322": {
  "name": "output",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "171",
  "column": "12",
  "context": " ' + output\n        if run_fix_style:\n            output = fix_style(output)\n        return output\n\n\n    def gen_output(self, f",
  "context_lines": "        output = self.gen_output(fmt_args)\n        if appended_clause:\n            output = appended_clause_match + ', ' + output\n        if run_fix_style:\n            output = fix_style(output)\n        return output\n\n\n    def gen_output(self, fmt_args):\n        \"\"\"By default, use self.out_pattern.  Can be overridden.\"\"\"\n        return self.out_pattern.format(*fmt_args)\n\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "323": {
  "name": "t_toks",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "188",
  "column": "8",
  "context": "tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_toks = q.rstrip('?.').split(' ')\n       ",
  "context_lines": "        self.replacement = replacement #unicode(replacement)\n        self.name = 'replace(%s)' % target\n        self.start = start\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_toks = q.rstrip('?.').split(' ')\n        replacement_text = self.replacement.format(a)\n        for i in range(len(q_toks)):\n            if self.start and i != 0: continue\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "324": {
  "name": "q_toks",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "189",
  "column": "8",
  "context": ":\n        t_toks = self.target.split(' ')\n        q_toks = q.rstrip('?.').split(' ')\n        replacement_text = self.replacement.format",
  "context_lines": "        self.name = 'replace(%s)' % target\n        self.start = start\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_toks = q.rstrip('?.').split(' ')\n        replacement_text = self.replacement.format(a)\n        for i in range(len(q_toks)):\n            if self.start and i != 0: continue\n            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "325": {
  "name": "replacement_text",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "190",
  "column": "8",
  "context": "       q_toks = q.rstrip('?.').split(' ')\n        replacement_text = self.replacement.format(a)\n        for i in range(len(q_toks)):\n            i",
  "context_lines": "        self.start = start\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_toks = q.rstrip('?.').split(' ')\n        replacement_text = self.replacement.format(a)\n        for i in range(len(q_toks)):\n            if self.start and i != 0: continue\n            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n                begin = q_toks[:i]\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "326": {
  "name": "begin",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "194",
  "column": "16",
  "context": "trip(',').lower() == self.target:\n                begin = q_toks[:i]\n                end = q_toks[i + len(t_toks):]\n   ",
  "context_lines": "        replacement_text = self.replacement.format(a)\n        for i in range(len(q_toks)):\n            if self.start and i != 0: continue\n            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n                begin = q_toks[:i]\n                end = q_toks[i + len(t_toks):]\n                output = ' '.join(begin + [replacement_text] + end)\n                if run_fix_style:\n                    output = fix_style(output)\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "327": {
  "name": "end",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "195",
  "column": "16",
  "context": "               begin = q_toks[:i]\n                end = q_toks[i + len(t_toks):]\n                output = ' '.join(begin + [replace",
  "context_lines": "        for i in range(len(q_toks)):\n            if self.start and i != 0: continue\n            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n                begin = q_toks[:i]\n                end = q_toks[i + len(t_toks):]\n                output = ' '.join(begin + [replacement_text] + end)\n                if run_fix_style:\n                    output = fix_style(output)\n                return output\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "328": {
  "name": "output",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "196",
  "column": "16",
  "context": "   end = q_toks[i + len(t_toks):]\n                output = ' '.join(begin + [replacement_text] + end)\n                if run_fix_style:\n                ",
  "context_lines": "            if self.start and i != 0: continue\n            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n                begin = q_toks[:i]\n                end = q_toks[i + len(t_toks):]\n                output = ' '.join(begin + [replacement_text] + end)\n                if run_fix_style:\n                    output = fix_style(output)\n                return output\n        return None\n\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "329": {
  "name": "output",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "198",
  "column": "20",
  "context": "            if run_fix_style:\n                    output = fix_style(output)\n                return output\n        return None\n",
  "context_lines": "                begin = q_toks[:i]\n                end = q_toks[i + len(t_toks):]\n                output = ' '.join(begin + [replacement_text] + end)\n                if run_fix_style:\n                    output = fix_style(output)\n                return output\n        return None\n\nclass FindWHPRule(ConversionRule):\n    \"\"\"A rule that looks for $WHP's from right to left and does replacements.\"\"\"\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "330": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "204",
  "column": "4",
  "context": " from right to left and does replacements.\"\"\"\n    name = 'FindWHP'\n    def _recursive_convert(self, node, q, a, token",
  "context_lines": "                return output\n        return None\n\nclass FindWHPRule(ConversionRule):\n    \"\"\"A rule that looks for $WHP's from right to left and does replacements.\"\"\"\n    name = 'FindWHP'\n    def _recursive_convert(self, node, q, a, tokens, found_whp):\n        if node.word:\n            return node.word, found_whp\n        if not found_whp:\n",
  "slicing": "    name = 'FindWHP'\n"
 },
 "331": {
  "name": "whp_phrase",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "209",
  "column": "12",
  "context": ", found_whp\n        if not found_whp:\n            whp_phrase = convert_whp(node, q, a, tokens)\n            if whp_phrase:\n                return ",
  "context_lines": "    def _recursive_convert(self, node, q, a, tokens, found_whp):\n        if node.word:\n            return node.word, found_whp\n        if not found_whp:\n            whp_phrase = convert_whp(node, q, a, tokens)\n            if whp_phrase:\n                return whp_phrase, True\n        child_phrases = []\n        for c in node.children[::-1]:\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "332": {
  "name": "child_phrases",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "212",
  "column": "8",
  "context": ":\n                return whp_phrase, True\n        child_phrases = []\n        for c in node.children[::-1]:\n            ",
  "context_lines": "        if not found_whp:\n            whp_phrase = convert_whp(node, q, a, tokens)\n            if whp_phrase:\n                return whp_phrase, True\n        child_phrases = []\n        for c in node.children[::-1]:\n            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n            child_phrases.append(c_phrase)\n        out_toks = []\n",
  "slicing": [
   "        child_phrases = []\n",
   "            child_phrases.append(c_phrase)\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n"
  ]
 },
 "333": {
  "name": "c_phrase",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "214",
  "column": "12",
  "context": "        for c in node.children[::-1]:\n            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n            child_phrases.append(c_phrase)\n       ",
  "context_lines": "            if whp_phrase:\n                return whp_phrase, True\n        child_phrases = []\n        for c in node.children[::-1]:\n            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n            child_phrases.append(c_phrase)\n        out_toks = []\n        for i, p in enumerate(child_phrases[::-1]):\n            if i == 0 or p.startswith(\"'\"):\n",
  "slicing": [
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n"
  ]
 },
 "334": {
  "name": "found_whp",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "214",
  "column": "22",
  "context": "r c in node.children[::-1]:\n            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n            child_phrases.append(c_phrase)\n       ",
  "context_lines": "            if whp_phrase:\n                return whp_phrase, True\n        child_phrases = []\n        for c in node.children[::-1]:\n            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n            child_phrases.append(c_phrase)\n        out_toks = []\n        for i, p in enumerate(child_phrases[::-1]):\n            if i == 0 or p.startswith(\"'\"):\n",
  "slicing": [
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        if found_whp:\n"
  ]
 },
 "335": {
  "name": "out_toks",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "216",
  "column": "8",
  "context": "           child_phrases.append(c_phrase)\n        out_toks = []\n        for i, p in enumerate(child_phrases[::-1])",
  "context_lines": "        child_phrases = []\n        for c in node.children[::-1]:\n            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n            child_phrases.append(c_phrase)\n        out_toks = []\n        for i, p in enumerate(child_phrases[::-1]):\n            if i == 0 or p.startswith(\"'\"):\n                out_toks.append(p)\n            else:\n",
  "slicing": [
   "        out_toks = []\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n"
  ]
 },
 "336": {
  "name": "out_phrase",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "225",
  "column": "8",
  "context": "tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n        if found_whp:\n            if run_fix_style",
  "context_lines": "            else:\n                out_toks.append(' ' + p)\n        return ''.join(out_toks), found_whp\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n        if found_whp:\n            if run_fix_style:\n                out_phrase = fix_style(out_phrase)\n            return out_phrase\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "337": {
  "name": "found_whp",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "225",
  "column": "20",
  "context": "t_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n        if found_whp:\n            if run_fix_style",
  "context_lines": "            else:\n                out_toks.append(' ' + p)\n        return ''.join(out_toks), found_whp\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n        if found_whp:\n            if run_fix_style:\n                out_phrase = fix_style(out_phrase)\n            return out_phrase\n",
  "slicing": [
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n"
  ]
 },
 "338": {
  "name": "out_phrase",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "228",
  "column": "16",
  "context": "hp:\n            if run_fix_style:\n                out_phrase = fix_style(out_phrase)\n            return out_phrase\n        return None\n",
  "context_lines": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n        if found_whp:\n            if run_fix_style:\n                out_phrase = fix_style(out_phrase)\n            return out_phrase\n        return None\n\nclass AnswerRule(ConversionRule):\n    \"\"\"Just return the answer.\"\"\"\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don'tests care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "339": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "234",
  "column": "4",
  "context": "rsionRule):\n    \"\"\"Just return the answer.\"\"\"\n    name = 'AnswerRule'\n    def convert(self, q, a, tokens, const_parse, r",
  "context_lines": "            return out_phrase\n        return None\n\nclass AnswerRule(ConversionRule):\n    \"\"\"Just return the answer.\"\"\"\n    name = 'AnswerRule'\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        return a\n\nCONVERSION_RULES = [\n        # Special rules\n",
  "slicing": "    name = 'AnswerRule'\n"
 },
 "340": {
  "name": "CONVERSION_RULES",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "238",
  "column": "0",
  "context": "nst_parse, run_fix_style=True):\n        return a\n\nCONVERSION_RULES = [\n        # Special rules\n        ConstituencyRule('",
  "context_lines": "    \"\"\"Just return the answer.\"\"\"\n    name = 'AnswerRule'\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        return a\n\nCONVERSION_RULES = [\n        # Special rules\n        ConstituencyRule('$WHP:what $Be $NP called that $VP', '{2} that {3} {1} called {1}'),\n\n        # What type of X\n        #ConstituencyRule(\"$WHP:what/which type/sort/kind/group of $NP/$Noun $Be $NP\", '{5} {4} a {1} {3}'),\n",
  "slicing": "CONVERSION_RULES = [\n"
 },
 "341": {
  "name": "WHP_RULES",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "296",
  "column": "0",
  "context": "Rules for going from WHP to an answer constituent\nWHP_RULES = [\n        # WHPP rules\n        ConstituencyRule('$IN",
  "context_lines": "                                         {1: 'lower'}),\n        FindWHPRule(),\n]\n\n# Rules for going from WHP to an answer constituent\nWHP_RULES = [\n        # WHPP rules\n        ConstituencyRule('$IN what/which type/sort/kind/group of $NP/$Noun', '{1} {0} {4}'),\n        ConstituencyRule('$IN what/which type/sort/kind/group of $NP/$Noun $PP', '{1} {0} {4} {5}'),\n        ConstituencyRule('$IN what/which $NP', '{1} the {3} of {0}'),\n",
  "slicing": "WHP_RULES = [\n"
 },
 "342": {
  "name": "logger",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "16",
  "column": "0",
  "context": "nerators.swag.utils import pairwise, postprocess\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@DatasetReader.register(\"activitynet_captions\")\n",
  "context_lines": "from allennlp.data.instance import Instance\nfrom allennlp.data.tokenizers import Tokenizer, WordTokenizer\nfrom allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n\nfrom adversarialnlp.generators.swag.utils import pairwise, postprocess\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n@DatasetReader.register(\"activitynet_captions\")\nclass ActivityNetCaptionsDatasetReader(DatasetReader):\n    r\"\"\" Reads ActivityNet Captions JSON files and creates a dataset suitable for crafting\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n"
  ]
 },
 "343": {
  "name": "json_data",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "67",
  "column": "12",
  "context": "ading instances from: %s\", file_path)\n            json_data = json.load(data_file)\n            for video_id, value in json_data.items",
  "context_lines": "    @overrides\n    def _read(self, file_path):\n        with open(cached_path(file_path), \"r\") as data_file:\n            logger.info(\"Reading instances from: %s\", file_path)\n            json_data = json.load(data_file)\n            for video_id, value in json_data.items():\n                sentences = [postprocess(unidecode(x.strip()))\n                             for x in value['sentences']]\n                for first_sentence, second_sentence in pairwise(sentences):\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        with open(cached_path(file_path), \"r\") as data_file:\n",
   "            logger.info(\"Reading instances from: %s\", file_path)\n",
   "            json_data = json.load(data_file)\n",
   "            for video_id, value in json_data.items():\n",
   "                sentences = [postprocess(unidecode(x.strip()))\n",
   "                             for x in value['sentences']]\n",
   "                for first_sentence, second_sentence in pairwise(sentences):\n",
   "                    yield self.text_to_instance(video_id, first_sentence, second_sentence)\n",
   "        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n",
   "        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n",
   "        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n",
   "        second_sentence_field = TextField(tokenized_second_sentence, self._token_indexers)\n",
   "        fields = {'video_id': MetadataField(video_id),\n",
   "                  'first_sentence': first_sentence_field,\n",
   "                  'second_sentence': second_sentence_field}\n",
   "        return Instance(fields)\n"
  ]
 },
 "344": {
  "name": "sentences",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "69",
  "column": "16",
  "context": "o_id, value in json_data.items():\n                sentences = [postprocess(unidecode(x.strip()))\n                             for x in value['sente",
  "context_lines": "        with open(cached_path(file_path), \"r\") as data_file:\n            logger.info(\"Reading instances from: %s\", file_path)\n            json_data = json.load(data_file)\n            for video_id, value in json_data.items():\n                sentences = [postprocess(unidecode(x.strip()))\n                             for x in value['sentences']]\n                for first_sentence, second_sentence in pairwise(sentences):\n                    yield self.text_to_instance(video_id, first_sentence, second_sentence)\n\n    @overrides\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        with open(cached_path(file_path), \"r\") as data_file:\n",
   "            logger.info(\"Reading instances from: %s\", file_path)\n",
   "            json_data = json.load(data_file)\n",
   "            for video_id, value in json_data.items():\n",
   "                sentences = [postprocess(unidecode(x.strip()))\n",
   "                             for x in value['sentences']]\n",
   "                for first_sentence, second_sentence in pairwise(sentences):\n",
   "                    yield self.text_to_instance(video_id, first_sentence, second_sentence)\n",
   "        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n",
   "        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n",
   "        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n",
   "        second_sentence_field = TextField(tokenized_second_sentence, self._token_indexers)\n",
   "        fields = {'video_id': MetadataField(video_id),\n",
   "                  'first_sentence': first_sentence_field,\n",
   "                  'second_sentence': second_sentence_field}\n",
   "        return Instance(fields)\n"
  ]
 },
 "345": {
  "name": "tokenized_first_sentence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "80",
  "column": "8",
  "context": "       # pylint: disable=arguments-differ\n        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n        tokenized_second_sentence = self._tokenize",
  "context_lines": "                         video_id: str,\n                         first_sentence: str,\n                         second_sentence: str) -> Instance:  # type: ignore\n        # pylint: disable=arguments-differ\n        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n        second_sentence_field = TextField(tokenized_second_sentence, self._token_indexers)\n        fields = {'video_id': MetadataField(video_id),\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        with open(cached_path(file_path), \"r\") as data_file:\n",
   "            logger.info(\"Reading instances from: %s\", file_path)\n",
   "            json_data = json.load(data_file)\n",
   "            for video_id, value in json_data.items():\n",
   "                sentences = [postprocess(unidecode(x.strip()))\n",
   "                             for x in value['sentences']]\n",
   "                for first_sentence, second_sentence in pairwise(sentences):\n",
   "                    yield self.text_to_instance(video_id, first_sentence, second_sentence)\n",
   "        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n",
   "        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n",
   "        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n",
   "        second_sentence_field = TextField(tokenized_second_sentence, self._token_indexers)\n",
   "        fields = {'video_id': MetadataField(video_id),\n",
   "                  'first_sentence': first_sentence_field,\n",
   "                  'second_sentence': second_sentence_field}\n",
   "        return Instance(fields)\n"
  ]
 },
 "346": {
  "name": "tokenized_second_sentence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "81",
  "column": "8",
  "context": " self._tokenizer.tokenize(first_sentence)\n        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n        first_sentence_field = TextField(tokenized",
  "context_lines": "                         first_sentence: str,\n                         second_sentence: str) -> Instance:  # type: ignore\n        # pylint: disable=arguments-differ\n        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n        second_sentence_field = TextField(tokenized_second_sentence, self._token_indexers)\n        fields = {'video_id': MetadataField(video_id),\n                  'first_sentence': first_sentence_field,\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        with open(cached_path(file_path), \"r\") as data_file:\n",
   "            logger.info(\"Reading instances from: %s\", file_path)\n",
   "            json_data = json.load(data_file)\n",
   "            for video_id, value in json_data.items():\n",
   "                sentences = [postprocess(unidecode(x.strip()))\n",
   "                             for x in value['sentences']]\n",
   "                for first_sentence, second_sentence in pairwise(sentences):\n",
   "                    yield self.text_to_instance(video_id, first_sentence, second_sentence)\n",
   "        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n",
   "        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n",
   "        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n",
   "        second_sentence_field = TextField(tokenized_second_sentence, self._token_indexers)\n",
   "        fields = {'video_id': MetadataField(video_id),\n",
   "                  'first_sentence': first_sentence_field,\n",
   "                  'second_sentence': second_sentence_field}\n",
   "        return Instance(fields)\n"
  ]
 },
 "347": {
  "name": "first_sentence_field",
  "type": "allennlp.data.fields.TextField",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "82",
  "column": "8",
  "context": "self._tokenizer.tokenize(second_sentence)\n        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n        second_sentence_field = TextField(tokenize",
  "context_lines": "                         second_sentence: str) -> Instance:  # type: ignore\n        # pylint: disable=arguments-differ\n        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n        second_sentence_field = TextField(tokenized_second_sentence, self._token_indexers)\n        fields = {'video_id': MetadataField(video_id),\n                  'first_sentence': first_sentence_field,\n                  'second_sentence': second_sentence_field}\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        with open(cached_path(file_path), \"r\") as data_file:\n",
   "            logger.info(\"Reading instances from: %s\", file_path)\n",
   "            json_data = json.load(data_file)\n",
   "            for video_id, value in json_data.items():\n",
   "                sentences = [postprocess(unidecode(x.strip()))\n",
   "                             for x in value['sentences']]\n",
   "                for first_sentence, second_sentence in pairwise(sentences):\n",
   "                    yield self.text_to_instance(video_id, first_sentence, second_sentence)\n",
   "        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n",
   "        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n",
   "        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n",
   "        second_sentence_field = TextField(tokenized_second_sentence, self._token_indexers)\n",
   "        fields = {'video_id': MetadataField(video_id),\n",
   "                  'first_sentence': first_sentence_field,\n",
   "                  'second_sentence': second_sentence_field}\n",
   "        return Instance(fields)\n"
  ]
 },
 "348": {
  "name": "second_sentence_field",
  "type": "allennlp.data.fields.TextField",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "83",
  "column": "8",
  "context": "zed_first_sentence, self._token_indexers)\n        second_sentence_field = TextField(tokenized_second_sentence, self._token_indexers)\n        fields = {'video_id': MetadataField(video_",
  "context_lines": "        # pylint: disable=arguments-differ\n        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n        second_sentence_field = TextField(tokenized_second_sentence, self._token_indexers)\n        fields = {'video_id': MetadataField(video_id),\n                  'first_sentence': first_sentence_field,\n                  'second_sentence': second_sentence_field}\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        with open(cached_path(file_path), \"r\") as data_file:\n",
   "            logger.info(\"Reading instances from: %s\", file_path)\n",
   "            json_data = json.load(data_file)\n",
   "            for video_id, value in json_data.items():\n",
   "                sentences = [postprocess(unidecode(x.strip()))\n",
   "                             for x in value['sentences']]\n",
   "                for first_sentence, second_sentence in pairwise(sentences):\n",
   "                    yield self.text_to_instance(video_id, first_sentence, second_sentence)\n",
   "        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n",
   "        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n",
   "        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n",
   "        second_sentence_field = TextField(tokenized_second_sentence, self._token_indexers)\n",
   "        fields = {'video_id': MetadataField(video_id),\n",
   "                  'first_sentence': first_sentence_field,\n",
   "                  'second_sentence': second_sentence_field}\n",
   "        return Instance(fields)\n"
  ]
 },
 "349": {
  "name": "fields",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "84",
  "column": "8",
  "context": "ed_second_sentence, self._token_indexers)\n        fields = {'video_id': MetadataField(video_id),\n                  'first_sentence': first_sentence",
  "context_lines": "        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n        second_sentence_field = TextField(tokenized_second_sentence, self._token_indexers)\n        fields = {'video_id': MetadataField(video_id),\n                  'first_sentence': first_sentence_field,\n                  'second_sentence': second_sentence_field}\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        with open(cached_path(file_path), \"r\") as data_file:\n",
   "            logger.info(\"Reading instances from: %s\", file_path)\n",
   "            json_data = json.load(data_file)\n",
   "            for video_id, value in json_data.items():\n",
   "                sentences = [postprocess(unidecode(x.strip()))\n",
   "                             for x in value['sentences']]\n",
   "                for first_sentence, second_sentence in pairwise(sentences):\n",
   "                    yield self.text_to_instance(video_id, first_sentence, second_sentence)\n",
   "        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n",
   "        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n",
   "        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n",
   "        second_sentence_field = TextField(tokenized_second_sentence, self._token_indexers)\n",
   "        fields = {'video_id': MetadataField(video_id),\n",
   "                  'first_sentence': first_sentence_field,\n",
   "                  'second_sentence': second_sentence_field}\n",
   "        return Instance(fields)\n"
  ]
 },
 "350": {
  "name": "dup_set",
  "type": "set",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "30",
  "column": "4",
  "context": "\n    :param generations:\n    :return:\n    \"\"\"\n    dup_set = set()\n    unique_idx = []\n    for i, gen_i in enumerate(",
  "context_lines": "    to the good ones\n    :param generations:\n    :return:\n    \"\"\"\n    dup_set = set()\n    unique_idx = []\n    for i, gen_i in enumerate(generations):\n        gen_i_str = ' '.join(gen_i)\n        if gen_i_str not in dup_set:\n",
  "slicing": [
   "    dup_set = set()\n",
   "        if gen_i_str not in dup_set:\n",
   "            dup_set.add(gen_i_str)\n"
  ]
 },
 "351": {
  "name": "unique_idx",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "31",
  "column": "4",
  "context": "ons:\n    :return:\n    \"\"\"\n    dup_set = set()\n    unique_idx = []\n    for i, gen_i in enumerate(generations):\n      ",
  "context_lines": "    :param generations:\n    :return:\n    \"\"\"\n    dup_set = set()\n    unique_idx = []\n    for i, gen_i in enumerate(generations):\n        gen_i_str = ' '.join(gen_i)\n        if gen_i_str not in dup_set:\n            unique_idx.append(i)\n",
  "slicing": [
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        return output_sequence, final_state_tuple\n",
   "                generation_scores[i, j] = v\n"
  ]
 },
 "352": {
  "name": "gen_i_str",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "33",
  "column": "8",
  "context": "  for i, gen_i in enumerate(generations):\n        gen_i_str = ' '.join(gen_i)\n        if gen_i_str not in dup_set:\n            u",
  "context_lines": "    \"\"\"\n    dup_set = set()\n    unique_idx = []\n    for i, gen_i in enumerate(generations):\n        gen_i_str = ' '.join(gen_i)\n        if gen_i_str not in dup_set:\n            unique_idx.append(i)\n            dup_set.add(gen_i_str)\n    return [generations[i] for i in unique_idx], np.array(unique_idx)\n\n\n",
  "slicing": [
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            dup_set.add(gen_i_str)\n"
  ]
 },
 "353": {
  "name": "layers",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "85",
  "column": "8",
  "context": "ize\n        self.num_layers = num_layers\n\n        layers = []\n        lstm_input_size = input_size\n        for l",
  "context_lines": "        # Required to be wrapped with a :class:`PytorchSeq2SeqWrapper`.\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        layers = []\n        lstm_input_size = input_size\n        for layer_index in range(num_layers):\n            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n                                  recurrent_dropout_probability=recurrent_dropout_probability,\n",
  "slicing": [
   "        layers = []\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n"
  ]
 },
 "354": {
  "name": "lstm_input_size",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "86",
  "column": "8",
  "context": "_layers = num_layers\n\n        layers = []\n        lstm_input_size = input_size\n        for layer_index in range(num_layers):\n    ",
  "context_lines": "        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        layers = []\n        lstm_input_size = input_size\n        for layer_index in range(num_layers):\n            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n                                  recurrent_dropout_probability=recurrent_dropout_probability,\n                                  use_highway=use_highway,\n",
  "slicing": [
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        return output_sequence, final_state_tuple\n",
   "                generation_scores[i, j] = v\n"
  ]
 },
 "355": {
  "name": "layer",
  "type": "allennlp.modules.augmented_lstm.AugmentedLstm",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "88",
  "column": "12",
  "context": "for layer_index in range(num_layers):\n            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n                                  recurrent_dropou",
  "context_lines": "        self.num_layers = num_layers\n\n        layers = []\n        lstm_input_size = input_size\n        for layer_index in range(num_layers):\n            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n                                  recurrent_dropout_probability=recurrent_dropout_probability,\n                                  use_highway=use_highway,\n                                  use_input_projection_bias=use_input_projection_bias)\n            lstm_input_size = hidden_size\n",
  "slicing": [
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        return output_sequence, final_state_tuple\n",
   "                generation_scores[i, j] = v\n"
  ]
 },
 "356": {
  "name": "lstm_input_size",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "92",
  "column": "12",
  "context": "ction_bias=use_input_projection_bias)\n            lstm_input_size = hidden_size\n            self.add_module('layer_{}'.format(laye",
  "context_lines": "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n                                  recurrent_dropout_probability=recurrent_dropout_probability,\n                                  use_highway=use_highway,\n                                  use_input_projection_bias=use_input_projection_bias)\n            lstm_input_size = hidden_size\n            self.add_module('layer_{}'.format(layer_index), layer)\n            layers.append(layer)\n        self.lstm_layers = layers\n\n    def forward(self,  # pylint: disable=arguments-differ\n",
  "slicing": [
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        return output_sequence, final_state_tuple\n",
   "                generation_scores[i, j] = v\n"
  ]
 },
 "357": {
  "name": "hidden_states",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "118",
  "column": "12",
  "context": "    \"\"\"\n        if not initial_state:\n            hidden_states = [None] * len(self.lstm_layers)\n        elif initial_state[0].size()[0] != len(sel",
  "context_lines": "            The per-layer final (state, memory) states of the LSTM, each with shape\n            (num_layers, batch_size, hidden_size).\n        \"\"\"\n        if not initial_state:\n            hidden_states = [None] * len(self.lstm_layers)\n        elif initial_state[0].size()[0] != len(self.lstm_layers):\n            raise ConfigurationError(\"Initial states were passed to forward() but the number of \"\n                                     \"initial states does not match the number of layers.\")\n        else:\n",
  "slicing": [
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        return output_sequence, final_state_tuple\n",
   "                generation_scores[i, j] = v\n"
  ]
 },
 "358": {
  "name": "output_sequence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "126",
  "column": "8",
  "context": "          initial_state[1].split(1, 0)))\n\n        output_sequence = inputs\n        final_states = []\n        for i, state in ",
  "context_lines": "                                     \"initial states does not match the number of layers.\")\n        else:\n            hidden_states = list(zip(initial_state[0].split(1, 0),\n                                     initial_state[1].split(1, 0)))\n\n        output_sequence = inputs\n        final_states = []\n        for i, state in enumerate(hidden_states):\n            layer = getattr(self, 'layer_{}'.format(i))\n            # The state is duplicated to mirror the Pytorch API for LSTMs.\n",
  "slicing": [
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        return output_sequence, final_state_tuple\n",
   "                generation_scores[i, j] = v\n"
  ]
 },
 "359": {
  "name": "final_states",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "127",
  "column": "8",
  "context": "1, 0)))\n\n        output_sequence = inputs\n        final_states = []\n        for i, state in enumerate(hidden_states):\n",
  "context_lines": "        else:\n            hidden_states = list(zip(initial_state[0].split(1, 0),\n                                     initial_state[1].split(1, 0)))\n\n        output_sequence = inputs\n        final_states = []\n        for i, state in enumerate(hidden_states):\n            layer = getattr(self, 'layer_{}'.format(i))\n            # The state is duplicated to mirror the Pytorch API for LSTMs.\n            output_sequence, final_state = layer(output_sequence, state)\n",
  "slicing": [
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "                generation_scores[i, j] = v\n"
  ]
 },
 "360": {
  "name": "layer",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "129",
  "column": "12",
  "context": "i, state in enumerate(hidden_states):\n            layer = getattr(self, 'layer_{}'.format(i))\n            # The state is duplicated to mirror th",
  "context_lines": "                                     initial_state[1].split(1, 0)))\n\n        output_sequence = inputs\n        final_states = []\n        for i, state in enumerate(hidden_states):\n            layer = getattr(self, 'layer_{}'.format(i))\n            # The state is duplicated to mirror the Pytorch API for LSTMs.\n            output_sequence, final_state = layer(output_sequence, state)\n            final_states.append(final_state)\n\n        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
  "slicing": [
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        return output_sequence, final_state_tuple\n",
   "                generation_scores[i, j] = v\n"
  ]
 },
 "361": {
  "name": "output_sequence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "131",
  "column": "12",
  "context": " to mirror the Pytorch API for LSTMs.\n            output_sequence, final_state = layer(output_sequence, state)\n            final_states.append(final_state)\n\n    ",
  "context_lines": "        final_states = []\n        for i, state in enumerate(hidden_states):\n            layer = getattr(self, 'layer_{}'.format(i))\n            # The state is duplicated to mirror the Pytorch API for LSTMs.\n            output_sequence, final_state = layer(output_sequence, state)\n            final_states.append(final_state)\n\n        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n        return output_sequence, final_state_tuple\n\n\nclass SimpleBiLM(torch.nn.Module):\n",
  "slicing": [
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        return output_sequence, final_state_tuple\n"
  ]
 },
 "362": {
  "name": "final_state",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "131",
  "column": "29",
  "context": "torch API for LSTMs.\n            output_sequence, final_state = layer(output_sequence, state)\n            final_states.append(final_state)\n\n    ",
  "context_lines": "        final_states = []\n        for i, state in enumerate(hidden_states):\n            layer = getattr(self, 'layer_{}'.format(i))\n            # The state is duplicated to mirror the Pytorch API for LSTMs.\n            output_sequence, final_state = layer(output_sequence, state)\n            final_states.append(final_state)\n\n        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n        return output_sequence, final_state_tuple\n\n\nclass SimpleBiLM(torch.nn.Module):\n",
  "slicing": [
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n"
  ]
 },
 "363": {
  "name": "final_state_tuple",
  "type": "tuple",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "134",
  "column": "8",
  "context": "        final_states.append(final_state)\n\n        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n        return output_sequence, final_state_tuple\n",
  "context_lines": "            layer = getattr(self, 'layer_{}'.format(i))\n            # The state is duplicated to mirror the Pytorch API for LSTMs.\n            output_sequence, final_state = layer(output_sequence, state)\n            final_states.append(final_state)\n\n        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n        return output_sequence, final_state_tuple\n\n\nclass SimpleBiLM(torch.nn.Module):\n    def __init__(self,\n                 vocab: Vocabulary,\n",
  "slicing": [
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "                generation_scores[i, j] = v\n"
  ]
 },
 "364": {
  "name": "hidden_states",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "123",
  "column": "12",
  "context": "the number of layers.\")\n        else:\n            hidden_states = list(zip(initial_state[0].split(1, 0),\n                                     initial_state",
  "context_lines": "        elif initial_state[0].size()[0] != len(self.lstm_layers):\n            raise ConfigurationError(\"Initial states were passed to forward() but the number of \"\n                                     \"initial states does not match the number of layers.\")\n        else:\n            hidden_states = list(zip(initial_state[0].split(1, 0),\n                                     initial_state[1].split(1, 0)))\n\n        output_sequence = inputs\n        final_states = []\n        for i, state in enumerate(hidden_states):\n",
  "slicing": [
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        return output_sequence, final_state_tuple\n",
   "                generation_scores[i, j] = v\n"
  ]
 },
 "365": {
  "name": "batch",
  "type": "allennlp.data.dataset.Batch",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "200",
  "column": "8",
  "context": "nsor of padded character ids.\n        \"\"\"\n        batch = Batch([Instance(\n            {'story': TextField([Token('@@bos@@')]",
  "context_lines": "        Simple wrapper around _elmo_batch_to_ids\n        :param batch: A list of tokenized sentences.\n        :return: A tensor of padded character ids.\n        \"\"\"\n        batch = Batch([Instance(\n            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n                                token_indexers={\n                                    'tokens': SingleIdTokenIndexer(namespace='tokens', lowercase_tokens=True)})})\n            for story in stories_tokenized])\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "366": {
  "name": "words",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "206",
  "column": "8",
  "context": "        batch.index_instances(self.vocab)\n        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n        return words\n\n    def conditional_generati",
  "context_lines": "                                token_indexers={\n                                    'tokens': SingleIdTokenIndexer(namespace='tokens', lowercase_tokens=True)})})\n            for story in stories_tokenized])\n        batch.index_instances(self.vocab)\n        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n        return words\n\n    def conditional_generation(self, context: List[str], gt_completion: List[str],\n                               batch_size: int = 128, max_gen_length: int = 25,\n                               same_length_as_gt: bool = False, first_is_gold: bool = False):\n",
  "slicing": [
   "        words = []\n",
   "                words.append(token['originalText'])\n",
   "        return list(zip(words, ner_tags))\n"
  ]
 },
 "367": {
  "name": "log_probs",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "224",
  "column": "8",
  "context": "(layer_index, batch_size, fwd hidden dim)\n        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n                                use_reverse=False,",
  "context_lines": "        :return:\n        \"\"\"\n        # Forward condition on context, then repeat to be the right batch size:\n        #  (layer_index, batch_size, fwd hidden dim)\n        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n                                use_reverse=False, compute_logprobs=True)\n        forward_logprobs = log_probs['forward_logprobs']\n        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n        # Each item will be (token, score)\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "368": {
  "name": "forward_logprobs",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "226",
  "column": "8",
  "context": "use_reverse=False, compute_logprobs=True)\n        forward_logprobs = log_probs['forward_logprobs']\n        self.forward_lm._states = tuple(x.repeat(1",
  "context_lines": "        # Forward condition on context, then repeat to be the right batch size:\n        #  (layer_index, batch_size, fwd hidden dim)\n        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n                                use_reverse=False, compute_logprobs=True)\n        forward_logprobs = log_probs['forward_logprobs']\n        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n        # Each item will be (token, score)\n        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n        mask = forward_logprobs.new(batch_size).long().fill_(1)\n\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "369": {
  "name": "generations",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "229",
  "column": "8",
  "context": "       # Each item will be (token, score)\n        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n        mask = forward_logprobs.new(batch_size).lo",
  "context_lines": "                                use_reverse=False, compute_logprobs=True)\n        forward_logprobs = log_probs['forward_logprobs']\n        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n        # Each item will be (token, score)\n        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n        mask = forward_logprobs.new(batch_size).long().fill_(1)\n\n        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n                                        max_gen_length - len(gt_completion))]\n\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "370": {
  "name": "mask",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "230",
  "column": "8",
  "context": "xt[-1], 0.0)] for i in range(batch_size)]\n        mask = forward_logprobs.new(batch_size).long().fill_(1)\n\n        gt_completion_padded = [self.vocab.get_to",
  "context_lines": "        forward_logprobs = log_probs['forward_logprobs']\n        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n        # Each item will be (token, score)\n        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n        mask = forward_logprobs.new(batch_size).long().fill_(1)\n\n        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n                                        max_gen_length - len(gt_completion))]\n\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "371": {
  "name": "gt_completion_padded",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "232",
  "column": "8",
  "context": "logprobs.new(batch_size).long().fill_(1)\n\n        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n                                [x.lower() for x i",
  "context_lines": "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n        # Each item will be (token, score)\n        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n        mask = forward_logprobs.new(batch_size).long().fill_(1)\n\n        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n                                        max_gen_length - len(gt_completion))]\n\n        for index, gt_token_ind in enumerate(gt_completion_padded):\n            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "372": {
  "name": "embeds",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "237",
  "column": "12",
  "context": "d in enumerate(gt_completion_padded):\n            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n            next_dists = F.softmax(self.decoder(se",
  "context_lines": "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n                                        max_gen_length - len(gt_completion))]\n\n        for index, gt_token_ind in enumerate(gt_completion_padded):\n            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n\n            # Perform hacky stuff on the distribution (disallowing BOS, EOS, that sorta thing\n            sampling_probs = next_dists.clone()\n            sampling_probs[:, self.invalid_tokens] = 0.0\n\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "373": {
  "name": "next_dists",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "238",
  "column": "12",
  "context": "[gen[-1][0] for gen in generations]))\n            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n\n            # Perform hacky stuff on the distribu",
  "context_lines": "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n                                        max_gen_length - len(gt_completion))]\n\n        for index, gt_token_ind in enumerate(gt_completion_padded):\n            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n\n            # Perform hacky stuff on the distribution (disallowing BOS, EOS, that sorta thing\n            sampling_probs = next_dists.clone()\n            sampling_probs[:, self.invalid_tokens] = 0.0\n\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "374": {
  "name": "sampling_probs",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "241",
  "column": "12",
  "context": "isallowing BOS, EOS, that sorta thing\n            sampling_probs = next_dists.clone()\n            sampling_probs[:, self.invalid_tokens]",
  "context_lines": "        for index, gt_token_ind in enumerate(gt_completion_padded):\n            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n\n            # Perform hacky stuff on the distribution (disallowing BOS, EOS, that sorta thing\n            sampling_probs = next_dists.clone()\n            sampling_probs[:, self.invalid_tokens] = 0.0\n\n            if first_is_gold:\n                # Gold truth is first row\n                sampling_probs[0].zero_()\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "375": {
  "name": "sampling_probs",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "256",
  "column": "12",
  "context": "ling_probs[:, self.eos_tokens] = 0.0\n\n            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n\n            next_preds = torch.multinomial(sampli",
  "context_lines": "                    sampling_probs.zero_()\n                    sampling_probs[:, gt_token_ind] = 1\n                else:\n                    sampling_probs[:, self.eos_tokens] = 0.0\n\n            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n\n            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n            next_scores = np.log(next_dists[\n                                     torch.arange(0, next_dists.size(0),\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "376": {
  "name": "next_preds",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "258",
  "column": "12",
  "context": " sampling_probs.sum(1, keepdim=True)\n\n            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n            next_scores = np.log(next_dists[\n     ",
  "context_lines": "                    sampling_probs[:, gt_token_ind] = 1\n                else:\n                    sampling_probs[:, self.eos_tokens] = 0.0\n\n            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n\n            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n            next_scores = np.log(next_dists[\n                                     torch.arange(0, next_dists.size(0),\n                                                  out=mask.data.new(next_dists.size(0))),\n                                     next_preds,\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "377": {
  "name": "next_scores",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "259",
  "column": "12",
  "context": "inomial(sampling_probs, 1).squeeze(1)\n            next_scores = np.log(next_dists[\n                                     torch.arange(",
  "context_lines": "                else:\n                    sampling_probs[:, self.eos_tokens] = 0.0\n\n            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n\n            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n            next_scores = np.log(next_dists[\n                                     torch.arange(0, next_dists.size(0),\n                                                  out=mask.data.new(next_dists.size(0))),\n                                     next_preds,\n                                 ].cpu().detach().numpy())\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "378": {
  "name": "is_eos",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "268",
  "column": "12",
  "context": "_token_from_index(pred_id), score_i))\n            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n            mask[is_eos] = 0\n            if mask.s",
  "context_lines": "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n                if mask_i:\n                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n            mask[is_eos] = 0\n            if mask.sum().item() == 0:\n                break\n        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "379": {
  "name": "generation_scores",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "272",
  "column": "8",
  "context": ".sum().item() == 0:\n                break\n        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n        for i, gen in enumerate(generations):\n    ",
  "context_lines": "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n            mask[is_eos] = 0\n            if mask.sum().item() == 0:\n                break\n        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n        for i, gen in enumerate(generations):\n            for j, (_, v) in enumerate(gen[1:]):\n                generation_scores[i, j] = v\n\n        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "380": {
  "name": "generation_toks",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "277",
  "column": "8",
  "context": "             generation_scores[i, j] = v\n\n        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n        return generation_toks, generation_scores[",
  "context_lines": "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n        for i, gen in enumerate(generations):\n            for j, (_, v) in enumerate(gen[1:]):\n                generation_scores[i, j] = v\n\n        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n\n    def _chunked_logsoftmaxes(self, activation, word_targets, chunk_size=256):\n        \"\"\"\n        do the softmax in chunks so memory doesnt explode\n",
  "slicing": [
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n"
  ]
 },
 "381": {
  "name": "idx",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "277",
  "column": "25",
  "context": "ration_scores[i, j] = v\n\n        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n        return generation_toks, generation_scores[",
  "context_lines": "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n        for i, gen in enumerate(generations):\n            for j, (_, v) in enumerate(gen[1:]):\n                generation_scores[i, j] = v\n\n        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n\n    def _chunked_logsoftmaxes(self, activation, word_targets, chunk_size=256):\n        \"\"\"\n        do the softmax in chunks so memory doesnt explode\n",
  "slicing": [
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n"
  ]
 },
 "382": {
  "name": "all_logprobs",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "288",
  "column": "8",
  "context": "on GPU specs\n        :return:\n        \"\"\"\n        all_logprobs = []\n        num_chunks = (activation.size(0) - 1) // c",
  "context_lines": "        :param targets: [batch, T] indices\n        :param chunk_size: you might need to tune this based on GPU specs\n        :return:\n        \"\"\"\n        all_logprobs = []\n        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
  "slicing": [
   "        all_logprobs = []\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "        return torch.cat(all_logprobs, 0)\n"
  ]
 },
 "383": {
  "name": "num_chunks",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "289",
  "column": "8",
  "context": "rn:\n        \"\"\"\n        all_logprobs = []\n        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n        for activation_chunk, target_chunk in zip(",
  "context_lines": "        :param chunk_size: you might need to tune this based on GPU specs\n        :return:\n        \"\"\"\n        all_logprobs = []\n        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n            targets_flat = target_chunk.view(-1)\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "384": {
  "name": "targets_flat",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "293",
  "column": "12",
  "context": "size()[:2] == target_chunk.size()[:2]\n            targets_flat = target_chunk.view(-1)\n            time_indexer = torch.arange(0, targets",
  "context_lines": "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n            targets_flat = target_chunk.view(-1)\n            time_indexer = torch.arange(0, targets_flat.size(0),\n                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n            batch_indexer = torch.arange(0, targets_flat.size(0),\n                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "385": {
  "name": "time_indexer",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "294",
  "column": "12",
  "context": " targets_flat = target_chunk.view(-1)\n            time_indexer = torch.arange(0, targets_flat.size(0),\n                                        out=target",
  "context_lines": "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n            targets_flat = target_chunk.view(-1)\n            time_indexer = torch.arange(0, targets_flat.size(0),\n                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n            batch_indexer = torch.arange(0, targets_flat.size(0),\n                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "386": {
  "name": "batch_indexer",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "296",
  "column": "12",
  "context": "flat.size(0))) % target_chunk.size(1)\n            batch_indexer = torch.arange(0, targets_flat.size(0),\n                                         out=targe",
  "context_lines": "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n            targets_flat = target_chunk.view(-1)\n            time_indexer = torch.arange(0, targets_flat.size(0),\n                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n            batch_indexer = torch.arange(0, targets_flat.size(0),\n                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n        return torch.cat(all_logprobs, 0)\n\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "387": {
  "name": "encoded_inputs",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "309",
  "column": "8",
  "context": "ith EOS here\n        :return:\n        \"\"\"\n        encoded_inputs = self.embed_words(words)\n        mask = (words != 0).long()[:, 2:]\n        ",
  "context_lines": "        use this for training the LM\n        :param words: [batch_size, N] words. assuming you're starting with BOS and ending with EOS here\n        :return:\n        \"\"\"\n        encoded_inputs = self.embed_words(words)\n        mask = (words != 0).long()[:, 2:]\n        word_targets = words[:, 1:-1].contiguous()\n\n        result_dict = {\n            'mask': mask,\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "388": {
  "name": "mask",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "310",
  "column": "8",
  "context": " encoded_inputs = self.embed_words(words)\n        mask = (words != 0).long()[:, 2:]\n        word_targets = words[:, 1:-1].contiguous()",
  "context_lines": "        :param words: [batch_size, N] words. assuming you're starting with BOS and ending with EOS here\n        :return:\n        \"\"\"\n        encoded_inputs = self.embed_words(words)\n        mask = (words != 0).long()[:, 2:]\n        word_targets = words[:, 1:-1].contiguous()\n\n        result_dict = {\n            'mask': mask,\n            'word_targets': word_targets,\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "389": {
  "name": "word_targets",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "311",
  "column": "8",
  "context": "        mask = (words != 0).long()[:, 2:]\n        word_targets = words[:, 1:-1].contiguous()\n\n        result_dict = {\n            'mask': mask,",
  "context_lines": "        :return:\n        \"\"\"\n        encoded_inputs = self.embed_words(words)\n        mask = (words != 0).long()[:, 2:]\n        word_targets = words[:, 1:-1].contiguous()\n\n        result_dict = {\n            'mask': mask,\n            'word_targets': word_targets,\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "390": {
  "name": "result_dict",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "313",
  "column": "8",
  "context": "rd_targets = words[:, 1:-1].contiguous()\n\n        result_dict = {\n            'mask': mask,\n            'word_target",
  "context_lines": "        \"\"\"\n        encoded_inputs = self.embed_words(words)\n        mask = (words != 0).long()[:, 2:]\n        word_targets = words[:, 1:-1].contiguous()\n\n        result_dict = {\n            'mask': mask,\n            'word_targets': word_targets,\n        }\n        # TODO: try to reduce duplicate code here\n",
  "slicing": [
   "        result_dict = {\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "        return result_dict\n"
  ]
 },
 "391": {
  "name": "forward_activation",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "320",
  "column": "12",
  "context": "       self.forward_lm.reset_states()\n            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n\n            if compute_logprobs:\n                ",
  "context_lines": "        }\n        # TODO: try to reduce duplicate code here\n        if use_forward:\n            self.forward_lm.reset_states()\n            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n\n            if compute_logprobs:\n                # being memory efficient here is critical if the input tensors are large\n                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "392": {
  "name": "reverse_activation",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "334",
  "column": "12",
  "context": "       self.reverse_lm.reset_states()\n            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n            if compute_logprobs:\n                r",
  "context_lines": "                                                                                 word_targets,\n                                                                                 mask)\n        if use_reverse:\n            self.reverse_lm.reset_states()\n            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n            if compute_logprobs:\n                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n                                                                             word_targets) * mask.float()\n            else:\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "    dup_set = set()\n",
   "    unique_idx = []\n",
   "    for i, gen_i in enumerate(generations):\n",
   "        gen_i_str = ' '.join(gen_i)\n",
   "        if gen_i_str not in dup_set:\n",
   "            unique_idx.append(i)\n",
   "            dup_set.add(gen_i_str)\n",
   "    return [generations[i] for i in unique_idx], np.array(unique_idx)\n",
   "        layers = []\n",
   "        lstm_input_size = input_size\n",
   "        for layer_index in range(num_layers):\n",
   "            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward,\n",
   "            lstm_input_size = hidden_size\n",
   "            self.add_module('layer_{}'.format(layer_index), layer)\n",
   "            layers.append(layer)\n",
   "        self.lstm_layers = layers\n",
   "            hidden_states = [None] * len(self.lstm_layers)\n",
   "            hidden_states = list(zip(initial_state[0].split(1, 0),\n",
   "        output_sequence = inputs\n",
   "        final_states = []\n",
   "        for i, state in enumerate(hidden_states):\n",
   "            layer = getattr(self, 'layer_{}'.format(i))\n",
   "            output_sequence, final_state = layer(output_sequence, state)\n",
   "            final_states.append(final_state)\n",
   "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n",
   "        return output_sequence, final_state_tuple\n",
   "        self.register_buffer('eos_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n",
   "        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n",
   "        batch = Batch([Instance(\n",
   "            {'story': TextField([Token('@@bos@@')] + [Token(x) for x in story] + [Token('@@eos@@')],\n",
   "            for story in stories_tokenized])\n",
   "        batch.index_instances(self.vocab)\n",
   "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n",
   "        return words\n",
   "        log_probs = self(self.batch_to_ids([context]), use_forward=True,\n",
   "        forward_logprobs = log_probs['forward_logprobs']\n",
   "        self.forward_lm._states = tuple(x.repeat(1, batch_size, 1).contiguous() for x in self.forward_lm._states)\n",
   "        generations = [[(context[-1], 0.0)] for i in range(batch_size)]\n",
   "        mask = forward_logprobs.new(batch_size).long().fill_(1)\n",
   "        gt_completion_padded = [self.vocab.get_token_index(gt_token) for gt_token in\n",
   "                                [x.lower() for x in gt_completion] + ['@@PADDING@@'] * (\n",
   "        for index, gt_token_ind in enumerate(gt_completion_padded):\n",
   "            embeds = self.embed_words(self.timestep_to_ids([gen[-1][0] for gen in generations]))\n",
   "            next_dists = F.softmax(self.decoder(self.forward_lm(embeds, mask[:, None]))[:, 0], dim=1)\n",
   "            sampling_probs = next_dists.clone()\n",
   "            sampling_probs[:, self.invalid_tokens] = 0.0\n",
   "                sampling_probs[0].zero_()\n",
   "                sampling_probs[0, gt_token_ind] = 1\n",
   "                if index == (len(gt_completion) - 1):\n",
   "                    sampling_probs.zero_()\n",
   "                    sampling_probs[:, gt_token_ind] = 1\n",
   "                    sampling_probs[:, self.eos_tokens] = 0.0\n",
   "            sampling_probs = sampling_probs / sampling_probs.sum(1, keepdim=True)\n",
   "            next_preds = torch.multinomial(sampling_probs, 1).squeeze(1)\n",
   "            next_scores = np.log(next_dists[\n",
   "                                     torch.arange(0, next_dists.size(0),\n",
   "                                                  out=mask.data.new(next_dists.size(0))),\n",
   "                                     next_preds,\n",
   "            for i, (gen_list, pred_id, score_i, mask_i) in enumerate(\n",
   "                    zip(generations, next_preds.cpu().detach().numpy(), next_scores, mask.data.cpu().detach().numpy())):\n",
   "                if mask_i:\n",
   "                    gen_list.append((self.vocab.get_token_from_index(pred_id), score_i))\n",
   "            is_eos = (next_preds[:, None] == self.eos_tokens[None]).max(1)[0]\n",
   "            mask[is_eos] = 0\n",
   "            if mask.sum().item() == 0:\n",
   "        generation_scores = np.zeros((len(generations), max([len(g) - 1 for g in generations])), dtype=np.float32)\n",
   "        for i, gen in enumerate(generations):\n",
   "            for j, (_, v) in enumerate(gen[1:]):\n",
   "                generation_scores[i, j] = v\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n",
   "        all_logprobs = []\n",
   "        num_chunks = (activation.size(0) - 1) // chunk_size + 1\n",
   "        for activation_chunk, target_chunk in zip(torch.chunk(activation, num_chunks, dim=0),\n",
   "                                                  torch.chunk(word_targets, num_chunks, dim=0)):\n",
   "            assert activation_chunk.size()[:2] == target_chunk.size()[:2]\n",
   "            targets_flat = target_chunk.view(-1)\n",
   "            time_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                        out=target_chunk.data.new(targets_flat.size(0))) % target_chunk.size(1)\n",
   "            batch_indexer = torch.arange(0, targets_flat.size(0),\n",
   "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n",
   "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n",
   "                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n",
   "        return torch.cat(all_logprobs, 0)\n",
   "        encoded_inputs = self.embed_words(words)\n",
   "        mask = (words != 0).long()[:, 2:]\n",
   "        word_targets = words[:, 1:-1].contiguous()\n",
   "        result_dict = {\n",
   "            'mask': mask,\n",
   "            'word_targets': word_targets,\n",
   "            forward_activation = self.forward_lm(encoded_inputs[:, :-2], mask)\n",
   "                result_dict['forward_logprobs'] = self._chunked_logsoftmaxes(forward_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['forward_logits'] = self.decoder(forward_activation)\n",
   "                result_dict['forward_loss'] = sequence_cross_entropy_with_logits(result_dict['forward_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "            reverse_activation = self.reverse_lm(encoded_inputs[:, 2:], mask)\n",
   "                result_dict['reverse_logprobs'] = self._chunked_logsoftmaxes(reverse_activation,\n",
   "                                                                             word_targets) * mask.float()\n",
   "                result_dict['reverse_logits'] = self.decoder(reverse_activation)\n",
   "                result_dict['reverse_loss'] = sequence_cross_entropy_with_logits(result_dict['reverse_logits'],\n",
   "                                                                                 word_targets,\n",
   "                                                                                 mask)\n",
   "        return result_dict\n"
  ]
 },
 "393": {
  "name": "BATCH_SIZE",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "19",
  "column": "0",
  "context": "s_reader import ActivityNetCaptionsDatasetReader\n\nBATCH_SIZE = 1\nBEAM_SIZE = 8 * BATCH_SIZE\n\nlogger = logging.getLo",
  "context_lines": "from adversarialnlp.generators import Generator\nfrom adversarialnlp.generators.swag.simple_bilm import SimpleBiLM\nfrom adversarialnlp.generators.swag.utils import optimistic_restore\nfrom adversarialnlp.generators.swag.activitynet_captions_reader import ActivityNetCaptionsDatasetReader\n\nBATCH_SIZE = 1\nBEAM_SIZE = 8 * BATCH_SIZE\n\nlogger = logging.getLogger(__name__)\n\nclass SwagGenerator(Generator):\n    \"\"\"\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "394": {
  "name": "BEAM_SIZE",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "20",
  "column": "0",
  "context": " ActivityNetCaptionsDatasetReader\n\nBATCH_SIZE = 1\nBEAM_SIZE = 8 * BATCH_SIZE\n\nlogger = logging.getLogger(__name__)\n\nclass SwagG",
  "context_lines": "from adversarialnlp.generators.swag.simple_bilm import SimpleBiLM\nfrom adversarialnlp.generators.swag.utils import optimistic_restore\nfrom adversarialnlp.generators.swag.activitynet_captions_reader import ActivityNetCaptionsDatasetReader\n\nBATCH_SIZE = 1\nBEAM_SIZE = 8 * BATCH_SIZE\n\nlogger = logging.getLogger(__name__)\n\nclass SwagGenerator(Generator):\n    \"\"\"\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "395": {
  "name": "logger",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "22",
  "column": "0",
  "context": "eader\n\nBATCH_SIZE = 1\nBEAM_SIZE = 8 * BATCH_SIZE\n\nlogger = logging.getLogger(__name__)\n\nclass SwagGenerator(Generator):\n    \"\"\"\n    ``Swa",
  "context_lines": "from adversarialnlp.generators.swag.utils import optimistic_restore\nfrom adversarialnlp.generators.swag.activitynet_captions_reader import ActivityNetCaptionsDatasetReader\n\nBATCH_SIZE = 1\nBEAM_SIZE = 8 * BATCH_SIZE\n\nlogger = logging.getLogger(__name__)\n\nclass SwagGenerator(Generator):\n    \"\"\"\n    ``SwagGenerator`` inherit from the ``Generator`` class.\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "logger = logging.getLogger(__name__)\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "396": {
  "name": "lm_files",
  "type": "adversarialnlp.common.file_utils.download_files",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "55",
  "column": "8",
  "context": "  super().__init__(default_seeds, quiet)\n\n        lm_files = download_files(fnames=['vocabulary.zip',\n                                          'lm-fold",
  "context_lines": "    def __init__(self,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n        super().__init__(default_seeds, quiet)\n\n        lm_files = download_files(fnames=['vocabulary.zip',\n                                          'lm-fold-0.bin'],\n                                  local_folder='swag_lm')\n\n        activity_data_files = download_files(fnames=['captions.zip'],\n                                             paths='https://cs.stanford.edu/people/ranjaykrishna/densevid/',\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "397": {
  "name": "activity_data_files",
  "type": "adversarialnlp.common.file_utils.download_files",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "59",
  "column": "8",
  "context": "                 local_folder='swag_lm')\n\n        activity_data_files = download_files(fnames=['captions.zip'],\n                                             paths",
  "context_lines": "        super().__init__(default_seeds, quiet)\n\n        lm_files = download_files(fnames=['vocabulary.zip',\n                                          'lm-fold-0.bin'],\n                                  local_folder='swag_lm')\n\n        activity_data_files = download_files(fnames=['captions.zip'],\n                                             paths='https://cs.stanford.edu/people/ranjaykrishna/densevid/',\n                                             local_folder='activitynet_captions')\n\n        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n                             cache_dir=str(DATA_ROOT / 'allennlp_constituency_parser'))\n\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "398": {
  "name": "const_parser_files",
  "type": "allennlp.common.file_utils.cached_path",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "63",
  "column": "8",
  "context": "    local_folder='activitynet_captions')\n\n        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n                             cache_dir=str(DATA_RO",
  "context_lines": "                                  local_folder='swag_lm')\n\n        activity_data_files = download_files(fnames=['captions.zip'],\n                                             paths='https://cs.stanford.edu/people/ranjaykrishna/densevid/',\n                                             local_folder='activitynet_captions')\n\n        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n                             cache_dir=str(DATA_ROOT / 'allennlp_constituency_parser'))\n\n        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n        vocab = Vocabulary.from_files(lm_files[0])\n        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "399": {
  "name": "vocab",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "67",
  "column": "8",
  "context": "files, 'constituency-parser').predictor()\n        vocab = Vocabulary.from_files(lm_files[0])\n        self.language_model = SimpleBiLM(vocab=voc",
  "context_lines": "                                             local_folder='activitynet_captions')\n\n        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n                             cache_dir=str(DATA_ROOT / 'allennlp_constituency_parser'))\n\n        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n        vocab = Vocabulary.from_files(lm_files[0])\n        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n                                         embedding_dropout_probability=0.2)\n        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n\n        if default_seeds is None:\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "400": {
  "name": "result",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "83",
  "column": "12",
  "context": "            assert 'children' in tree\n            result = []\n            for child in tree['children']:\n       ",
  "context_lines": "        r\"\"\"Recurse on a constituency parse tree until we find verb phrases\"\"\"\n\n        # Recursion is annoying because we need to check whether each is a list or not\n        def _recurse_on_children():\n            assert 'children' in tree\n            result = []\n            for child in tree['children']:\n                res = self._find_VP(child)\n                if isinstance(res, tuple):\n                    result.append(res)\n",
  "slicing": [
   "            result = []\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n"
  ]
 },
 "401": {
  "name": "res",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "85",
  "column": "16",
  "context": "   for child in tree['children']:\n                res = self._find_VP(child)\n                if isinstance(res, tuple):\n       ",
  "context_lines": "        def _recurse_on_children():\n            assert 'children' in tree\n            result = []\n            for child in tree['children']:\n                res = self._find_VP(child)\n                if isinstance(res, tuple):\n                    result.append(res)\n                else:\n                    result.extend(res)\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "402": {
  "name": "sentence_txt",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "114",
  "column": "8",
  "context": "s a sentence on the final verb phrase \"\"\"\n        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n        res = self.const_parser.predict(sentence_t",
  "context_lines": "        # try recursing on everything\n        return _recurse_on_children()\n\n    def _split_on_final_vp(self, sentence: Instance) -> (List[str], List[str]):\n        r\"\"\"Splits a sentence on the final verb phrase \"\"\"\n        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n        res = self.const_parser.predict(sentence_txt)\n        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n        if not is_vp:\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "403": {
  "name": "res",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "115",
  "column": "8",
  "context": "' '.join(t.text for t in sentence.tokens)\n        res = self.const_parser.predict(sentence_txt)\n        res_chunked = self._find_VP(res['hierplane",
  "context_lines": "        return _recurse_on_children()\n\n    def _split_on_final_vp(self, sentence: Instance) -> (List[str], List[str]):\n        r\"\"\"Splits a sentence on the final verb phrase \"\"\"\n        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n        res = self.const_parser.predict(sentence_txt)\n        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n        if not is_vp:\n            return None, None\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "404": {
  "name": "res_chunked",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "116",
  "column": "8",
  "context": "= self.const_parser.predict(sentence_txt)\n        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n        is_vp: List[int] = [i for i, (word, pos) i",
  "context_lines": "    def _split_on_final_vp(self, sentence: Instance) -> (List[str], List[str]):\n        r\"\"\"Splits a sentence on the final verb phrase \"\"\"\n        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n        res = self.const_parser.predict(sentence_txt)\n        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n        if not is_vp:\n            return None, None\n        vp_ind = max(is_vp)\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "405": {
  "name": "is_vp",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "117",
  "column": "8",
  "context": "f._find_VP(res['hierplane_tree']['root'])\n        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n        if not is_vp:\n            return None, Non",
  "context_lines": "        r\"\"\"Splits a sentence on the final verb phrase \"\"\"\n        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n        res = self.const_parser.predict(sentence_txt)\n        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n        if not is_vp:\n            return None, None\n        vp_ind = max(is_vp)\n        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "406": {
  "name": "vp_ind",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "120",
  "column": "8",
  "context": " not is_vp:\n            return None, None\n        vp_ind = max(is_vp)\n        not_vp = [token for x in res_chunked[:vp_i",
  "context_lines": "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n        if not is_vp:\n            return None, None\n        vp_ind = max(is_vp)\n        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n        return not_vp, is_vp\n\n    def generate_from_seed(self, seed: Tuple):\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "407": {
  "name": "not_vp",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "121",
  "column": "8",
  "context": "rn None, None\n        vp_ind = max(is_vp)\n        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n        is_vp = [token for x in res_chunked[vp_ind",
  "context_lines": "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n        if not is_vp:\n            return None, None\n        vp_ind = max(is_vp)\n        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n        return not_vp, is_vp\n\n    def generate_from_seed(self, seed: Tuple):\n        \"\"\"Edit a seed example.\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "408": {
  "name": "is_vp",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "122",
  "column": "8",
  "context": "ed[:vp_ind] for token in x[0].split(' ')]\n        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n        return not_vp, is_vp\n\n    def generate_fro",
  "context_lines": "        if not is_vp:\n            return None, None\n        vp_ind = max(is_vp)\n        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n        return not_vp, is_vp\n\n    def generate_from_seed(self, seed: Tuple):\n        \"\"\"Edit a seed example.\n        \"\"\"\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "409": {
  "name": "first_sentence",
  "type": "allennlp.data.fields.TextField",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "128",
  "column": "8",
  "context": "      \"\"\"Edit a seed example.\n        \"\"\"\n        first_sentence: TextField = seed.fields[\"first_sentence\"]\n        second_sentence: TextField = seed.fields[\"",
  "context_lines": "        return not_vp, is_vp\n\n    def generate_from_seed(self, seed: Tuple):\n        \"\"\"Edit a seed example.\n        \"\"\"\n        first_sentence: TextField = seed.fields[\"first_sentence\"]\n        second_sentence: TextField = seed.fields[\"second_sentence\"]\n        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n        if not eos_bounds:\n            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "410": {
  "name": "second_sentence",
  "type": "allennlp.data.fields.TextField",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "129",
  "column": "8",
  "context": "TextField = seed.fields[\"first_sentence\"]\n        second_sentence: TextField = seed.fields[\"second_sentence\"]\n        eos_bounds = [i + 1 for i, x in enumerate(",
  "context_lines": "    def generate_from_seed(self, seed: Tuple):\n        \"\"\"Edit a seed example.\n        \"\"\"\n        first_sentence: TextField = seed.fields[\"first_sentence\"]\n        second_sentence: TextField = seed.fields[\"second_sentence\"]\n        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n        if not eos_bounds:\n            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n                                        token_indexers=first_sentence.token_indexers)\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "411": {
  "name": "eos_bounds",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "130",
  "column": "8",
  "context": "extField = seed.fields[\"second_sentence\"]\n        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n        if not eos_bounds:\n            first_sente",
  "context_lines": "        \"\"\"Edit a seed example.\n        \"\"\"\n        first_sentence: TextField = seed.fields[\"first_sentence\"]\n        second_sentence: TextField = seed.fields[\"second_sentence\"]\n        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n        if not eos_bounds:\n            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n                                        token_indexers=first_sentence.token_indexers)\n        context_len = len(first_sentence.tokens)\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "412": {
  "name": "first_sentence",
  "type": "allennlp.data.fields.TextField",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "132",
  "column": "12",
  "context": "'?', '!')]\n        if not eos_bounds:\n            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n                                        token_inde",
  "context_lines": "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n        second_sentence: TextField = seed.fields[\"second_sentence\"]\n        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n        if not eos_bounds:\n            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n                                        token_indexers=first_sentence.token_indexers)\n        context_len = len(first_sentence.tokens)\n        if context_len < 6 or context_len > 100:\n            print(\"skipping on {} (too short or long)\".format(\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "413": {
  "name": "context_len",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "134",
  "column": "8",
  "context": "n_indexers=first_sentence.token_indexers)\n        context_len = len(first_sentence.tokens)\n        if context_len < 6 or context_len > 100:\n ",
  "context_lines": "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n        if not eos_bounds:\n            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n                                        token_indexers=first_sentence.token_indexers)\n        context_len = len(first_sentence.tokens)\n        if context_len < 6 or context_len > 100:\n            print(\"skipping on {} (too short or long)\".format(\n                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n            return\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "414": {
  "name": "eos_bounds_s2",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "141",
  "column": "8",
  "context": "iple periods, etc. in s2 or in the middle\n        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n        if len(eos_bounds_s2) > 1 or max(eos_bound",
  "context_lines": "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n            return\n        # Something I should have done: \n        # make sure that there aren't multiple periods, etc. in s2 or in the middle\n        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n            return\n        elif not eos_bounds_s2:\n            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "415": {
  "name": "startphrase",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "149",
  "column": "8",
  "context": "_indexers)\n\n        # Now split on the VP\n        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n        if startphrase is None or not startphrase ",
  "context_lines": "        elif not eos_bounds_s2:\n            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n                                        token_indexers=second_sentence.token_indexers)\n\n        # Now split on the VP\n        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n                                                 startphrase, endphrase), flush=True)\n            return\n\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "416": {
  "name": "endphrase",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "149",
  "column": "21",
  "context": "       # Now split on the VP\n        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n        if startphrase is None or not startphrase ",
  "context_lines": "        elif not eos_bounds_s2:\n            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n                                        token_indexers=second_sentence.token_indexers)\n\n        # Now split on the VP\n        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n                                                 startphrase, endphrase), flush=True)\n            return\n\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "417": {
  "name": "context",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "161",
  "column": "8",
  "context": "1_toks + s2_toks)))\n        #     return\n\n        context = [token.text for token in first_sentence.tokens] + startphrase\n\n        lm_out = self.language_model.conditional_",
  "context_lines": "        # if any(vocab.get_token_index(tok.lower()) == vocab.get_token_index(vocab._oov_token)\n        #        for tok in endphrase):\n        #     print(\"skipping on {} (unk!)\".format(' '.join(s1_toks + s2_toks)))\n        #     return\n\n        context = [token.text for token in first_sentence.tokens] + startphrase\n\n        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n                                                            batch_size=BEAM_SIZE,\n                                                            max_gen_length=25)\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "418": {
  "name": "lm_out",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "163",
  "column": "8",
  "context": " in first_sentence.tokens] + startphrase\n\n        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n                                                  ",
  "context_lines": "        #        for tok in endphrase):\n        #     print(\"skipping on {} (unk!)\".format(' '.join(s1_toks + s2_toks)))\n        #     return\n\n        context = [token.text for token in first_sentence.tokens] + startphrase\n\n        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n                                                            batch_size=BEAM_SIZE,\n                                                            max_gen_length=25)\n        gens0, fwd_scores, ctx_scores = lm_out\n        if len(gens0) < BATCH_SIZE:\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "419": {
  "name": "gens0",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "166",
  "column": "8",
  "context": "                       max_gen_length=25)\n        gens0, fwd_scores, ctx_scores = lm_out\n        if len(gens0) < BATCH_SIZE:\n            pr",
  "context_lines": "        context = [token.text for token in first_sentence.tokens] + startphrase\n\n        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n                                                            batch_size=BEAM_SIZE,\n                                                            max_gen_length=25)\n        gens0, fwd_scores, ctx_scores = lm_out\n        if len(gens0) < BATCH_SIZE:\n            print(\"Couldn't generate enough candidates so skipping\")\n            return\n        gens0 = gens0[:BATCH_SIZE]\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "420": {
  "name": "fwd_scores",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "166",
  "column": "15",
  "context": "                max_gen_length=25)\n        gens0, fwd_scores, ctx_scores = lm_out\n        if len(gens0) < BATCH_SIZE:\n            pr",
  "context_lines": "        context = [token.text for token in first_sentence.tokens] + startphrase\n\n        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n                                                            batch_size=BEAM_SIZE,\n                                                            max_gen_length=25)\n        gens0, fwd_scores, ctx_scores = lm_out\n        if len(gens0) < BATCH_SIZE:\n            print(\"Couldn't generate enough candidates so skipping\")\n            return\n        gens0 = gens0[:BATCH_SIZE]\n",
  "slicing": "        gens0, fwd_scores, ctx_scores = lm_out\n"
 },
 "421": {
  "name": "ctx_scores",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "166",
  "column": "27",
  "context": "    max_gen_length=25)\n        gens0, fwd_scores, ctx_scores = lm_out\n        if len(gens0) < BATCH_SIZE:\n            pr",
  "context_lines": "        context = [token.text for token in first_sentence.tokens] + startphrase\n\n        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n                                                            batch_size=BEAM_SIZE,\n                                                            max_gen_length=25)\n        gens0, fwd_scores, ctx_scores = lm_out\n        if len(gens0) < BATCH_SIZE:\n            print(\"Couldn't generate enough candidates so skipping\")\n            return\n        gens0 = gens0[:BATCH_SIZE]\n",
  "slicing": "        gens0, fwd_scores, ctx_scores = lm_out\n"
 },
 "422": {
  "name": "gens0",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "170",
  "column": "8",
  "context": "ndidates so skipping\")\n            return\n        gens0 = gens0[:BATCH_SIZE]\n        yield gens0\n        # fwd_scores = fwd_sco",
  "context_lines": "        gens0, fwd_scores, ctx_scores = lm_out\n        if len(gens0) < BATCH_SIZE:\n            print(\"Couldn't generate enough candidates so skipping\")\n            return\n        gens0 = gens0[:BATCH_SIZE]\n        yield gens0\n        # fwd_scores = fwd_scores[:BATCH_SIZE]\n\n        # # Now get the backward scores.\n        # full_sents = [context + gen for gen in gens0]  # NOTE: #1 is GT\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "423": {
  "name": "second_sentence",
  "type": "allennlp.data.fields.TextField",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "145",
  "column": "12",
  "context": "eturn\n        elif not eos_bounds_s2:\n            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n                                        token_inde",
  "context_lines": "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n            return\n        elif not eos_bounds_s2:\n            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n                                        token_indexers=second_sentence.token_indexers)\n\n        # Now split on the VP\n        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
  "slicing": [
   "BATCH_SIZE = 1\n",
   "BEAM_SIZE = 8 * BATCH_SIZE\n",
   "        lm_files = download_files(fnames=['vocabulary.zip',\n",
   "        activity_data_files = download_files(fnames=['captions.zip'],\n",
   "        const_parser_files = cached_path('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz',\n",
   "        self.const_parser = PretrainedModel(const_parser_files, 'constituency-parser').predictor()\n",
   "        vocab = Vocabulary.from_files(lm_files[0])\n",
   "        self.language_model = SimpleBiLM(vocab=vocab, recurrent_dropout_probability=0.2,\n",
   "        optimistic_restore(self.language_model, torch.load(lm_files[1], map_location='cpu')['state_dict'])\n",
   "            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n",
   "            result = []\n",
   "            for child in tree['children']:\n",
   "                res = self._find_VP(child)\n",
   "                if isinstance(res, tuple):\n",
   "                    result.append(res)\n",
   "                    result.extend(res)\n",
   "            return result\n",
   "        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n",
   "        res = self.const_parser.predict(sentence_txt)\n",
   "        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
   "        is_vp: List[int] = [i for i, (word, pos) in enumerate(res_chunked) if pos == 'VP']\n",
   "        if not is_vp:\n",
   "        vp_ind = max(is_vp)\n",
   "        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n",
   "        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n",
   "        return not_vp, is_vp\n",
   "        first_sentence: TextField = seed.fields[\"first_sentence\"]\n",
   "        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
   "        eos_bounds = [i + 1 for i, x in enumerate(first_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if not eos_bounds:\n",
   "            first_sentence = TextField(tokens=first_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=first_sentence.token_indexers)\n",
   "        context_len = len(first_sentence.tokens)\n",
   "        if context_len < 6 or context_len > 100:\n",
   "                    ' '.join(first_sentence.tokens + second_sentence.tokens)))\n",
   "        eos_bounds_s2 = [i + 1 for i, x in enumerate(second_sentence.tokens) if x.text in ('.', '?', '!')]\n",
   "        if len(eos_bounds_s2) > 1 or max(eos_bounds_s2) != len(second_sentence.tokens):\n",
   "        elif not eos_bounds_s2:\n",
   "            second_sentence = TextField(tokens=second_sentence.tokens + [Token(text='.')],\n",
   "                                        token_indexers=second_sentence.token_indexers)\n",
   "        startphrase, endphrase = self._split_on_final_vp(second_sentence)\n",
   "        if startphrase is None or not startphrase or len(endphrase) < 5 or len(endphrase) > 25:\n",
   "            print(\"skipping on {}->{},{}\".format(' '.join(first_sentence.tokens + second_sentence.tokens),\n",
   "                                                 startphrase, endphrase), flush=True)\n",
   "        context = [token.text for token in first_sentence.tokens] + startphrase\n",
   "        lm_out = self.language_model.conditional_generation(context, gt_completion=endphrase,\n",
   "                                                            batch_size=BEAM_SIZE,\n",
   "        gens0, fwd_scores, ctx_scores = lm_out\n",
   "        if len(gens0) < BATCH_SIZE:\n",
   "        gens0 = gens0[:BATCH_SIZE]\n",
   "        yield gens0\n"
  ]
 },
 "424": {
  "name": "model_path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "32",
  "column": "8",
  "context": " -> None:\n        super().__init__(vocab)\n        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n        indexer = OpenaiTransformerBytePairIndexer",
  "context_lines": "                 vocab: Vocabulary,\n                 openai_token_embedder: OpenaiTransformerEmbedder,\n                 remove_bos_eos: bool = True) -> None:\n        super().__init__(vocab)\n        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n        transformer = OpenaiTransformer(model_path=model_path)\n        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n        self._remove_bos_eos = remove_bos_eos\n\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n"
  ]
 },
 "425": {
  "name": "indexer",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "33",
  "column": "8",
  "context": "/openai-transformer-lm-2018.07.23.tar.gz\"\n        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n        transformer = OpenaiTransformer(model_path",
  "context_lines": "                 openai_token_embedder: OpenaiTransformerEmbedder,\n                 remove_bos_eos: bool = True) -> None:\n        super().__init__(vocab)\n        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n        transformer = OpenaiTransformer(model_path=model_path)\n        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n        self._remove_bos_eos = remove_bos_eos\n\n    def _get_target_token_embedding(self,\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n"
  ]
 },
 "426": {
  "name": "transformer",
  "type": "allennlp.modules.openai_transformer.OpenaiTransformer",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "34",
  "column": "8",
  "context": "merBytePairIndexer(model_path=model_path)\n        transformer = OpenaiTransformer(model_path=model_path)\n        self._token_embedders = OpenaiTransformerE",
  "context_lines": "                 remove_bos_eos: bool = True) -> None:\n        super().__init__(vocab)\n        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n        transformer = OpenaiTransformer(model_path=model_path)\n        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n        self._remove_bos_eos = remove_bos_eos\n\n    def _get_target_token_embedding(self,\n                                    token_embeddings: torch.Tensor,\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n"
  ]
 },
 "427": {
  "name": "zero_col",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "43",
  "column": "8",
  "context": "o shift the mask in the correct direction\n        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n        if direction == 0:\n            # forward d",
  "context_lines": "                                    token_embeddings: torch.Tensor,\n                                    mask: torch.Tensor,\n                                    direction: int) -> torch.Tensor:\n        # Need to shift the mask in the correct direction\n        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n        if direction == 0:\n            # forward direction, get token to right\n            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n        else:\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n"
  ]
 },
 "428": {
  "name": "shifted_mask",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "46",
  "column": "12",
  "context": "forward direction, get token to right\n            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n        else:\n            shifted_mask = torch.cat",
  "context_lines": "        # Need to shift the mask in the correct direction\n        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n        if direction == 0:\n            # forward direction, get token to right\n            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n        else:\n            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n\n    def _compute_loss(self,\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n"
  ]
 },
 "429": {
  "name": "shifted_mask",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "48",
  "column": "12",
  "context": " mask[:, 0:-1]], dim=1)\n        else:\n            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n        return token_embeddings.masked_select(shif",
  "context_lines": "        if direction == 0:\n            # forward direction, get token to right\n            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n        else:\n            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n\n    def _compute_loss(self,\n                      lm_embeddings: torch.Tensor,\n                      token_embeddings: torch.Tensor,\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n"
  ]
 },
 "430": {
  "name": "forward_embeddings",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "59",
  "column": "8",
  "context": "_size, timesteps)\n        # masked with 0\n        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n        losses: List[torch.Tensor] = []\n        fo",
  "context_lines": "                      backward_targets: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        # lm_embeddings is shape (batch_size, timesteps, dim * 2)\n        # forward_targets, backward_targets are shape (batch_size, timesteps)\n        # masked with 0\n        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n        losses: List[torch.Tensor] = []\n        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n                                        (1, backward_embeddings, backward_targets)):\n            mask = targets > 0\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "431": {
  "name": "backward_embeddings",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "59",
  "column": "28",
  "context": "      # masked with 0\n        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n        losses: List[torch.Tensor] = []\n        fo",
  "context_lines": "                      backward_targets: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        # lm_embeddings is shape (batch_size, timesteps, dim * 2)\n        # forward_targets, backward_targets are shape (batch_size, timesteps)\n        # masked with 0\n        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n        losses: List[torch.Tensor] = []\n        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n                                        (1, backward_embeddings, backward_targets)):\n            mask = targets > 0\n",
  "slicing": [
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "                                        (1, backward_embeddings, backward_targets)):\n"
  ]
 },
 "432": {
  "name": "losses",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "60",
  "column": "8",
  "context": "d_embeddings = lm_embeddings.chunk(2, -1)\n        losses: List[torch.Tensor] = []\n        for idx, embedding, targets in ((0, forwar",
  "context_lines": "        # lm_embeddings is shape (batch_size, timesteps, dim * 2)\n        # forward_targets, backward_targets are shape (batch_size, timesteps)\n        # masked with 0\n        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n        losses: List[torch.Tensor] = []\n        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n                                        (1, backward_embeddings, backward_targets)):\n            mask = targets > 0\n            # we need to subtract 1 to undo the padding id since the softmax\n",
  "slicing": [
   "        losses: List[torch.Tensor] = []\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "        return losses[0], losses[1]\n"
  ]
 },
 "433": {
  "name": "mask",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "63",
  "column": "12",
  "context": "kward_embeddings, backward_targets)):\n            mask = targets > 0\n            # we need to subtract 1 to undo the pa",
  "context_lines": "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n        losses: List[torch.Tensor] = []\n        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n                                        (1, backward_embeddings, backward_targets)):\n            mask = targets > 0\n            # we need to subtract 1 to undo the padding id since the softmax\n            # does not include a padding dimension\n            non_masked_targets = targets.masked_select(mask) - 1\n            non_masked_embedding = embedding.masked_select(\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "434": {
  "name": "non_masked_targets",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "66",
  "column": "12",
  "context": " does not include a padding dimension\n            non_masked_targets = targets.masked_select(mask) - 1\n            non_masked_embedding = embedding.maske",
  "context_lines": "                                        (1, backward_embeddings, backward_targets)):\n            mask = targets > 0\n            # we need to subtract 1 to undo the padding id since the softmax\n            # does not include a padding dimension\n            non_masked_targets = targets.masked_select(mask) - 1\n            non_masked_embedding = embedding.masked_select(\n                    mask.unsqueeze(-1)\n            ).view(-1, self._forward_dim)\n            # note: need to return average loss across forward and backward\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "435": {
  "name": "non_masked_embedding",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "67",
  "column": "12",
  "context": "ets = targets.masked_select(mask) - 1\n            non_masked_embedding = embedding.masked_select(\n                    mask.unsqueeze(-1)\n           ",
  "context_lines": "            mask = targets > 0\n            # we need to subtract 1 to undo the padding id since the softmax\n            # does not include a padding dimension\n            non_masked_targets = targets.masked_select(mask) - 1\n            non_masked_embedding = embedding.masked_select(\n                    mask.unsqueeze(-1)\n            ).view(-1, self._forward_dim)\n            # note: need to return average loss across forward and backward\n            # directions, but total sum loss across all batches.\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "436": {
  "name": "non_masked_token_embedding",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "82",
  "column": "16",
  "context": "    # pylint: disable=unreachable\n                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n                losses.append(self._softmax(non_ma",
  "context_lines": "                # we also need the token embeddings corresponding to the\n                # the targets\n                raise NotImplementedError(\"This requires SampledSoftmaxLoss, which isn't implemented yet.\")\n                # pylint: disable=unreachable\n                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n                losses.append(self._softmax(non_masked_embedding,\n                                            non_masked_targets,\n                                            non_masked_token_embedding))\n\n        return losses[0], losses[1]\n\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "437": {
  "name": "mask",
  "type": "allennlp.nn.util.get_text_field_mask",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "123",
  "column": "8",
  "context": "       # pylint: disable=arguments-differ\n        mask = get_text_field_mask(source)\n\n        # We must have token_ids so that we can c",
  "context_lines": "        ``'mask'``: ``torch.Tensor``\n            (batch_size, timesteps) mask for the embeddings\n        \"\"\"\n        # pylint: disable=arguments-differ\n        mask = get_text_field_mask(source)\n\n        # We must have token_ids so that we can compute targets\n        token_ids = source.get(\"tokens\")\n        if token_ids is None:\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "438": {
  "name": "token_ids",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "126",
  "column": "8",
  "context": " token_ids so that we can compute targets\n        token_ids = source.get(\"tokens\")\n        if token_ids is None:\n            raise Co",
  "context_lines": "        \"\"\"\n        # pylint: disable=arguments-differ\n        mask = get_text_field_mask(source)\n\n        # We must have token_ids so that we can compute targets\n        token_ids = source.get(\"tokens\")\n        if token_ids is None:\n            raise ConfigurationError(\"Your data must have a 'tokens': SingleIdTokenIndexer() \"\n                                     \"in order to use the BidirectionalLM\")\n\n        # Use token_ids to compute targets\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "439": {
  "name": "forward_targets",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "132",
  "column": "8",
  "context": "       # Use token_ids to compute targets\n        forward_targets = torch.zeros_like(token_ids)\n        backward_targets = torch.zeros_like(token_",
  "context_lines": "        if token_ids is None:\n            raise ConfigurationError(\"Your data must have a 'tokens': SingleIdTokenIndexer() \"\n                                     \"in order to use the BidirectionalLM\")\n\n        # Use token_ids to compute targets\n        forward_targets = torch.zeros_like(token_ids)\n        backward_targets = torch.zeros_like(token_ids)\n        forward_targets[:, 0:-1] = token_ids[:, 1:]\n        backward_targets[:, 1:] = token_ids[:, 0:-1]\n\n        embeddings = self._text_field_embedder(source)\n\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "440": {
  "name": "backward_targets",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "133",
  "column": "8",
  "context": "ard_targets = torch.zeros_like(token_ids)\n        backward_targets = torch.zeros_like(token_ids)\n        forward_targets[:, 0:-1] = token_ids[:, 1:",
  "context_lines": "            raise ConfigurationError(\"Your data must have a 'tokens': SingleIdTokenIndexer() \"\n                                     \"in order to use the BidirectionalLM\")\n\n        # Use token_ids to compute targets\n        forward_targets = torch.zeros_like(token_ids)\n        backward_targets = torch.zeros_like(token_ids)\n        forward_targets[:, 0:-1] = token_ids[:, 1:]\n        backward_targets[:, 1:] = token_ids[:, 0:-1]\n\n        embeddings = self._text_field_embedder(source)\n\n        # Apply LayerNorm if appropriate.\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "441": {
  "name": "embeddings",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "137",
  "column": "8",
  "context": "ward_targets[:, 1:] = token_ids[:, 0:-1]\n\n        embeddings = self._text_field_embedder(source)\n\n        # Apply LayerNorm if appropriate.\n       ",
  "context_lines": "        forward_targets = torch.zeros_like(token_ids)\n        backward_targets = torch.zeros_like(token_ids)\n        forward_targets[:, 0:-1] = token_ids[:, 1:]\n        backward_targets[:, 1:] = token_ids[:, 0:-1]\n\n        embeddings = self._text_field_embedder(source)\n\n        # Apply LayerNorm if appropriate.\n        embeddings = self._layer_norm(embeddings)\n\n        contextual_embeddings = self._contextualizer(embeddings, mask)\n\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "442": {
  "name": "embeddings",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "140",
  "column": "8",
  "context": "        # Apply LayerNorm if appropriate.\n        embeddings = self._layer_norm(embeddings)\n\n        contextual_embeddings = self._contextuali",
  "context_lines": "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n        backward_targets[:, 1:] = token_ids[:, 0:-1]\n\n        embeddings = self._text_field_embedder(source)\n\n        # Apply LayerNorm if appropriate.\n        embeddings = self._layer_norm(embeddings)\n\n        contextual_embeddings = self._contextualizer(embeddings, mask)\n\n        # add dropout\n        contextual_embeddings = self._dropout(contextual_embeddings)\n\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "443": {
  "name": "contextual_embeddings",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "142",
  "column": "8",
  "context": "mbeddings = self._layer_norm(embeddings)\n\n        contextual_embeddings = self._contextualizer(embeddings, mask)\n\n        # add dropout\n        contextual_embeddin",
  "context_lines": "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n\n        embeddings = self._text_field_embedder(source)\n\n        # Apply LayerNorm if appropriate.\n        embeddings = self._layer_norm(embeddings)\n\n        contextual_embeddings = self._contextualizer(embeddings, mask)\n\n        # add dropout\n        contextual_embeddings = self._dropout(contextual_embeddings)\n\n        # compute softmax loss\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "444": {
  "name": "contextual_embeddings",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "145",
  "column": "8",
  "context": "(embeddings, mask)\n\n        # add dropout\n        contextual_embeddings = self._dropout(contextual_embeddings)\n\n        # compute softmax loss\n        forward_lo",
  "context_lines": "        # Apply LayerNorm if appropriate.\n        embeddings = self._layer_norm(embeddings)\n\n        contextual_embeddings = self._contextualizer(embeddings, mask)\n\n        # add dropout\n        contextual_embeddings = self._dropout(contextual_embeddings)\n\n        # compute softmax loss\n        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n                                                         embeddings,\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "445": {
  "name": "forward_loss",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "148",
  "column": "8",
  "context": "beddings)\n\n        # compute softmax loss\n        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n                                                  ",
  "context_lines": "        contextual_embeddings = self._contextualizer(embeddings, mask)\n\n        # add dropout\n        contextual_embeddings = self._dropout(contextual_embeddings)\n\n        # compute softmax loss\n        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n                                                         embeddings,\n                                                         forward_targets,\n                                                         backward_targets)\n\n        num_targets = torch.sum((forward_targets > 0).long())\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "446": {
  "name": "backward_loss",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "148",
  "column": "22",
  "context": "     # compute softmax loss\n        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n                                                  ",
  "context_lines": "        contextual_embeddings = self._contextualizer(embeddings, mask)\n\n        # add dropout\n        contextual_embeddings = self._dropout(contextual_embeddings)\n\n        # compute softmax loss\n        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n                                                         embeddings,\n                                                         forward_targets,\n                                                         backward_targets)\n\n        num_targets = torch.sum((forward_targets > 0).long())\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "447": {
  "name": "num_targets",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "153",
  "column": "8",
  "context": "                       backward_targets)\n\n        num_targets = torch.sum((forward_targets > 0).long())\n        if num_targets > 0:\n            average_lo",
  "context_lines": "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n                                                         embeddings,\n                                                         forward_targets,\n                                                         backward_targets)\n\n        num_targets = torch.sum((forward_targets > 0).long())\n        if num_targets > 0:\n            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n        else:\n            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "448": {
  "name": "average_loss",
  "type": "float",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "155",
  "column": "12",
  "context": ").long())\n        if num_targets > 0:\n            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n        else:\n            average_loss = torch.ten",
  "context_lines": "                                                         forward_targets,\n                                                         backward_targets)\n\n        num_targets = torch.sum((forward_targets > 0).long())\n        if num_targets > 0:\n            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n        else:\n            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n        # this is stored to compute perplexity if needed\n        self._last_average_loss[0] = average_loss.detach().item()\n\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "449": {
  "name": "scale_factor",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "164",
  "column": "16",
  "context": " self._loss_scale == 'n_samples':\n                scale_factor = num_targets.float()\n            else:\n                scale_factor = s",
  "context_lines": "        self._last_average_loss[0] = average_loss.detach().item()\n\n        if num_targets > 0:\n            # loss is directly minimized\n            if self._loss_scale == 'n_samples':\n                scale_factor = num_targets.float()\n            else:\n                scale_factor = self._loss_scale\n\n            return_dict = {\n                    'loss': average_loss * scale_factor,\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "450": {
  "name": "return_dict",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "168",
  "column": "12",
  "context": "     scale_factor = self._loss_scale\n\n            return_dict = {\n                    'loss': average_loss * scale_f",
  "context_lines": "            if self._loss_scale == 'n_samples':\n                scale_factor = num_targets.float()\n            else:\n                scale_factor = self._loss_scale\n\n            return_dict = {\n                    'loss': average_loss * scale_factor,\n                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n            }\n",
  "slicing": [
   "            return_dict = {\n",
   "        return_dict.update({\n",
   "        return return_dict\n"
  ]
 },
 "451": {
  "name": "contextual_embeddings",
  "type": "allennlp.nn.util.remove_sentence_boundaries",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "182",
  "column": "12",
  "context": "  }\n\n        if self._remove_bos_eos:\n            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n\n        return_dict.update({\n                'lm_",
  "context_lines": "                    'forward_loss': average_loss,\n                    'backward_loss': average_loss\n            }\n\n        if self._remove_bos_eos:\n            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n\n        return_dict.update({\n                'lm_embeddings': contextual_embeddings,\n                'mask': mask\n",
  "slicing": [
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "452": {
  "name": "mask",
  "type": "allennlp.nn.util.remove_sentence_boundaries",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "182",
  "column": "35",
  "context": "emove_bos_eos:\n            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n\n        return_dict.update({\n                'lm_",
  "context_lines": "                    'forward_loss': average_loss,\n                    'backward_loss': average_loss\n            }\n\n        if self._remove_bos_eos:\n            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n\n        return_dict.update({\n                'lm_embeddings': contextual_embeddings,\n                'mask': mask\n",
  "slicing": [
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "453": {
  "name": "average_loss",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "157",
  "column": "12",
  "context": ") / num_targets.float()\n        else:\n            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n        # this is stored to compute perplexity if ",
  "context_lines": "        num_targets = torch.sum((forward_targets > 0).long())\n        if num_targets > 0:\n            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n        else:\n            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n        # this is stored to compute perplexity if needed\n        self._last_average_loss[0] = average_loss.detach().item()\n\n        if num_targets > 0:\n            # loss is directly minimized\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "454": {
  "name": "scale_factor",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "166",
  "column": "16",
  "context": "targets.float()\n            else:\n                scale_factor = self._loss_scale\n\n            return_dict = {\n                    '",
  "context_lines": "            # loss is directly minimized\n            if self._loss_scale == 'n_samples':\n                scale_factor = num_targets.float()\n            else:\n                scale_factor = self._loss_scale\n\n            return_dict = {\n                    'loss': average_loss * scale_factor,\n                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
  "slicing": [
   "        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
   "        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
   "        transformer = OpenaiTransformer(model_path=model_path)\n",
   "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n",
   "        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
   "            shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)\n",
   "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n",
   "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n",
   "        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
   "        losses: List[torch.Tensor] = []\n",
   "        for idx, embedding, targets in ((0, forward_embeddings, forward_targets),\n",
   "                                        (1, backward_embeddings, backward_targets)):\n",
   "            mask = targets > 0\n",
   "            non_masked_targets = targets.masked_select(mask) - 1\n",
   "            non_masked_embedding = embedding.masked_select(\n",
   "                    mask.unsqueeze(-1)\n",
   "                losses.append(self._softmax_loss(non_masked_embedding, non_masked_targets))\n",
   "                non_masked_token_embedding = self._get_target_token_embedding(token_embeddings, mask, idx)\n",
   "                losses.append(self._softmax(non_masked_embedding,\n",
   "                                            non_masked_targets,\n",
   "                                            non_masked_token_embedding))\n",
   "        return losses[0], losses[1]\n",
   "        mask = get_text_field_mask(source)\n",
   "        token_ids = source.get(\"tokens\")\n",
   "        if token_ids is None:\n",
   "        forward_targets = torch.zeros_like(token_ids)\n",
   "        backward_targets = torch.zeros_like(token_ids)\n",
   "        forward_targets[:, 0:-1] = token_ids[:, 1:]\n",
   "        backward_targets[:, 1:] = token_ids[:, 0:-1]\n",
   "        embeddings = self._text_field_embedder(source)\n",
   "        embeddings = self._layer_norm(embeddings)\n",
   "        contextual_embeddings = self._contextualizer(embeddings, mask)\n",
   "        contextual_embeddings = self._dropout(contextual_embeddings)\n",
   "        forward_loss, backward_loss = self._compute_loss(contextual_embeddings,\n",
   "                                                         embeddings,\n",
   "                                                         forward_targets,\n",
   "                                                         backward_targets)\n",
   "        num_targets = torch.sum((forward_targets > 0).long())\n",
   "        if num_targets > 0:\n",
   "            average_loss = 0.5 * (forward_loss + backward_loss) / num_targets.float()\n",
   "            average_loss = torch.tensor(0.0).to(forward_targets.device)  # pylint: disable=not-callable\n",
   "        self._last_average_loss[0] = average_loss.detach().item()\n",
   "        if num_targets > 0:\n",
   "                scale_factor = num_targets.float()\n",
   "                scale_factor = self._loss_scale\n",
   "                    'loss': average_loss * scale_factor,\n",
   "                    'forward_loss': forward_loss * scale_factor / num_targets.float(),\n",
   "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n",
   "                    'loss': average_loss,\n",
   "                    'forward_loss': average_loss,\n",
   "                    'backward_loss': average_loss\n",
   "            contextual_embeddings, mask = remove_sentence_boundaries(contextual_embeddings, mask)\n",
   "                'lm_embeddings': contextual_embeddings,\n",
   "                'mask': mask\n"
  ]
 },
 "455": {
  "name": "return_dict",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "175",
  "column": "12",
  "context": "e_loss zero tensor, return it for all\n            return_dict = {\n                    'loss': average_loss,\n        ",
  "context_lines": "                    'backward_loss': backward_loss * scale_factor / num_targets.float()\n            }\n        else:\n            # average_loss zero tensor, return it for all\n            return_dict = {\n                    'loss': average_loss,\n                    'forward_loss': average_loss,\n                    'backward_loss': average_loss\n            }\n\n",
  "slicing": [
   "            return_dict = {\n",
   "        return_dict.update({\n",
   "        return return_dict\n"
  ]
 },
 "456": {
  "name": "mismatch",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "7",
  "column": "4",
  "context": "\ndef optimistic_restore(network, state_dict):\n    mismatch = False\n    own_state = network.state_dict()\n    for name,",
  "context_lines": "import re\nfrom itertools import tee\n\nfrom num2words import num2words\n\ndef optimistic_restore(network, state_dict):\n    mismatch = False\n    own_state = network.state_dict()\n    for name, param in state_dict.items():\n        if name not in own_state:\n            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n",
  "slicing": [
   "    mismatch = False\n",
   "    return not mismatch\n"
  ]
 },
 "457": {
  "name": "own_state",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "8",
  "column": "4",
  "context": "re(network, state_dict):\n    mismatch = False\n    own_state = network.state_dict()\n    for name, param in state_dict.items():\n       ",
  "context_lines": "from itertools import tee\n\nfrom num2words import num2words\n\ndef optimistic_restore(network, state_dict):\n    mismatch = False\n    own_state = network.state_dict()\n    for name, param in state_dict.items():\n        if name not in own_state:\n            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n            mismatch = True\n",
  "slicing": [
   "    own_state = network.state_dict()\n",
   "    for name, param in state_dict.items():\n",
   "        if name not in own_state:\n",
   "            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n",
   "        elif param.size() == own_state[name].size():\n",
   "            own_state[name].copy_(param)\n",
   "            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n",
   "                                                                    own_state[name].size(),\n",
   "                                                                    param.size()))\n",
   "    missing = set(own_state.keys()) - set(state_dict.keys())\n",
   "    if len(missing) > 0:\n",
   "        print(\"We couldn'tests find {}\".format(','.join(missing)))\n"
  ]
 },
 "458": {
  "name": "mismatch",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "12",
  "column": "12",
  "context": " size {}\".format(name, param.size()))\n            mismatch = True\n        elif param.size() == own_state[name].size(",
  "context_lines": "    own_state = network.state_dict()\n    for name, param in state_dict.items():\n        if name not in own_state:\n            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n            mismatch = True\n        elif param.size() == own_state[name].size():\n            own_state[name].copy_(param)\n        else:\n            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n",
  "slicing": [
   "            mismatch = True\n",
   "    return not mismatch\n"
  ]
 },
 "459": {
  "name": "missing",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "21",
  "column": "4",
  "context": "  param.size()))\n            mismatch = True\n\n    missing = set(own_state.keys()) - set(state_dict.keys())\n    if len(missing) > 0:\n        print(\"We couldn'",
  "context_lines": "            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n                                                                    own_state[name].size(),\n                                                                    param.size()))\n            mismatch = True\n\n    missing = set(own_state.keys()) - set(state_dict.keys())\n    if len(missing) > 0:\n        print(\"We couldn't find {}\".format(','.join(missing)))\n        mismatch = True\n    return not mismatch\n\n",
  "slicing": [
   "    own_state = network.state_dict()\n",
   "    for name, param in state_dict.items():\n",
   "        if name not in own_state:\n",
   "            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n",
   "        elif param.size() == own_state[name].size():\n",
   "            own_state[name].copy_(param)\n",
   "            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n",
   "                                                                    own_state[name].size(),\n",
   "                                                                    param.size()))\n",
   "    missing = set(own_state.keys()) - set(state_dict.keys())\n",
   "    if len(missing) > 0:\n",
   "        print(\"We couldn'tests find {}\".format(','.join(missing)))\n"
  ]
 },
 "460": {
  "name": "mismatch",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "24",
  "column": "8",
  "context": "ldn't find {}\".format(','.join(missing)))\n        mismatch = True\n    return not mismatch\n\ndef pairwise(iterable):\n ",
  "context_lines": "            mismatch = True\n\n    missing = set(own_state.keys()) - set(state_dict.keys())\n    if len(missing) > 0:\n        print(\"We couldn't find {}\".format(','.join(missing)))\n        mismatch = True\n    return not mismatch\n\ndef pairwise(iterable):\n    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n    a, b = tee(iterable)\n",
  "slicing": [
   "        mismatch = True\n",
   "    return not mismatch\n"
  ]
 },
 "461": {
  "name": "mismatch",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "19",
  "column": "12",
  "context": "                       param.size()))\n            mismatch = True\n\n    missing = set(own_state.keys()) - set(state_d",
  "context_lines": "        else:\n            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n                                                                    own_state[name].size(),\n                                                                    param.size()))\n            mismatch = True\n\n    missing = set(own_state.keys()) - set(state_dict.keys())\n    if len(missing) > 0:\n        print(\"We couldn't find {}\".format(','.join(missing)))\n",
  "slicing": [
   "            mismatch = True\n",
   "    return not mismatch\n"
  ]
 },
 "462": {
  "name": "a",
  "type": "itertools.tee",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "29",
  "column": "4",
  "context": "):\n    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\ndef n2w_1k",
  "context_lines": "        mismatch = True\n    return not mismatch\n\ndef pairwise(iterable):\n    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\ndef n2w_1k(num, use_ordinal=False):\n    if num > 1000:\n",
  "slicing": [
   "    a, b = tee(iterable)\n",
   "    return zip(a, b)\n"
  ]
 },
 "463": {
  "name": "b",
  "type": "itertools.tee",
  "class": "imported",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "29",
  "column": "7",
  "context": "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\ndef n2w_1k",
  "context_lines": "        mismatch = True\n    return not mismatch\n\ndef pairwise(iterable):\n    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\ndef n2w_1k(num, use_ordinal=False):\n    if num > 1000:\n",
  "slicing": [
   "    a, b = tee(iterable)\n",
   "    next(b, None)\n",
   "    return zip(a, b)\n"
  ]
 },
 "464": {
  "name": "sentence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "44",
  "column": "4",
  "context": "ace\n    :param sentence:\n    :return:\n    \"\"\"\n    sentence = remove_allcaps(sentence)\n    # Aggressively get rid of some punctuation mar",
  "context_lines": "    make sure punctuation is followed by a space\n    :param sentence:\n    :return:\n    \"\"\"\n    sentence = remove_allcaps(sentence)\n    # Aggressively get rid of some punctuation markers\n    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n\n    # Less aggressively get rid of quotes, apostrophes\n",
  "slicing": [
   "    mismatch = False\n",
   "    own_state = network.state_dict()\n",
   "    for name, param in state_dict.items():\n",
   "        if name not in own_state:\n",
   "            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n",
   "            mismatch = True\n",
   "        elif param.size() == own_state[name].size():\n",
   "            own_state[name].copy_(param)\n",
   "            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n",
   "                                                                    own_state[name].size(),\n",
   "                                                                    param.size()))\n",
   "            mismatch = True\n",
   "    missing = set(own_state.keys()) - set(state_dict.keys())\n",
   "    if len(missing) > 0:\n",
   "        print(\"We couldn'tests find {}\".format(','.join(missing)))\n",
   "        mismatch = True\n",
   "    return not mismatch\n",
   "    a, b = tee(iterable)\n",
   "    next(b, None)\n",
   "    return zip(a, b)\n",
   "    sentence = remove_allcaps(sentence)\n",
   "                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n"
  ]
 },
 "465": {
  "name": "sent0",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "46",
  "column": "4",
  "context": "ressively get rid of some punctuation markers\n    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n                   ' ', sentence, flags=re.MULTILI",
  "context_lines": "    :return:\n    \"\"\"\n    sentence = remove_allcaps(sentence)\n    # Aggressively get rid of some punctuation markers\n    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n\n    # Less aggressively get rid of quotes, apostrophes\n    sent1 = re.sub(r'\"', ' ', sent0)\n    sent2 = re.sub(r'`', '\\'', sent1)\n\n",
  "slicing": [
   "    mismatch = False\n",
   "    own_state = network.state_dict()\n",
   "    for name, param in state_dict.items():\n",
   "        if name not in own_state:\n",
   "            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n",
   "            mismatch = True\n",
   "        elif param.size() == own_state[name].size():\n",
   "            own_state[name].copy_(param)\n",
   "            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n",
   "                                                                    own_state[name].size(),\n",
   "                                                                    param.size()))\n",
   "            mismatch = True\n",
   "    missing = set(own_state.keys()) - set(state_dict.keys())\n",
   "    if len(missing) > 0:\n",
   "        print(\"We couldn'tests find {}\".format(','.join(missing)))\n",
   "        mismatch = True\n",
   "    return not mismatch\n",
   "    a, b = tee(iterable)\n",
   "    next(b, None)\n",
   "    return zip(a, b)\n",
   "    sentence = remove_allcaps(sentence)\n",
   "    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n",
   "                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n",
   "    sent1 = re.sub(r'\"', ' ', sent0)\n",
   "    sent2 = re.sub(r'`', '\\'', sent1)\n",
   "                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n"
  ]
 },
 "466": {
  "name": "sent1",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "50",
  "column": "4",
  "context": "s aggressively get rid of quotes, apostrophes\n    sent1 = re.sub(r'\"', ' ', sent0)\n    sent2 = re.sub(r'`', '\\'', sent1)\n\n    # match",
  "context_lines": "    # Aggressively get rid of some punctuation markers\n    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n\n    # Less aggressively get rid of quotes, apostrophes\n    sent1 = re.sub(r'\"', ' ', sent0)\n    sent2 = re.sub(r'`', '\\'', sent1)\n\n    # match ordinals\n    sent3 = re.sub(r'(\\d+(?:rd|st|nd))',\n                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n\n",
  "slicing": [
   "    mismatch = False\n",
   "    own_state = network.state_dict()\n",
   "    for name, param in state_dict.items():\n",
   "        if name not in own_state:\n",
   "            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n",
   "            mismatch = True\n",
   "        elif param.size() == own_state[name].size():\n",
   "            own_state[name].copy_(param)\n",
   "            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n",
   "                                                                    own_state[name].size(),\n",
   "                                                                    param.size()))\n",
   "            mismatch = True\n",
   "    missing = set(own_state.keys()) - set(state_dict.keys())\n",
   "    if len(missing) > 0:\n",
   "        print(\"We couldn'tests find {}\".format(','.join(missing)))\n",
   "        mismatch = True\n",
   "    return not mismatch\n",
   "    a, b = tee(iterable)\n",
   "    next(b, None)\n",
   "    return zip(a, b)\n",
   "    sentence = remove_allcaps(sentence)\n",
   "    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n",
   "                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n",
   "    sent1 = re.sub(r'\"', ' ', sent0)\n",
   "    sent2 = re.sub(r'`', '\\'', sent1)\n",
   "                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n"
  ]
 },
 "467": {
  "name": "sent2",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "51",
  "column": "4",
  "context": "strophes\n    sent1 = re.sub(r'\"', ' ', sent0)\n    sent2 = re.sub(r'`', '\\'', sent1)\n\n    # match ordinals\n    sent3 = re.sub(r'(\\d+(?:",
  "context_lines": "    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n\n    # Less aggressively get rid of quotes, apostrophes\n    sent1 = re.sub(r'\"', ' ', sent0)\n    sent2 = re.sub(r'`', '\\'', sent1)\n\n    # match ordinals\n    sent3 = re.sub(r'(\\d+(?:rd|st|nd))',\n                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n\n",
  "slicing": [
   "    mismatch = False\n",
   "    own_state = network.state_dict()\n",
   "    for name, param in state_dict.items():\n",
   "        if name not in own_state:\n",
   "            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n",
   "            mismatch = True\n",
   "        elif param.size() == own_state[name].size():\n",
   "            own_state[name].copy_(param)\n",
   "            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n",
   "                                                                    own_state[name].size(),\n",
   "                                                                    param.size()))\n",
   "            mismatch = True\n",
   "    missing = set(own_state.keys()) - set(state_dict.keys())\n",
   "    if len(missing) > 0:\n",
   "        print(\"We couldn'tests find {}\".format(','.join(missing)))\n",
   "        mismatch = True\n",
   "    return not mismatch\n",
   "    a, b = tee(iterable)\n",
   "    next(b, None)\n",
   "    return zip(a, b)\n",
   "    sentence = remove_allcaps(sentence)\n",
   "    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n",
   "                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n",
   "    sent1 = re.sub(r'\"', ' ', sent0)\n",
   "    sent2 = re.sub(r'`', '\\'', sent1)\n",
   "                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n"
  ]
 },
 "468": {
  "name": "sent3",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "54",
  "column": "4",
  "context": ".sub(r'`', '\\'', sent1)\n\n    # match ordinals\n    sent3 = re.sub(r'(\\d+(?:rd|st|nd))',\n                   lambda x: n2w_1k(int(x.group(0)",
  "context_lines": "    # Less aggressively get rid of quotes, apostrophes\n    sent1 = re.sub(r'\"', ' ', sent0)\n    sent2 = re.sub(r'`', '\\'', sent1)\n\n    # match ordinals\n    sent3 = re.sub(r'(\\d+(?:rd|st|nd))',\n                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n\n    #These things all need to be followed by spaces or else we'll run into problems\n    sent4 = re.sub(r'[:;,\\\"\\!\\.\\-\\?](?! )', lambda x: x.group(0) + ' ', sent3)\n\n    #These things all need to be preceded by spaces or else we'll run into problems\n",
  "slicing": [
   "    mismatch = False\n",
   "    own_state = network.state_dict()\n",
   "    for name, param in state_dict.items():\n",
   "        if name not in own_state:\n",
   "            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n",
   "            mismatch = True\n",
   "        elif param.size() == own_state[name].size():\n",
   "            own_state[name].copy_(param)\n",
   "            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n",
   "                                                                    own_state[name].size(),\n",
   "                                                                    param.size()))\n",
   "            mismatch = True\n",
   "    missing = set(own_state.keys()) - set(state_dict.keys())\n",
   "    if len(missing) > 0:\n",
   "        print(\"We couldn'tests find {}\".format(','.join(missing)))\n",
   "        mismatch = True\n",
   "    return not mismatch\n",
   "    a, b = tee(iterable)\n",
   "    next(b, None)\n",
   "    return zip(a, b)\n",
   "def n2w_1k(num, use_ordinal=False):\n",
   "    return num2words(num, to='ordinal' if use_ordinal else 'cardinal')\n",
   "    sentence = remove_allcaps(sentence)\n",
   "    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n",
   "                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n",
   "    sent1 = re.sub(r'\"', ' ', sent0)\n",
   "    sent2 = re.sub(r'`', '\\'', sent1)\n",
   "    sent3 = re.sub(r'(\\d+(?:rd|st|nd))',\n",
   "                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n",
   "    sent4 = re.sub(r'[:;,\\\"\\!\\.\\-\\?](?! )', lambda x: x.group(0) + ' ', sent3)\n",
   "    sent5 = re.sub(r'(?! )[-]', lambda x: ' ' + x.group(0), sent4)\n",
   "    sent6 = re.sub(r'\\s\\s+', ' ', sent5)\n",
   "    sent7 = sent6.strip()\n",
   "    return sent7\n"
  ]
 },
 "469": {
  "name": "sent4",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "58",
  "column": "4",
  "context": "wed by spaces or else we'll run into problems\n    sent4 = re.sub(r'[:;,\\\"\\!\\.\\-\\?](?! )', lambda x: x.group(0) + ' ', sent3)\n\n    #These things all need to be preceded by spac",
  "context_lines": "    # match ordinals\n    sent3 = re.sub(r'(\\d+(?:rd|st|nd))',\n                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n\n    #These things all need to be followed by spaces or else we'll run into problems\n    sent4 = re.sub(r'[:;,\\\"\\!\\.\\-\\?](?! )', lambda x: x.group(0) + ' ', sent3)\n\n    #These things all need to be preceded by spaces or else we'll run into problems\n    sent5 = re.sub(r'(?! )[-]', lambda x: ' ' + x.group(0), sent4)\n\n    # Several spaces\n",
  "slicing": [
   "    mismatch = False\n",
   "    own_state = network.state_dict()\n",
   "    for name, param in state_dict.items():\n",
   "        if name not in own_state:\n",
   "            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n",
   "            mismatch = True\n",
   "        elif param.size() == own_state[name].size():\n",
   "            own_state[name].copy_(param)\n",
   "            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n",
   "                                                                    own_state[name].size(),\n",
   "                                                                    param.size()))\n",
   "            mismatch = True\n",
   "    missing = set(own_state.keys()) - set(state_dict.keys())\n",
   "    if len(missing) > 0:\n",
   "        print(\"We couldn'tests find {}\".format(','.join(missing)))\n",
   "        mismatch = True\n",
   "    return not mismatch\n",
   "    a, b = tee(iterable)\n",
   "    next(b, None)\n",
   "    return zip(a, b)\n",
   "def n2w_1k(num, use_ordinal=False):\n",
   "    return num2words(num, to='ordinal' if use_ordinal else 'cardinal')\n",
   "    sentence = remove_allcaps(sentence)\n",
   "    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n",
   "                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n",
   "    sent1 = re.sub(r'\"', ' ', sent0)\n",
   "    sent2 = re.sub(r'`', '\\'', sent1)\n",
   "    sent3 = re.sub(r'(\\d+(?:rd|st|nd))',\n",
   "                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n",
   "    sent4 = re.sub(r'[:;,\\\"\\!\\.\\-\\?](?! )', lambda x: x.group(0) + ' ', sent3)\n",
   "    sent5 = re.sub(r'(?! )[-]', lambda x: ' ' + x.group(0), sent4)\n",
   "    sent6 = re.sub(r'\\s\\s+', ' ', sent5)\n",
   "    sent7 = sent6.strip()\n",
   "    return sent7\n"
  ]
 },
 "470": {
  "name": "sent5",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "61",
  "column": "4",
  "context": "ded by spaces or else we'll run into problems\n    sent5 = re.sub(r'(?! )[-]', lambda x: ' ' + x.group(0), sent4)\n\n    # Several spaces\n    sent6 = re.sub(r'\\s\\s+',",
  "context_lines": "                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n\n    #These things all need to be followed by spaces or else we'll run into problems\n    sent4 = re.sub(r'[:;,\\\"\\!\\.\\-\\?](?! )', lambda x: x.group(0) + ' ', sent3)\n\n    #These things all need to be preceded by spaces or else we'll run into problems\n    sent5 = re.sub(r'(?! )[-]', lambda x: ' ' + x.group(0), sent4)\n\n    # Several spaces\n    sent6 = re.sub(r'\\s\\s+', ' ', sent5)\n\n    sent7 = sent6.strip()\n",
  "slicing": [
   "    mismatch = False\n",
   "    own_state = network.state_dict()\n",
   "    for name, param in state_dict.items():\n",
   "        if name not in own_state:\n",
   "            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n",
   "            mismatch = True\n",
   "        elif param.size() == own_state[name].size():\n",
   "            own_state[name].copy_(param)\n",
   "            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n",
   "                                                                    own_state[name].size(),\n",
   "                                                                    param.size()))\n",
   "            mismatch = True\n",
   "    missing = set(own_state.keys()) - set(state_dict.keys())\n",
   "    if len(missing) > 0:\n",
   "        print(\"We couldn'tests find {}\".format(','.join(missing)))\n",
   "        mismatch = True\n",
   "    return not mismatch\n",
   "    a, b = tee(iterable)\n",
   "    next(b, None)\n",
   "    return zip(a, b)\n",
   "def n2w_1k(num, use_ordinal=False):\n",
   "    return num2words(num, to='ordinal' if use_ordinal else 'cardinal')\n",
   "    sentence = remove_allcaps(sentence)\n",
   "    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n",
   "                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n",
   "    sent1 = re.sub(r'\"', ' ', sent0)\n",
   "    sent2 = re.sub(r'`', '\\'', sent1)\n",
   "    sent3 = re.sub(r'(\\d+(?:rd|st|nd))',\n",
   "                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n",
   "    sent4 = re.sub(r'[:;,\\\"\\!\\.\\-\\?](?! )', lambda x: x.group(0) + ' ', sent3)\n",
   "    sent5 = re.sub(r'(?! )[-]', lambda x: ' ' + x.group(0), sent4)\n",
   "    sent6 = re.sub(r'\\s\\s+', ' ', sent5)\n",
   "    sent7 = sent6.strip()\n",
   "    return sent7\n"
  ]
 },
 "471": {
  "name": "sent6",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "64",
  "column": "4",
  "context": " ' + x.group(0), sent4)\n\n    # Several spaces\n    sent6 = re.sub(r'\\s\\s+', ' ', sent5)\n\n    sent7 = sent6.strip()\n    return sent7\n\ndef r",
  "context_lines": "    sent4 = re.sub(r'[:;,\\\"\\!\\.\\-\\?](?! )', lambda x: x.group(0) + ' ', sent3)\n\n    #These things all need to be preceded by spaces or else we'll run into problems\n    sent5 = re.sub(r'(?! )[-]', lambda x: ' ' + x.group(0), sent4)\n\n    # Several spaces\n    sent6 = re.sub(r'\\s\\s+', ' ', sent5)\n\n    sent7 = sent6.strip()\n    return sent7\n\ndef remove_allcaps(sent):\n",
  "slicing": [
   "    mismatch = False\n",
   "    own_state = network.state_dict()\n",
   "    for name, param in state_dict.items():\n",
   "        if name not in own_state:\n",
   "            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n",
   "            mismatch = True\n",
   "        elif param.size() == own_state[name].size():\n",
   "            own_state[name].copy_(param)\n",
   "            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n",
   "                                                                    own_state[name].size(),\n",
   "                                                                    param.size()))\n",
   "            mismatch = True\n",
   "    missing = set(own_state.keys()) - set(state_dict.keys())\n",
   "    if len(missing) > 0:\n",
   "        print(\"We couldn'tests find {}\".format(','.join(missing)))\n",
   "        mismatch = True\n",
   "    return not mismatch\n",
   "    a, b = tee(iterable)\n",
   "    next(b, None)\n",
   "    return zip(a, b)\n",
   "def n2w_1k(num, use_ordinal=False):\n",
   "    return num2words(num, to='ordinal' if use_ordinal else 'cardinal')\n",
   "    sentence = remove_allcaps(sentence)\n",
   "    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n",
   "                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n",
   "    sent1 = re.sub(r'\"', ' ', sent0)\n",
   "    sent2 = re.sub(r'`', '\\'', sent1)\n",
   "    sent3 = re.sub(r'(\\d+(?:rd|st|nd))',\n",
   "                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n",
   "    sent4 = re.sub(r'[:;,\\\"\\!\\.\\-\\?](?! )', lambda x: x.group(0) + ' ', sent3)\n",
   "    sent5 = re.sub(r'(?! )[-]', lambda x: ' ' + x.group(0), sent4)\n",
   "    sent6 = re.sub(r'\\s\\s+', ' ', sent5)\n",
   "    sent7 = sent6.strip()\n",
   "    return sent7\n"
  ]
 },
 "472": {
  "name": "sent7",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "66",
  "column": "4",
  "context": "ces\n    sent6 = re.sub(r'\\s\\s+', ' ', sent5)\n\n    sent7 = sent6.strip()\n    return sent7\n\ndef remove_allcaps(sent):\n    \"\"",
  "context_lines": "    #These things all need to be preceded by spaces or else we'll run into problems\n    sent5 = re.sub(r'(?! )[-]', lambda x: ' ' + x.group(0), sent4)\n\n    # Several spaces\n    sent6 = re.sub(r'\\s\\s+', ' ', sent5)\n\n    sent7 = sent6.strip()\n    return sent7\n\ndef remove_allcaps(sent):\n    \"\"\"\n    Given a sentence, filter it so that it doesn't contain some words that are ALLcaps\n",
  "slicing": [
   "    mismatch = False\n",
   "    own_state = network.state_dict()\n",
   "    for name, param in state_dict.items():\n",
   "        if name not in own_state:\n",
   "            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n",
   "            mismatch = True\n",
   "        elif param.size() == own_state[name].size():\n",
   "            own_state[name].copy_(param)\n",
   "            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n",
   "                                                                    own_state[name].size(),\n",
   "                                                                    param.size()))\n",
   "            mismatch = True\n",
   "    missing = set(own_state.keys()) - set(state_dict.keys())\n",
   "    if len(missing) > 0:\n",
   "        print(\"We couldn'tests find {}\".format(','.join(missing)))\n",
   "        mismatch = True\n",
   "    return not mismatch\n",
   "    a, b = tee(iterable)\n",
   "    next(b, None)\n",
   "    return zip(a, b)\n",
   "def n2w_1k(num, use_ordinal=False):\n",
   "    return num2words(num, to='ordinal' if use_ordinal else 'cardinal')\n",
   "    sentence = remove_allcaps(sentence)\n",
   "    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n",
   "                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n",
   "    sent1 = re.sub(r'\"', ' ', sent0)\n",
   "    sent2 = re.sub(r'`', '\\'', sent1)\n",
   "    sent3 = re.sub(r'(\\d+(?:rd|st|nd))',\n",
   "                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n",
   "    sent4 = re.sub(r'[:;,\\\"\\!\\.\\-\\?](?! )', lambda x: x.group(0) + ' ', sent3)\n",
   "    sent5 = re.sub(r'(?! )[-]', lambda x: ' ' + x.group(0), sent4)\n",
   "    sent6 = re.sub(r'\\s\\s+', ' ', sent5)\n",
   "    sent7 = sent6.strip()\n",
   "    return sent7\n"
  ]
 },
 "473": {
  "name": "num_capitals",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "79",
  "column": "8",
  "context": "  if word == \"I\":\n            return word\n        num_capitals = len([x for x in word if not x.islower()])\n        if num_capitals > len(word) // 2:\n        ",
  "context_lines": "    # Remove all caps\n    def _sanitize(word, is_first):\n        if word == \"I\":\n            return word\n        num_capitals = len([x for x in word if not x.islower()])\n        if num_capitals > len(word) // 2:\n            # We have an all caps word here.\n            if is_first:\n                return word[0] + word[1:].lower()\n",
  "slicing": [
   "    mismatch = False\n",
   "    own_state = network.state_dict()\n",
   "    for name, param in state_dict.items():\n",
   "        if name not in own_state:\n",
   "            print(\"Unexpected key {} in state_dict with size {}\".format(name, param.size()))\n",
   "            mismatch = True\n",
   "        elif param.size() == own_state[name].size():\n",
   "            own_state[name].copy_(param)\n",
   "            print(\"Network has {} with size {}, ckpt has {}\".format(name,\n",
   "                                                                    own_state[name].size(),\n",
   "                                                                    param.size()))\n",
   "            mismatch = True\n",
   "    missing = set(own_state.keys()) - set(state_dict.keys())\n",
   "    if len(missing) > 0:\n",
   "        print(\"We couldn'tests find {}\".format(','.join(missing)))\n",
   "        mismatch = True\n",
   "    return not mismatch\n",
   "    a, b = tee(iterable)\n",
   "    next(b, None)\n",
   "    return zip(a, b)\n",
   "def n2w_1k(num, use_ordinal=False):\n",
   "    return num2words(num, to='ordinal' if use_ordinal else 'cardinal')\n",
   "    sentence = remove_allcaps(sentence)\n",
   "    sent0 = re.sub(r'^.*(\\\\|/|!!!|~|=|#|@|\\*|¡|©|¿|«|»|¬|{|}|\\||\\(|\\)|\\+|\\]|\\[).*$',\n",
   "                   ' ', sentence, flags=re.MULTILINE|re.IGNORECASE)\n",
   "    sent1 = re.sub(r'\"', ' ', sent0)\n",
   "    sent2 = re.sub(r'`', '\\'', sent1)\n",
   "    sent3 = re.sub(r'(\\d+(?:rd|st|nd))',\n",
   "                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n",
   "    sent4 = re.sub(r'[:;,\\\"\\!\\.\\-\\?](?! )', lambda x: x.group(0) + ' ', sent3)\n",
   "    sent5 = re.sub(r'(?! )[-]', lambda x: ' ' + x.group(0), sent4)\n",
   "    sent6 = re.sub(r'\\s\\s+', ' ', sent5)\n",
   "    sent7 = sent6.strip()\n",
   "    return sent7\n",
   "        num_capitals = len([x for x in word if not x.islower()])\n",
   "        if num_capitals > len(word) // 2:\n"
  ]
 },
 "474": {
  "name": "logger",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/__init__.py",
  "lineno": "11",
  "column": "0",
  "context": "rialnlp.commands.test_install import TestInstall\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef main(prog: str = None,\n         subcommand_o",
  "context_lines": "from allennlp.commands.subcommand import Subcommand\nfrom allennlp.common.util import import_submodules\n\nfrom adversarialnlp import __version__\nfrom adversarialnlp.commands.test_install import TestInstall\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef main(prog: str = None,\n         subcommand_overrides: Dict[str, Subcommand] = {}) -> None:\n    \"\"\"\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n"
  ]
 },
 "475": {
  "name": "parser",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/__init__.py",
  "lineno": "20",
  "column": "4",
  "context": "    # pylint: disable=dangerous-default-value\n    parser = argparse.ArgumentParser(description=\"Run AdversarialNLP\", usage='%(prog)s', prog=prog)\n    parser.add_argument('--version', action='versi",
  "context_lines": "    \"\"\"\n    :mod:`~adversarialnlp.run` command.\n    \"\"\"\n    # pylint: disable=dangerous-default-value\n    parser = argparse.ArgumentParser(description=\"Run AdversarialNLP\", usage='%(prog)s', prog=prog)\n    parser.add_argument('--version', action='version', version='%(prog)s ' + __version__)\n\n    subparsers = parser.add_subparsers(title='Commands', metavar='')\n\n    subcommands = {\n            # Default commands\n",
  "slicing": [
   "    parser = argparse.ArgumentParser(description=\"Run AdversarialNLP\", usage='%(prog)s', prog=prog)\n",
   "    parser.add_argument('--version', action='version', version='%(prog)s ' + __version__)\n",
   "    subparsers = parser.add_subparsers(title='Commands', metavar='')\n",
   "    subcommands = {\n",
   "    for name, subcommand in subcommands.items():\n",
   "        subparser = subcommand.add_subparser(name, subparsers)\n",
   "        if name != \"configure\":\n",
   "            subparser.add_argument('--include-package',\n",
   "    args = parser.parse_args()\n",
   "    if 'func' in dir(args):\n",
   "        for package_name in getattr(args, 'include_package', ()):\n",
   "            import_submodules(package_name)\n",
   "        args.func(args)\n",
   "        parser.print_help()\n"
  ]
 },
 "476": {
  "name": "subparsers",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/__init__.py",
  "lineno": "23",
  "column": "4",
  "context": "version', version='%(prog)s ' + __version__)\n\n    subparsers = parser.add_subparsers(title='Commands', metavar='')\n\n    subcommands = {\n            # Default command",
  "context_lines": "    \"\"\"\n    # pylint: disable=dangerous-default-value\n    parser = argparse.ArgumentParser(description=\"Run AdversarialNLP\", usage='%(prog)s', prog=prog)\n    parser.add_argument('--version', action='version', version='%(prog)s ' + __version__)\n\n    subparsers = parser.add_subparsers(title='Commands', metavar='')\n\n    subcommands = {\n            # Default commands\n            \"test-install\": TestInstall(),\n\n",
  "slicing": [
   "    parser = argparse.ArgumentParser(description=\"Run AdversarialNLP\", usage='%(prog)s', prog=prog)\n",
   "    parser.add_argument('--version', action='version', version='%(prog)s ' + __version__)\n",
   "    subparsers = parser.add_subparsers(title='Commands', metavar='')\n",
   "    subcommands = {\n",
   "    for name, subcommand in subcommands.items():\n",
   "        subparser = subcommand.add_subparser(name, subparsers)\n",
   "        if name != \"configure\":\n",
   "            subparser.add_argument('--include-package',\n",
   "    args = parser.parse_args()\n",
   "    if 'func' in dir(args):\n",
   "        for package_name in getattr(args, 'include_package', ()):\n",
   "            import_submodules(package_name)\n",
   "        args.func(args)\n",
   "        parser.print_help()\n"
  ]
 },
 "477": {
  "name": "subcommands",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/commands/__init__.py",
  "lineno": "25",
  "column": "4",
  "context": "add_subparsers(title='Commands', metavar='')\n\n    subcommands = {\n            # Default commands\n            \"test-i",
  "context_lines": "    # pylint: disable=dangerous-default-value\n    parser = argparse.ArgumentParser(description=\"Run AdversarialNLP\", usage='%(prog)s', prog=prog)\n    parser.add_argument('--version', action='version', version='%(prog)s ' + __version__)\n\n    subparsers = parser.add_subparsers(title='Commands', metavar='')\n\n    subcommands = {\n            # Default commands\n            \"test-install\": TestInstall(),\n\n            # Superseded by overrides\n            **subcommand_overrides\n",
  "slicing": [
   "    parser = argparse.ArgumentParser(description=\"Run AdversarialNLP\", usage='%(prog)s', prog=prog)\n",
   "    parser.add_argument('--version', action='version', version='%(prog)s ' + __version__)\n",
   "    subparsers = parser.add_subparsers(title='Commands', metavar='')\n",
   "    subcommands = {\n",
   "    for name, subcommand in subcommands.items():\n",
   "        subparser = subcommand.add_subparser(name, subparsers)\n",
   "        if name != \"configure\":\n",
   "            subparser.add_argument('--include-package',\n",
   "    args = parser.parse_args()\n",
   "    if 'func' in dir(args):\n",
   "        for package_name in getattr(args, 'include_package', ()):\n",
   "            import_submodules(package_name)\n",
   "        args.func(args)\n",
   "        parser.print_help()\n"
  ]
 },
 "478": {
  "name": "subparser",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/__init__.py",
  "lineno": "34",
  "column": "8",
  "context": " name, subcommand in subcommands.items():\n        subparser = subcommand.add_subparser(name, subparsers)\n        # configure doesn't need include-package b",
  "context_lines": "            # Superseded by overrides\n            **subcommand_overrides\n    }\n\n    for name, subcommand in subcommands.items():\n        subparser = subcommand.add_subparser(name, subparsers)\n        # configure doesn't need include-package because it imports\n        # whatever classes it needs.\n        if name != \"configure\":\n            subparser.add_argument('--include-package',\n",
  "slicing": [
   "        description = '''Test that installation works by running the unit tests.'''\n",
   "        subparser = parser.add_parser(\n",
   "                name, description=description, help='Run the unit tests.')\n",
   "        subparser.add_argument('--run-all', action=\"store_true\",\n",
   "        subparser.set_defaults(func=_run_test)\n",
   "        return subparser\n"
  ]
 },
 "479": {
  "name": "args",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/__init__.py",
  "lineno": "44",
  "column": "4",
  "context": "      help='additional packages to include')\n\n    args = parser.parse_args()\n\n    # If a subparser is triggered, it adds its wo",
  "context_lines": "                                   type=str,\n                                   action='append',\n                                   default=[],\n                                   help='additional packages to include')\n\n    args = parser.parse_args()\n\n    # If a subparser is triggered, it adds its work as `args.func`.\n    # So if no such attribute has been added, no subparser was triggered,\n    # so give the user some help.\n",
  "slicing": [
   "    parser = argparse.ArgumentParser(description=\"Run AdversarialNLP\", usage='%(prog)s', prog=prog)\n",
   "    parser.add_argument('--version', action='version', version='%(prog)s ' + __version__)\n",
   "    subparsers = parser.add_subparsers(title='Commands', metavar='')\n",
   "    subcommands = {\n",
   "    for name, subcommand in subcommands.items():\n",
   "        subparser = subcommand.add_subparser(name, subparsers)\n",
   "        if name != \"configure\":\n",
   "            subparser.add_argument('--include-package',\n",
   "    args = parser.parse_args()\n",
   "    if 'func' in dir(args):\n",
   "        for package_name in getattr(args, 'include_package', ()):\n",
   "            import_submodules(package_name)\n",
   "        args.func(args)\n",
   "        parser.print_help()\n"
  ]
 },
 "480": {
  "name": "logger",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/test_install.py",
  "lineno": "28",
  "column": "0",
  "context": "command import Subcommand\n\nimport adversarialnlp\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nclass TestInstall(Subcommand):\n    def add_subpar",
  "context_lines": "import pathlib\n\nimport pytest\n\nfrom allennlp.commands.subcommand import Subcommand\n\nimport adversarialnlp\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nclass TestInstall(Subcommand):\n    def add_subparser(self, name: str, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        # pylint: disable=protected-access\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "    logger.info(\"Changing directory to %s\", module_parent)\n",
   "    logger.info(\"Running tests at %s\", test_dir)\n"
  ]
 },
 "481": {
  "name": "description",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/test_install.py",
  "lineno": "33",
  "column": "8",
  "context": "       # pylint: disable=protected-access\n        description = '''Test that installation works by running the unit tests.'''\n        subparser = parser.add_parser(\n           ",
  "context_lines": "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nclass TestInstall(Subcommand):\n    def add_subparser(self, name: str, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        # pylint: disable=protected-access\n        description = '''Test that installation works by running the unit tests.'''\n        subparser = parser.add_parser(\n                name, description=description, help='Run the unit tests.')\n\n        subparser.add_argument('--run-all', action=\"store_true\",\n                               help=\"By default, we skip tests that are slow \"\n",
  "slicing": [
   "        description = '''Test that installation works by running the unit tests.'''\n",
   "                name, description=description, help='Run the unit tests.')\n"
  ]
 },
 "482": {
  "name": "subparser",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/test_install.py",
  "lineno": "34",
  "column": "8",
  "context": "ation works by running the unit tests.'''\n        subparser = parser.add_parser(\n                name, description=description, hel",
  "context_lines": "class TestInstall(Subcommand):\n    def add_subparser(self, name: str, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        # pylint: disable=protected-access\n        description = '''Test that installation works by running the unit tests.'''\n        subparser = parser.add_parser(\n                name, description=description, help='Run the unit tests.')\n\n        subparser.add_argument('--run-all', action=\"store_true\",\n                               help=\"By default, we skip tests that are slow \"\n                               \"or download large files. This flag will run all tests.\")\n\n",
  "slicing": [
   "        description = '''Test that installation works by running the unit tests.'''\n",
   "        subparser = parser.add_parser(\n",
   "                name, description=description, help='Run the unit tests.')\n",
   "        subparser.add_argument('--run-all', action=\"store_true\",\n",
   "        subparser.set_defaults(func=_run_test)\n",
   "        return subparser\n"
  ]
 },
 "483": {
  "name": "initial_working_dir",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/test_install.py",
  "lineno": "51",
  "column": "4",
  "context": "nt\n\n\ndef _run_test(args: argparse.Namespace):\n    initial_working_dir = os.getcwd()\n    module_parent = _get_module_root().parent\n    ",
  "context_lines": "        return subparser\n\n\ndef _get_module_root():\n    return pathlib.Path(adversarialnlp.__file__).parent\n\n\ndef _run_test(args: argparse.Namespace):\n    initial_working_dir = os.getcwd()\n    module_parent = _get_module_root().parent\n    logger.info(\"Changing directory to %s\", module_parent)\n    os.chdir(module_parent)\n    test_dir = os.path.join(module_parent, \"adversarialnlp\")\n",
  "slicing": [
   "        description = '''Test that installation works by running the unit tests.'''\n",
   "        subparser = parser.add_parser(\n",
   "                name, description=description, help='Run the unit tests.')\n",
   "        subparser.add_argument('--run-all', action=\"store_true\",\n",
   "        subparser.set_defaults(func=_run_test)\n",
   "        return subparser\n",
   "    initial_working_dir = os.getcwd()\n",
   "    os.chdir(initial_working_dir)\n"
  ]
 },
 "484": {
  "name": "module_parent",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/test_install.py",
  "lineno": "52",
  "column": "4",
  "context": "space):\n    initial_working_dir = os.getcwd()\n    module_parent = _get_module_root().parent\n    logger.info(\"Changing directory to %s\", module",
  "context_lines": "def _get_module_root():\n    return pathlib.Path(adversarialnlp.__file__).parent\n\n\ndef _run_test(args: argparse.Namespace):\n    initial_working_dir = os.getcwd()\n    module_parent = _get_module_root().parent\n    logger.info(\"Changing directory to %s\", module_parent)\n    os.chdir(module_parent)\n    test_dir = os.path.join(module_parent, \"adversarialnlp\")\n    logger.info(\"Running tests at %s\", test_dir)\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        description = '''Test that installation works by running the unit tests.'''\n",
   "        subparser = parser.add_parser(\n",
   "                name, description=description, help='Run the unit tests.')\n",
   "        subparser.add_argument('--run-all', action=\"store_true\",\n",
   "        subparser.set_defaults(func=_run_test)\n",
   "        return subparser\n",
   "    module_parent = _get_module_root().parent\n",
   "    logger.info(\"Changing directory to %s\", module_parent)\n",
   "    os.chdir(module_parent)\n",
   "    test_dir = os.path.join(module_parent, \"adversarialnlp\")\n",
   "    logger.info(\"Running tests at %s\", test_dir)\n",
   "        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not notebooks_test'])\n",
   "        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not sniff_test and not notebooks_test',\n",
   "    exit(exit_code)\n"
  ]
 },
 "485": {
  "name": "test_dir",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/test_install.py",
  "lineno": "55",
  "column": "4",
  "context": "\", module_parent)\n    os.chdir(module_parent)\n    test_dir = os.path.join(module_parent, \"adversarialnlp\")\n    logger.info(\"Running tests at %s\", test_dir)\n ",
  "context_lines": "    initial_working_dir = os.getcwd()\n    module_parent = _get_module_root().parent\n    logger.info(\"Changing directory to %s\", module_parent)\n    os.chdir(module_parent)\n    test_dir = os.path.join(module_parent, \"adversarialnlp\")\n    logger.info(\"Running tests at %s\", test_dir)\n    if args.run_all:\n        # TODO(nfliu): remove this when notebooks have been rewritten as markdown.\n        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not notebooks_test'])\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        description = '''Test that installation works by running the unit tests.'''\n",
   "        subparser = parser.add_parser(\n",
   "                name, description=description, help='Run the unit tests.')\n",
   "        subparser.add_argument('--run-all', action=\"store_true\",\n",
   "        subparser.set_defaults(func=_run_test)\n",
   "        return subparser\n",
   "    module_parent = _get_module_root().parent\n",
   "    logger.info(\"Changing directory to %s\", module_parent)\n",
   "    os.chdir(module_parent)\n",
   "    test_dir = os.path.join(module_parent, \"adversarialnlp\")\n",
   "    logger.info(\"Running tests at %s\", test_dir)\n",
   "        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not notebooks_test'])\n",
   "        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not sniff_test and not notebooks_test',\n",
   "    exit(exit_code)\n"
  ]
 },
 "486": {
  "name": "exit_code",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/test_install.py",
  "lineno": "59",
  "column": "8",
  "context": "otebooks have been rewritten as markdown.\n        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not notebooks_test'])\n    else:\n        exit_code = pytest.main([test_di",
  "context_lines": "    test_dir = os.path.join(module_parent, \"adversarialnlp\")\n    logger.info(\"Running tests at %s\", test_dir)\n    if args.run_all:\n        # TODO(nfliu): remove this when notebooks have been rewritten as markdown.\n        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not notebooks_test'])\n    else:\n        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not sniff_test and not notebooks_test',\n                                 '-m', 'not java'])\n    # Change back to original working directory after running tests\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        description = '''Test that installation works by running the unit tests.'''\n",
   "        subparser = parser.add_parser(\n",
   "                name, description=description, help='Run the unit tests.')\n",
   "        subparser.add_argument('--run-all', action=\"store_true\",\n",
   "        subparser.set_defaults(func=_run_test)\n",
   "        return subparser\n",
   "    module_parent = _get_module_root().parent\n",
   "    logger.info(\"Changing directory to %s\", module_parent)\n",
   "    os.chdir(module_parent)\n",
   "    test_dir = os.path.join(module_parent, \"adversarialnlp\")\n",
   "    logger.info(\"Running tests at %s\", test_dir)\n",
   "        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not notebooks_test'])\n",
   "        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not sniff_test and not notebooks_test',\n",
   "    exit(exit_code)\n"
  ]
 },
 "487": {
  "name": "exit_code",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/test_install.py",
  "lineno": "61",
  "column": "8",
  "context": "', '-k', 'not notebooks_test'])\n    else:\n        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not sniff_test and not notebooks_test',\n                                 '-m', 'not java']",
  "context_lines": "    if args.run_all:\n        # TODO(nfliu): remove this when notebooks have been rewritten as markdown.\n        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not notebooks_test'])\n    else:\n        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not sniff_test and not notebooks_test',\n                                 '-m', 'not java'])\n    # Change back to original working directory after running tests\n    os.chdir(initial_working_dir)\n",
  "slicing": [
   "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
   "        description = '''Test that installation works by running the unit tests.'''\n",
   "        subparser = parser.add_parser(\n",
   "                name, description=description, help='Run the unit tests.')\n",
   "        subparser.add_argument('--run-all', action=\"store_true\",\n",
   "        subparser.set_defaults(func=_run_test)\n",
   "        return subparser\n",
   "    module_parent = _get_module_root().parent\n",
   "    logger.info(\"Changing directory to %s\", module_parent)\n",
   "    os.chdir(module_parent)\n",
   "    test_dir = os.path.join(module_parent, \"adversarialnlp\")\n",
   "    logger.info(\"Running tests at %s\", test_dir)\n",
   "        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not notebooks_test'])\n",
   "        exit_code = pytest.main([test_dir, '--color=no', '-k', 'not sniff_test and not notebooks_test',\n",
   "    exit(exit_code)\n"
  ]
 },
 "488": {
  "name": "MODULE_ROOT",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "20",
  "column": "0",
  "context": "datetime\nimport os\nimport shutil\nimport requests\n\nMODULE_ROOT = Path(__file__).parent.parent\nFIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures",
  "context_lines": "import datetime\nimport os\nimport shutil\nimport requests\n\nMODULE_ROOT = Path(__file__).parent.parent\nFIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\nPACKAGE_ROOT = MODULE_ROOT.parent\nDATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n\nclass ProgressLogger(object):\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "489": {
  "name": "FIXTURES_ROOT",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "21",
  "column": "0",
  "context": "uests\n\nMODULE_ROOT = Path(__file__).parent.parent\nFIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\nPACKAGE_ROOT = MODULE_ROOT.parent\nDATA_ROOT = (PAC",
  "context_lines": "import os\nimport shutil\nimport requests\n\nMODULE_ROOT = Path(__file__).parent.parent\nFIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\nPACKAGE_ROOT = MODULE_ROOT.parent\nDATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n\nclass ProgressLogger(object):\n    \"\"\"Throttles and display progress in human readable form.\"\"\"\n\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "490": {
  "name": "PACKAGE_ROOT",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "22",
  "column": "0",
  "context": " = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\nPACKAGE_ROOT = MODULE_ROOT.parent\nDATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n\ncla",
  "context_lines": "import shutil\nimport requests\n\nMODULE_ROOT = Path(__file__).parent.parent\nFIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\nPACKAGE_ROOT = MODULE_ROOT.parent\nDATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n\nclass ProgressLogger(object):\n    \"\"\"Throttles and display progress in human readable form.\"\"\"\n\n    def __init__(self, throttle=1, should_humanize=True):\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "491": {
  "name": "DATA_ROOT",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "23",
  "column": "0",
  "context": "res\").resolve()\nPACKAGE_ROOT = MODULE_ROOT.parent\nDATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n\nclass ProgressLogger(object):\n    \"\"\"Throttles an",
  "context_lines": "import requests\n\nMODULE_ROOT = Path(__file__).parent.parent\nFIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\nPACKAGE_ROOT = MODULE_ROOT.parent\nDATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n\nclass ProgressLogger(object):\n    \"\"\"Throttles and display progress in human readable form.\"\"\"\n\n    def __init__(self, throttle=1, should_humanize=True):\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "492": {
  "name": "num",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "44",
  "column": "12",
  "context": "urn \"%3.1f%s%s\" % (num, unit, suffix)\n            num /= 1024.0\n        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n\n ",
  "context_lines": "            return num\n        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n            if abs(num) < 1024.0:\n                return \"%3.1f%s%s\" % (num, unit, suffix)\n            num /= 1024.0\n        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n\n    def log(self, curr, total, width=40, force=False):\n        \"\"\"Display a bar showing the current progress.\"\"\"\n        if curr == 0 and total == -1:\n",
  "slicing": [
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n"
  ]
 },
 "493": {
  "name": "curr_time",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "52",
  "column": "8",
  "context": "his file ]', end='\\r')\n            return\n        curr_time = time.time()\n        if not force and curr_time - self.latest <",
  "context_lines": "        \"\"\"Display a bar showing the current progress.\"\"\"\n        if curr == 0 and total == -1:\n            print('[ no data received for this file ]', end='\\r')\n            return\n        curr_time = time.time()\n        if not force and curr_time - self.latest < self.throttle_speed:\n            return\n        else:\n            self.latest = curr_time\n\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "494": {
  "name": "done",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "59",
  "column": "8",
  "context": "urr_time\n\n        self.latest = curr_time\n        done = min(curr * width // total, width)\n        remain = width - done\n\n        if self.sho",
  "context_lines": "            return\n        else:\n            self.latest = curr_time\n\n        self.latest = curr_time\n        done = min(curr * width // total, width)\n        remain = width - done\n\n        if self.should_humanize:\n            curr = self.humanize(curr)\n            total = self.humanize(total)\n\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "495": {
  "name": "remain",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "60",
  "column": "8",
  "context": " done = min(curr * width // total, width)\n        remain = width - done\n\n        if self.should_humanize:\n            curr",
  "context_lines": "        else:\n            self.latest = curr_time\n\n        self.latest = curr_time\n        done = min(curr * width // total, width)\n        remain = width - done\n\n        if self.should_humanize:\n            curr = self.humanize(curr)\n            total = self.humanize(total)\n\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "496": {
  "name": "curr",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "63",
  "column": "12",
  "context": "one\n\n        if self.should_humanize:\n            curr = self.humanize(curr)\n            total = self.humanize(total)\n\n        ",
  "context_lines": "        self.latest = curr_time\n        done = min(curr * width // total, width)\n        remain = width - done\n\n        if self.should_humanize:\n            curr = self.humanize(curr)\n            total = self.humanize(total)\n\n        progress = '[{}{}] {} / {}'.format(\n            ''.join(['|'] * done),\n            ''.join(['.'] * remain),\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "497": {
  "name": "total",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "64",
  "column": "12",
  "context": "           curr = self.humanize(curr)\n            total = self.humanize(total)\n\n        progress = '[{}{}] {} / {}'.format(\n     ",
  "context_lines": "        done = min(curr * width // total, width)\n        remain = width - done\n\n        if self.should_humanize:\n            curr = self.humanize(curr)\n            total = self.humanize(total)\n\n        progress = '[{}{}] {} / {}'.format(\n            ''.join(['|'] * done),\n            ''.join(['.'] * remain),\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "498": {
  "name": "progress",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "66",
  "column": "8",
  "context": "            total = self.humanize(total)\n\n        progress = '[{}{}] {} / {}'.format(\n            ''.join(['|'] * done),\n            ''.",
  "context_lines": "        remain = width - done\n\n        if self.should_humanize:\n            curr = self.humanize(curr)\n            total = self.humanize(total)\n\n        progress = '[{}{}] {} / {}'.format(\n            ''.join(['|'] * done),\n            ''.join(['.'] * remain),\n            curr,\n            total\n",
  "slicing": [
   "        progress = '[{}{}] {} / {}'.format(\n",
   "        print(progress, end='\\r')\n"
  ]
 },
 "499": {
  "name": "built_file_path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "79",
  "column": "4",
  "context": "version\n    is regarded as not built.\n    \"\"\"\n    built_file_path = os.path.join(path, '.built')\n    if not os.path.isfile(built_file_path):\n      ",
  "context_lines": "    \"\"\"Checks if '.built' flag has been set for that task.\n    If a version_string is provided, this has to match, or the version\n    is regarded as not built.\n    \"\"\"\n    built_file_path = os.path.join(path, '.built')\n    if not os.path.isfile(built_file_path):\n        return False\n    else:\n        with open(built_file_path, 'r') as built_file:\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "500": {
  "name": "text",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "84",
  "column": "12",
  "context": "(built_file_path, 'r') as built_file:\n            text = built_file.read().split('\\n')\n        if len(text) <= 2:\n            return Fals",
  "context_lines": "    if not os.path.isfile(built_file_path):\n        return False\n    else:\n        with open(built_file_path, 'r') as built_file:\n            text = built_file.read().split('\\n')\n        if len(text) <= 2:\n            return False\n        for fname in text[1:-1]:\n            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "501": {
  "name": "fname",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "100",
  "column": "12",
  "context": "oday()))\n        for fname in fnames:\n            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n            built_file.write('\\n' + fname)\n       ",
  "context_lines": "    \"\"\"\n    with open(os.path.join(path, '.built'), 'w') as built_file:\n        built_file.write(str(datetime.datetime.today()))\n        for fname in fnames:\n            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n            built_file.write('\\n' + fname)\n        built_file.write('\\n' + version_string)\n\n\ndef download(url, path, fname, redownload=False):\n    \"\"\"Downloads file using `requests`. If ``redownload`` is set to false, then\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "502": {
  "name": "outfile",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "108",
  "column": "4",
  "context": "again if it is present (default ``True``).\"\"\"\n    outfile = os.path.join(path, fname)\n    curr_download = not os.path.isfile(outfile) or",
  "context_lines": "        built_file.write('\\n' + version_string)\n\n\ndef download(url, path, fname, redownload=False):\n    \"\"\"Downloads file using `requests`. If ``redownload`` is set to false, then\n    will not download tar file again if it is present (default ``True``).\"\"\"\n    outfile = os.path.join(path, fname)\n    curr_download = not os.path.isfile(outfile) or redownload\n    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n    retry = 5\n    exp_backoff = [2 ** r for r in reversed(range(retry))]\n\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "503": {
  "name": "curr_download",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "109",
  "column": "4",
  "context": ").\"\"\"\n    outfile = os.path.join(path, fname)\n    curr_download = not os.path.isfile(outfile) or redownload\n    print(\"[ downloading: \" + url + \" to \" + outfi",
  "context_lines": "def download(url, path, fname, redownload=False):\n    \"\"\"Downloads file using `requests`. If ``redownload`` is set to false, then\n    will not download tar file again if it is present (default ``True``).\"\"\"\n    outfile = os.path.join(path, fname)\n    curr_download = not os.path.isfile(outfile) or redownload\n    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n    retry = 5\n    exp_backoff = [2 ** r for r in reversed(range(retry))]\n\n    logger = ProgressLogger()\n\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "504": {
  "name": "retry",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "111",
  "column": "4",
  "context": "wnloading: \" + url + \" to \" + outfile + \" ]\")\n    retry = 5\n    exp_backoff = [2 ** r for r in reversed(range(",
  "context_lines": "    will not download tar file again if it is present (default ``True``).\"\"\"\n    outfile = os.path.join(path, fname)\n    curr_download = not os.path.isfile(outfile) or redownload\n    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n    retry = 5\n    exp_backoff = [2 ** r for r in reversed(range(retry))]\n\n    logger = ProgressLogger()\n\n    while curr_download and retry >= 0:\n        resume_file = outfile + '.part'\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "505": {
  "name": "exp_backoff",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "112",
  "column": "4",
  "context": " url + \" to \" + outfile + \" ]\")\n    retry = 5\n    exp_backoff = [2 ** r for r in reversed(range(retry))]\n\n    logger = ProgressLogger()\n\n    while curr_dow",
  "context_lines": "    outfile = os.path.join(path, fname)\n    curr_download = not os.path.isfile(outfile) or redownload\n    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n    retry = 5\n    exp_backoff = [2 ** r for r in reversed(range(retry))]\n\n    logger = ProgressLogger()\n\n    while curr_download and retry >= 0:\n        resume_file = outfile + '.part'\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "506": {
  "name": "logger",
  "type": "ProgressLogger",
  "class": "customized",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "114",
  "column": "4",
  "context": "f = [2 ** r for r in reversed(range(retry))]\n\n    logger = ProgressLogger()\n\n    while curr_download and retry >= 0:\n        r",
  "context_lines": "    curr_download = not os.path.isfile(outfile) or redownload\n    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n    retry = 5\n    exp_backoff = [2 ** r for r in reversed(range(retry))]\n\n    logger = ProgressLogger()\n\n    while curr_download and retry >= 0:\n        resume_file = outfile + '.part'\n        resume = os.path.isfile(resume_file)\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "507": {
  "name": "resume_file",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "117",
  "column": "8",
  "context": "\n\n    while curr_download and retry >= 0:\n        resume_file = outfile + '.part'\n        resume = os.path.isfile(resume_file)\n     ",
  "context_lines": "    retry = 5\n    exp_backoff = [2 ** r for r in reversed(range(retry))]\n\n    logger = ProgressLogger()\n\n    while curr_download and retry >= 0:\n        resume_file = outfile + '.part'\n        resume = os.path.isfile(resume_file)\n        if resume:\n            resume_pos = os.path.getsize(resume_file)\n            mode = 'ab'\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "508": {
  "name": "resume",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "118",
  "column": "8",
  "context": ":\n        resume_file = outfile + '.part'\n        resume = os.path.isfile(resume_file)\n        if resume:\n            resume_pos = os.pat",
  "context_lines": "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n\n    logger = ProgressLogger()\n\n    while curr_download and retry >= 0:\n        resume_file = outfile + '.part'\n        resume = os.path.isfile(resume_file)\n        if resume:\n            resume_pos = os.path.getsize(resume_file)\n            mode = 'ab'\n        else:\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "509": {
  "name": "resume_pos",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "120",
  "column": "12",
  "context": "sfile(resume_file)\n        if resume:\n            resume_pos = os.path.getsize(resume_file)\n            mode = 'ab'\n        else:\n            ",
  "context_lines": "    while curr_download and retry >= 0:\n        resume_file = outfile + '.part'\n        resume = os.path.isfile(resume_file)\n        if resume:\n            resume_pos = os.path.getsize(resume_file)\n            mode = 'ab'\n        else:\n            resume_pos = 0\n            mode = 'wb'\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "510": {
  "name": "mode",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "121",
  "column": "12",
  "context": "me_pos = os.path.getsize(resume_file)\n            mode = 'ab'\n        else:\n            resume_pos = 0\n         ",
  "context_lines": "        resume_file = outfile + '.part'\n        resume = os.path.isfile(resume_file)\n        if resume:\n            resume_pos = os.path.getsize(resume_file)\n            mode = 'ab'\n        else:\n            resume_pos = 0\n            mode = 'wb'\n        response = None\n\n",
  "slicing": [
   "            mode = 'ab'\n",
   "                with open(resume_file, mode) as f:\n"
  ]
 },
 "511": {
  "name": "response",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "125",
  "column": "8",
  "context": "   resume_pos = 0\n            mode = 'wb'\n        response = None\n\n        with requests.Session() as session:\n     ",
  "context_lines": "            mode = 'ab'\n        else:\n            resume_pos = 0\n            mode = 'wb'\n        response = None\n\n        with requests.Session() as session:\n            try:\n                header = {'Range': 'bytes=%d-' % resume_pos,\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "512": {
  "name": "resume_pos",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "123",
  "column": "12",
  "context": "            mode = 'ab'\n        else:\n            resume_pos = 0\n            mode = 'wb'\n        response = None\n\n ",
  "context_lines": "        if resume:\n            resume_pos = os.path.getsize(resume_file)\n            mode = 'ab'\n        else:\n            resume_pos = 0\n            mode = 'wb'\n        response = None\n\n        with requests.Session() as session:\n            try:\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "513": {
  "name": "mode",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "124",
  "column": "12",
  "context": "     else:\n            resume_pos = 0\n            mode = 'wb'\n        response = None\n\n        with requests.Ses",
  "context_lines": "            resume_pos = os.path.getsize(resume_file)\n            mode = 'ab'\n        else:\n            resume_pos = 0\n            mode = 'wb'\n        response = None\n\n        with requests.Session() as session:\n            try:\n                header = {'Range': 'bytes=%d-' % resume_pos,\n",
  "slicing": [
   "            mode = 'wb'\n",
   "                with open(resume_file, mode) as f:\n"
  ]
 },
 "514": {
  "name": "header",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "129",
  "column": "16",
  "context": "on() as session:\n            try:\n                header = {'Range': 'bytes=%d-' % resume_pos,\n                          'Accept-Encoding': 'iden",
  "context_lines": "            mode = 'wb'\n        response = None\n\n        with requests.Session() as session:\n            try:\n                header = {'Range': 'bytes=%d-' % resume_pos,\n                          'Accept-Encoding': 'identity'} if resume else {}\n                response = session.get(url, stream=True, timeout=5, headers=header)\n\n                # negative reply could be 'none' or just missing\n                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "515": {
  "name": "response",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "131",
  "column": "16",
  "context": "g': 'identity'} if resume else {}\n                response = session.get(url, stream=True, timeout=5, headers=header)\n\n                # negative reply could be 'none' ",
  "context_lines": "        with requests.Session() as session:\n            try:\n                header = {'Range': 'bytes=%d-' % resume_pos,\n                          'Accept-Encoding': 'identity'} if resume else {}\n                response = session.get(url, stream=True, timeout=5, headers=header)\n\n                # negative reply could be 'none' or just missing\n                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n                    resume_pos = 0\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "516": {
  "name": "resume_pos",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "135",
  "column": "20",
  "context": "t-Ranges', 'none') == 'none':\n                    resume_pos = 0\n                    mode = 'wb'\n\n                C",
  "context_lines": "                          'Accept-Encoding': 'identity'} if resume else {}\n                response = session.get(url, stream=True, timeout=5, headers=header)\n\n                # negative reply could be 'none' or just missing\n                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n                    resume_pos = 0\n                    mode = 'wb'\n\n                CHUNK_SIZE = 32768\n                total_size = int(response.headers.get('Content-Length', -1))\n                # server returns remaining size if resuming, so adjust total\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "517": {
  "name": "mode",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "136",
  "column": "20",
  "context": "               resume_pos = 0\n                    mode = 'wb'\n\n                CHUNK_SIZE = 32768\n              ",
  "context_lines": "                response = session.get(url, stream=True, timeout=5, headers=header)\n\n                # negative reply could be 'none' or just missing\n                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n                    resume_pos = 0\n                    mode = 'wb'\n\n                CHUNK_SIZE = 32768\n                total_size = int(response.headers.get('Content-Length', -1))\n                # server returns remaining size if resuming, so adjust total\n",
  "slicing": [
   "                    mode = 'wb'\n",
   "                with open(resume_file, mode) as f:\n"
  ]
 },
 "518": {
  "name": "CHUNK_SIZE",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "138",
  "column": "16",
  "context": "\n                    mode = 'wb'\n\n                CHUNK_SIZE = 32768\n                total_size = int(response.headers.",
  "context_lines": "                # negative reply could be 'none' or just missing\n                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n                    resume_pos = 0\n                    mode = 'wb'\n\n                CHUNK_SIZE = 32768\n                total_size = int(response.headers.get('Content-Length', -1))\n                # server returns remaining size if resuming, so adjust total\n                total_size += resume_pos\n                done = resume_pos\n\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "519": {
  "name": "total_size",
  "type": "int",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "139",
  "column": "16",
  "context": "               CHUNK_SIZE = 32768\n                total_size = int(response.headers.get('Content-Length', -1))\n                # server returns remaining size if",
  "context_lines": "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n                    resume_pos = 0\n                    mode = 'wb'\n\n                CHUNK_SIZE = 32768\n                total_size = int(response.headers.get('Content-Length', -1))\n                # server returns remaining size if resuming, so adjust total\n                total_size += resume_pos\n                done = resume_pos\n\n                with open(resume_file, mode) as f:\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "520": {
  "name": "total_size",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "141",
  "column": "16",
  "context": "size if resuming, so adjust total\n                total_size += resume_pos\n                done = resume_pos\n\n               ",
  "context_lines": "                    mode = 'wb'\n\n                CHUNK_SIZE = 32768\n                total_size = int(response.headers.get('Content-Length', -1))\n                # server returns remaining size if resuming, so adjust total\n                total_size += resume_pos\n                done = resume_pos\n\n                with open(resume_file, mode) as f:\n                    for chunk in response.iter_content(CHUNK_SIZE):\n                        if chunk:  # filter out keep-alive new chunks\n",
  "slicing": [
   "                total_size += resume_pos\n",
   "                        if total_size > 0:\n",
   "                            if total_size < done:\n",
   "                            logger.log(done, total_size)\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n"
  ]
 },
 "521": {
  "name": "done",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "142",
  "column": "16",
  "context": "         total_size += resume_pos\n                done = resume_pos\n\n                with open(resume_file, mode) as f",
  "context_lines": "                CHUNK_SIZE = 32768\n                total_size = int(response.headers.get('Content-Length', -1))\n                # server returns remaining size if resuming, so adjust total\n                total_size += resume_pos\n                done = resume_pos\n\n                with open(resume_file, mode) as f:\n                    for chunk in response.iter_content(CHUNK_SIZE):\n                        if chunk:  # filter out keep-alive new chunks\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "522": {
  "name": "done",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "149",
  "column": "28",
  "context": "   if total_size > 0:\n                            done += len(chunk)\n                            if total_size < done:\n",
  "context_lines": "                    for chunk in response.iter_content(CHUNK_SIZE):\n                        if chunk:  # filter out keep-alive new chunks\n                            f.write(chunk)\n                        if total_size > 0:\n                            done += len(chunk)\n                            if total_size < done:\n                                # don't freak out if content-length was too small\n                                total_size = done\n                            logger.log(done, total_size)\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "523": {
  "name": "total_size",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "152",
  "column": "32",
  "context": "gth was too small\n                                total_size = done\n                            logger.log(done, total",
  "context_lines": "                        if total_size > 0:\n                            done += len(chunk)\n                            if total_size < done:\n                                # don't freak out if content-length was too small\n                                total_size = done\n                            logger.log(done, total_size)\n                    break\n            except requests.exceptions.ConnectionError:\n                retry -= 1\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "524": {
  "name": "retry",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "156",
  "column": "16",
  "context": "uests.exceptions.ConnectionError:\n                retry -= 1\n                # TODO Better way to clean progres",
  "context_lines": "                                total_size = done\n                            logger.log(done, total_size)\n                    break\n            except requests.exceptions.ConnectionError:\n                retry -= 1\n                # TODO Better way to clean progress bar?\n                print(''.join([' '] * 60), end='\\r')\n                if retry >= 0:\n                    print('Connection error, retrying. (%d retries left)' % retry)\n",
  "slicing": [
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n"
  ]
 },
 "525": {
  "name": "fullpath",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "202",
  "column": "4",
  "context": "file.\n    \"\"\"\n    print('unpacking ' + fname)\n    fullpath = os.path.join(path, fname)\n    if '.tar.gz' in fname:\n        shutil.unpack_a",
  "context_lines": "    \"\"\"Unpacks the given archive file to the same directory, then (by default)\n    deletes the archive file.\n    \"\"\"\n    print('unpacking ' + fname)\n    fullpath = os.path.join(path, fname)\n    if '.tar.gz' in fname:\n        shutil.unpack_archive(fullpath, path, format='gztar')\n    else:\n        shutil.unpack_archive(fullpath, path)\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "526": {
  "name": "URL",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "230",
  "column": "4",
  "context": "kage to download a file from Google Drive.\"\"\"\n    URL = 'https://docs.google.com/uc?export=download'\n\n    with requests.Session() as session:\n        r",
  "context_lines": "            return value\n    return None\n\ndef download_from_google_drive(gd_id, destination):\n    \"\"\"Uses the requests package to download a file from Google Drive.\"\"\"\n    URL = 'https://docs.google.com/uc?export=download'\n\n    with requests.Session() as session:\n        response = session.get(URL, params={'id': gd_id}, stream=True)\n        token = _get_confirm_token(response)\n\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "527": {
  "name": "response",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "233",
  "column": "8",
  "context": "\n\n    with requests.Session() as session:\n        response = session.get(URL, params={'id': gd_id}, stream=True)\n        token = _get_confirm_token(response)\n\n    ",
  "context_lines": "def download_from_google_drive(gd_id, destination):\n    \"\"\"Uses the requests package to download a file from Google Drive.\"\"\"\n    URL = 'https://docs.google.com/uc?export=download'\n\n    with requests.Session() as session:\n        response = session.get(URL, params={'id': gd_id}, stream=True)\n        token = _get_confirm_token(response)\n\n        if token:\n            response.close()\n            params = {'id': gd_id, 'confirm': token}\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "528": {
  "name": "token",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "234",
  "column": "8",
  "context": "t(URL, params={'id': gd_id}, stream=True)\n        token = _get_confirm_token(response)\n\n        if token:\n            response.close()\n  ",
  "context_lines": "    \"\"\"Uses the requests package to download a file from Google Drive.\"\"\"\n    URL = 'https://docs.google.com/uc?export=download'\n\n    with requests.Session() as session:\n        response = session.get(URL, params={'id': gd_id}, stream=True)\n        token = _get_confirm_token(response)\n\n        if token:\n            response.close()\n            params = {'id': gd_id, 'confirm': token}\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "529": {
  "name": "params",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "238",
  "column": "12",
  "context": "f token:\n            response.close()\n            params = {'id': gd_id, 'confirm': token}\n            response = session.get(URL, params=par",
  "context_lines": "        response = session.get(URL, params={'id': gd_id}, stream=True)\n        token = _get_confirm_token(response)\n\n        if token:\n            response.close()\n            params = {'id': gd_id, 'confirm': token}\n            response = session.get(URL, params=params, stream=True)\n\n        CHUNK_SIZE = 32768\n        with open(destination, 'wb') as f:\n            for chunk in response.iter_content(CHUNK_SIZE):\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "530": {
  "name": "response",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "239",
  "column": "12",
  "context": "ams = {'id': gd_id, 'confirm': token}\n            response = session.get(URL, params=params, stream=True)\n\n        CHUNK_SIZE = 32768\n        with open(dest",
  "context_lines": "        token = _get_confirm_token(response)\n\n        if token:\n            response.close()\n            params = {'id': gd_id, 'confirm': token}\n            response = session.get(URL, params=params, stream=True)\n\n        CHUNK_SIZE = 32768\n        with open(destination, 'wb') as f:\n            for chunk in response.iter_content(CHUNK_SIZE):\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "531": {
  "name": "CHUNK_SIZE",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "241",
  "column": "8",
  "context": "ion.get(URL, params=params, stream=True)\n\n        CHUNK_SIZE = 32768\n        with open(destination, 'wb') as f:\n       ",
  "context_lines": "        if token:\n            response.close()\n            params = {'id': gd_id, 'confirm': token}\n            response = session.get(URL, params=params, stream=True)\n\n        CHUNK_SIZE = 32768\n        with open(destination, 'wb') as f:\n            for chunk in response.iter_content(CHUNK_SIZE):\n                if chunk:  # filter out keep-alive new chunks\n                    f.write(chunk)\n",
  "slicing": [
   "        CHUNK_SIZE = 32768\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n"
  ]
 },
 "532": {
  "name": "dpath",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "269",
  "column": "4",
  "context": "     containing the extracted files.\n    \"\"\"\n\n    dpath = str(DATA_ROOT / local_folder)\n    out_paths = list(dpath + '/' + fname.replace('",
  "context_lines": "            If the downloaded file was a compressed file (`.tar.gz`,\n            `.zip`, `.tgz`, `.gz`), return the path of the folder\n            containing the extracted files.\n    \"\"\"\n\n    dpath = str(DATA_ROOT / local_folder)\n    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n                     for fname in fnames)\n\n    if not built(dpath, version):\n        for fname in fnames:\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "533": {
  "name": "out_paths",
  "type": "list",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "270",
  "column": "4",
  "context": "\"\"\n\n    dpath = str(DATA_ROOT / local_folder)\n    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n                     for fname in fnames)\n\n    if ",
  "context_lines": "            `.zip`, `.tgz`, `.gz`), return the path of the folder\n            containing the extracted files.\n    \"\"\"\n\n    dpath = str(DATA_ROOT / local_folder)\n    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n                     for fname in fnames)\n\n    if not built(dpath, version):\n        for fname in fnames:\n            print('[building data: ' + dpath + '/' + fname + ']')\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "534": {
  "name": "paths",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "282",
  "column": "12",
  "context": ")\n\n        if isinstance(paths, str):\n            paths = [paths] * len(fnames)\n        # Download the data.\n        for fname, pa",
  "context_lines": "            # An older version exists, so remove these outdated files.\n            remove_dir(dpath)\n        make_dir(dpath)\n\n        if isinstance(paths, str):\n            paths = [paths] * len(fnames)\n        # Download the data.\n        for fname, path in zip(fnames, paths):\n            if path == 'aws':\n                url = 'http://huggingface.co/downloads/models/'\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "535": {
  "name": "url",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "286",
  "column": "16",
  "context": "s):\n            if path == 'aws':\n                url = 'http://huggingface.co/downloads/models/'\n                url += local_folder + '/'\n        ",
  "context_lines": "            paths = [paths] * len(fnames)\n        # Download the data.\n        for fname, path in zip(fnames, paths):\n            if path == 'aws':\n                url = 'http://huggingface.co/downloads/models/'\n                url += local_folder + '/'\n                url += fname\n            else:\n                url = path + '/' + fname\n",
  "slicing": [
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "            download(url, dpath, fname)\n"
  ]
 },
 "536": {
  "name": "url",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "287",
  "column": "16",
  "context": "huggingface.co/downloads/models/'\n                url += local_folder + '/'\n                url += fname\n            else:\n   ",
  "context_lines": "        # Download the data.\n        for fname, path in zip(fnames, paths):\n            if path == 'aws':\n                url = 'http://huggingface.co/downloads/models/'\n                url += local_folder + '/'\n                url += fname\n            else:\n                url = path + '/' + fname\n            download(url, dpath, fname)\n",
  "slicing": [
   "                url += local_folder + '/'\n",
   "            download(url, dpath, fname)\n"
  ]
 },
 "537": {
  "name": "url",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "288",
  "column": "16",
  "context": "        url += local_folder + '/'\n                url += fname\n            else:\n                url = path + '/'",
  "context_lines": "        for fname, path in zip(fnames, paths):\n            if path == 'aws':\n                url = 'http://huggingface.co/downloads/models/'\n                url += local_folder + '/'\n                url += fname\n            else:\n                url = path + '/' + fname\n            download(url, dpath, fname)\n            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
  "slicing": [
   "                url += fname\n",
   "            download(url, dpath, fname)\n"
  ]
 },
 "538": {
  "name": "url",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "290",
  "column": "16",
  "context": "   url += fname\n            else:\n                url = path + '/' + fname\n            download(url, dpath, fname)\n          ",
  "context_lines": "                url = 'http://huggingface.co/downloads/models/'\n                url += local_folder + '/'\n                url += fname\n            else:\n                url = path + '/' + fname\n            download(url, dpath, fname)\n            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n                untar(dpath, fname)\n        # Mark the data as built.\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "539": {
  "name": "lazy",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/tests/dataset_readers/activitynet_captions_test.py",
  "lineno": "11",
  "column": "34",
  "context": " (True, False))\n    def test_read_from_file(self, lazy):\n        reader = ActivityNetCaptionsDatasetReader(",
  "context_lines": "from adversarialnlp.dataset_readers import ActivityNetCaptionsDatasetReader\nfrom adversarialnlp.tests.utils import FIXTURES_ROOT\n\nclass TestActivityNetCaptionsReader():\n    @pytest.mark.parametrize(\"lazy\", (True, False))\n    def test_read_from_file(self, lazy):\n        reader = ActivityNetCaptionsDatasetReader(lazy=lazy)\n        instances = reader.read(FIXTURES_ROOT / 'activitynet_captions.json')\n        instances = ensure_list(instances)\n\n        instance1 = {\"video_id\": \"v_uqiMw7tQ1Cc\",\n",
  "slicing": "    def test_read_from_file(self, lazy):\n"
 },
 "540": {
  "name": "first_sentence",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/swag_generator_test.py",
  "lineno": "44",
  "column": "30",
  "context": "s = LazyIterable()\n\n    def create_instance(self, first_sentence: List[str], second_sentence: List[str]):\n        first_tokens = [Token(t) for t in first_se",
  "context_lines": "            def __iter__(self):\n                return (instance for instance in instances)\n\n        self.instances = instances\n        self.lazy_instances = LazyIterable()\n\n    def create_instance(self, first_sentence: List[str], second_sentence: List[str]):\n        first_tokens = [Token(t) for t in first_sentence]\n        second_tokens = [Token(t) for t in second_sentence]\n        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n",
  "slicing": "    def create_instance(self, first_sentence: List[str], second_sentence: List[str]):\n"
 },
 "541": {
  "name": "second_sentence",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/swag_generator_test.py",
  "lineno": "44",
  "column": "57",
  "context": " create_instance(self, first_sentence: List[str], second_sentence: List[str]):\n        first_tokens = [Token(t) for t in first_se",
  "context_lines": "            def __iter__(self):\n                return (instance for instance in instances)\n\n        self.instances = instances\n        self.lazy_instances = LazyIterable()\n\n    def create_instance(self, first_sentence: List[str], second_sentence: List[str]):\n        first_tokens = [Token(t) for t in first_sentence]\n        second_tokens = [Token(t) for t in second_sentence]\n        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n",
  "slicing": "    def create_instance(self, first_sentence: List[str], second_sentence: List[str]):\n"
 },
 "542": {
  "name": "candidate_instances",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/tests/generators/swag_generator_test.py",
  "lineno": "51",
  "column": "43",
  "context": "tance\n\n    def assert_instances_are_correct(self, candidate_instances):\n        # First we need to remove padding tokens f",
  "context_lines": "        second_tokens = [Token(t) for t in second_sentence]\n        instance = Instance({'first_sentence': TextField(first_tokens, self.token_indexers),\n                             'second_sentence': TextField(second_tokens, self.token_indexers)})\n        return instance\n\n    def assert_instances_are_correct(self, candidate_instances):\n        # First we need to remove padding tokens from the candidates.\n        # pylint: disable=protected-access\n        candidate_instances = [tuple(w for w in instance if w != 0) for instance in candidate_instances]\n        expected_instances = [tuple(instance.fields[\"first_sentence\"]._indexed_tokens[\"tokens\"])\n",
  "slicing": "    def assert_instances_are_correct(self, candidate_instances):\n"
 },
 "543": {
  "name": "default_seeds",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/generator.py",
  "lineno": "53",
  "column": "17",
  "context": "\n    \"\"\"\n\n    def __init__(self,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n        sel",
  "context_lines": "        >> generator = Generator()\n        >> examples = generator(num_epochs=1)\n    \"\"\"\n\n    def __init__(self,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n        self.default_seeds = default_seeds\n        self.quiet: bool = quiet\n\n        self._epochs: Dict[int, int] = defaultdict(int)\n\n",
  "slicing": "                 default_seeds: Iterable = None,\n"
 },
 "544": {
  "name": "quiet",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/generator.py",
  "lineno": "54",
  "column": "17",
  "context": " default_seeds: Iterable = None,\n                 quiet: bool = False):\n        self.default_seeds = default_seeds\n       ",
  "context_lines": "        >> examples = generator(num_epochs=1)\n    \"\"\"\n\n    def __init__(self,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n        self.default_seeds = default_seeds\n        self.quiet: bool = quiet\n\n        self._epochs: Dict[int, int] = defaultdict(int)\n\n    def generate_from_seed(self, seed: any):\n",
  "slicing": "                 quiet: bool = False):\n"
 },
 "545": {
  "name": "seed",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/generator.py",
  "lineno": "60",
  "column": "33",
  "context": "efaultdict(int)\n\n    def generate_from_seed(self, seed: any):\n        r\"\"\"Generate an adversarial example from a",
  "context_lines": "                 quiet: bool = False):\n        self.default_seeds = default_seeds\n        self.quiet: bool = quiet\n\n        self._epochs: Dict[int, int] = defaultdict(int)\n\n    def generate_from_seed(self, seed: any):\n        r\"\"\"Generate an adversarial example from a seed.\n        \"\"\"\n        raise NotImplementedError\n\n    def __call__(self,\n",
  "slicing": "    def generate_from_seed(self, seed: any):\n"
 },
 "546": {
  "name": "seeds",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/generator.py",
  "lineno": "66",
  "column": "17",
  "context": "tedError\n\n    def __call__(self,\n                 seeds: Iterable = None,\n                 num_epochs: int = None,\n         ",
  "context_lines": "        r\"\"\"Generate an adversarial example from a seed.\n        \"\"\"\n        raise NotImplementedError\n\n    def __call__(self,\n                 seeds: Iterable = None,\n                 num_epochs: int = None,\n                 shuffle: bool = True) -> Iterable:\n        r\"\"\"Generate adversarial examples using _generate_from_seed.\n\n        Args:\n",
  "slicing": "                 seeds: Iterable = None,\n"
 },
 "547": {
  "name": "num_epochs",
  "type": "int",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/generator.py",
  "lineno": "67",
  "column": "17",
  "context": "         seeds: Iterable = None,\n                 num_epochs: int = None,\n                 shuffle: bool = True) -> Iterable",
  "context_lines": "        \"\"\"\n        raise NotImplementedError\n\n    def __call__(self,\n                 seeds: Iterable = None,\n                 num_epochs: int = None,\n                 shuffle: bool = True) -> Iterable:\n        r\"\"\"Generate adversarial examples using _generate_from_seed.\n\n        Args:\n            seeds: Instances to use as seed for adversarial\n",
  "slicing": "                 num_epochs: int = None,\n"
 },
 "548": {
  "name": "shuffle",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/generator.py",
  "lineno": "68",
  "column": "17",
  "context": "         num_epochs: int = None,\n                 shuffle: bool = True) -> Iterable:\n        r\"\"\"Generate adversarial examples using _g",
  "context_lines": "        raise NotImplementedError\n\n    def __call__(self,\n                 seeds: Iterable = None,\n                 num_epochs: int = None,\n                 shuffle: bool = True) -> Iterable:\n        r\"\"\"Generate adversarial examples using _generate_from_seed.\n\n        Args:\n            seeds: Instances to use as seed for adversarial\n                example generation.\n",
  "slicing": "                 shuffle: bool = True) -> Iterable:\n"
 },
 "549": {
  "name": "file_path",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/squad_reader.py",
  "lineno": "10",
  "column": "17",
  "context": "# pylint: disable=invalid-name\n\n\ndef squad_reader(file_path: str = None) -> Iterator[List[Tuple[str, str]]]:\n    r\"\"\" Reads a JSON-formatted SQuAD file and ret",
  "context_lines": "import logging\nfrom typing import Iterator, List, Tuple\n\nfrom adversarialnlp.common.file_utils import download_files\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef squad_reader(file_path: str = None) -> Iterator[List[Tuple[str, str]]]:\n    r\"\"\" Reads a JSON-formatted SQuAD file and returns an Iterator.\n\n    Args:\n        file_path: Path to a JSON-formatted SQuAD file.\n            If no path is provided, download and use SQuAD v1.0 training dataset.\n\n",
  "slicing": "def squad_reader(file_path: str = None) -> Iterator[List[Tuple[str, str]]]:\n"
 },
 "550": {
  "name": "alteration_strategy",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "59",
  "column": "17",
  "context": "\n\n    \"\"\"\n    def __init__(self,\n                 alteration_strategy: str = 'high-conf',\n                 prepend: bool = False,\n          ",
  "context_lines": "        set of the\n        `SQuAD V1.0 dataset <https://rajpurkar.github.io/SQuAD-explorer/>`_.\n\n    \"\"\"\n    def __init__(self,\n                 alteration_strategy: str = 'high-conf',\n                 prepend: bool = False,\n                 use_answer_placeholder: bool = False,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n",
  "slicing": "                 alteration_strategy: str = 'high-conf',\n"
 },
 "551": {
  "name": "prepend",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "60",
  "column": "17",
  "context": "ion_strategy: str = 'high-conf',\n                 prepend: bool = False,\n                 use_answer_placeholder: bool = Fa",
  "context_lines": "        `SQuAD V1.0 dataset <https://rajpurkar.github.io/SQuAD-explorer/>`_.\n\n    \"\"\"\n    def __init__(self,\n                 alteration_strategy: str = 'high-conf',\n                 prepend: bool = False,\n                 use_answer_placeholder: bool = False,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n        super(AddSentGenerator).__init__(default_seeds, quiet)\n",
  "slicing": "                 prepend: bool = False,\n"
 },
 "552": {
  "name": "use_answer_placeholder",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "61",
  "column": "17",
  "context": "          prepend: bool = False,\n                 use_answer_placeholder: bool = False,\n                 default_seeds: Iterable = None,\n ",
  "context_lines": "    \"\"\"\n    def __init__(self,\n                 alteration_strategy: str = 'high-conf',\n                 prepend: bool = False,\n                 use_answer_placeholder: bool = False,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n        super(AddSentGenerator).__init__(default_seeds, quiet)\n        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n",
  "slicing": "                 use_answer_placeholder: bool = False,\n"
 },
 "553": {
  "name": "default_seeds",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "62",
  "column": "17",
  "context": "nswer_placeholder: bool = False,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n        sup",
  "context_lines": "    def __init__(self,\n                 alteration_strategy: str = 'high-conf',\n                 prepend: bool = False,\n                 use_answer_placeholder: bool = False,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n        super(AddSentGenerator).__init__(default_seeds, quiet)\n        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n                                             'postag_dict.json'],\n",
  "slicing": "                 default_seeds: Iterable = None,\n"
 },
 "554": {
  "name": "quiet",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "63",
  "column": "17",
  "context": " default_seeds: Iterable = None,\n                 quiet: bool = False):\n        super(AddSentGenerator).__init__(default_s",
  "context_lines": "                 alteration_strategy: str = 'high-conf',\n                 prepend: bool = False,\n                 use_answer_placeholder: bool = False,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n        super(AddSentGenerator).__init__(default_seeds, quiet)\n        model_files = download_files(fnames=['nearby_n100_glove_6B_100d.json',\n                                             'postag_dict.json'],\n                                     local_folder='addsent')\n",
  "slicing": "                 quiet: bool = False):\n"
 },
 "555": {
  "name": "text",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "89",
  "column": "24",
  "context": "        self.nlp.close()\n\n    def _annotate(self, text: str, annotators: str):\n        r\"\"\"Wrapper to call CoreNLP. \"\"\"\n        p",
  "context_lines": "        else:\n            self.default_seeds = default_seeds\n\n    def close(self):\n        self.nlp.close()\n\n    def _annotate(self, text: str, annotators: str):\n        r\"\"\"Wrapper to call CoreNLP. \"\"\"\n        props = {'annotators': annotators,\n                 'ssplit.newlineIsSentenceBreak': 'always',\n                 'outputFormat':'json'}\n",
  "slicing": "    def _annotate(self, text: str, annotators: str):\n"
 },
 "556": {
  "name": "annotators",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "89",
  "column": "35",
  "context": "f.nlp.close()\n\n    def _annotate(self, text: str, annotators: str):\n        r\"\"\"Wrapper to call CoreNLP. \"\"\"\n        p",
  "context_lines": "        else:\n            self.default_seeds = default_seeds\n\n    def close(self):\n        self.nlp.close()\n\n    def _annotate(self, text: str, annotators: str):\n        r\"\"\"Wrapper to call CoreNLP. \"\"\"\n        props = {'annotators': annotators,\n                 'ssplit.newlineIsSentenceBreak': 'always',\n                 'outputFormat':'json'}\n",
  "slicing": "    def _annotate(self, text: str, annotators: str):\n"
 },
 "557": {
  "name": "question",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "96",
  "column": "30",
  "context": "properties=props))\n\n    def _alter_question(self, question, tokens, const_parse):\n        r\"\"\"Alter the question to make it ask some",
  "context_lines": "        props = {'annotators': annotators,\n                 'ssplit.newlineIsSentenceBreak': 'always',\n                 'outputFormat':'json'}\n        return json.loads(self.nlp.annotate(text, properties=props))\n\n    def _alter_question(self, question, tokens, const_parse):\n        r\"\"\"Alter the question to make it ask something else. \"\"\"\n        used_words = [tok['word'].lower() for tok in tokens]\n        new_qs = []\n        toks_all = []\n",
  "slicing": "    def _alter_question(self, question, tokens, const_parse):\n"
 },
 "558": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "96",
  "column": "40",
  "context": "=props))\n\n    def _alter_question(self, question, tokens, const_parse):\n        r\"\"\"Alter the question to make it ask some",
  "context_lines": "        props = {'annotators': annotators,\n                 'ssplit.newlineIsSentenceBreak': 'always',\n                 'outputFormat':'json'}\n        return json.loads(self.nlp.annotate(text, properties=props))\n\n    def _alter_question(self, question, tokens, const_parse):\n        r\"\"\"Alter the question to make it ask something else. \"\"\"\n        used_words = [tok['word'].lower() for tok in tokens]\n        new_qs = []\n        toks_all = []\n",
  "slicing": "    def _alter_question(self, question, tokens, const_parse):\n"
 },
 "559": {
  "name": "const_parse",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "96",
  "column": "48",
  "context": "\n\n    def _alter_question(self, question, tokens, const_parse):\n        r\"\"\"Alter the question to make it ask some",
  "context_lines": "        props = {'annotators': annotators,\n                 'ssplit.newlineIsSentenceBreak': 'always',\n                 'outputFormat':'json'}\n        return json.loads(self.nlp.annotate(text, properties=props))\n\n    def _alter_question(self, question, tokens, const_parse):\n        r\"\"\"Alter the question to make it ask something else. \"\"\"\n        used_words = [tok['word'].lower() for tok in tokens]\n        new_qs = []\n        toks_all = []\n",
  "slicing": "    def _alter_question(self, question, tokens, const_parse):\n"
 },
 "560": {
  "name": "seed",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/addsent_generator.py",
  "lineno": "155",
  "column": "33",
  "context": "  return new_qs\n\n    def generate_from_seed(self, seed: Tuple):\n        r\"\"\"Edit a SQuAD example using rules. \"\"\"\n",
  "context_lines": "                    const_parse, [tok['word'] for tok in toks_all])\n            if new_q != question:\n                new_qs.append((rejoin(toks_all), toks_all, new_const_parse, self.alteration_strategy))\n        return new_qs\n\n    def generate_from_seed(self, seed: Tuple):\n        r\"\"\"Edit a SQuAD example using rules. \"\"\"\n        qas, paragraph = seed\n        question = qas['question'].strip()\n        if not self.quiet:\n",
  "slicing": "    def generate_from_seed(self, seed: Tuple):\n"
 },
 "561": {
  "name": "path_or_host",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "28",
  "column": "23",
  "context": "s\n\n\nclass StanfordCoreNLP:\n    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, ma",
  "context_lines": "except ImportError:\n    from urllib.parse import urlparse\n\nimport requests\n\n\nclass StanfordCoreNLP:\n    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, max_retries=5):\n        self.path_or_host = path_or_host\n        self.port = port\n        self.memory = memory\n",
  "slicing": "    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n"
 },
 "562": {
  "name": "port",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "28",
  "column": "37",
  "context": "fordCoreNLP:\n    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, ma",
  "context_lines": "except ImportError:\n    from urllib.parse import urlparse\n\nimport requests\n\n\nclass StanfordCoreNLP:\n    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, max_retries=5):\n        self.path_or_host = path_or_host\n        self.port = port\n        self.memory = memory\n",
  "slicing": "    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n"
 },
 "563": {
  "name": "memory",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "28",
  "column": "48",
  "context": ":\n    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, ma",
  "context_lines": "except ImportError:\n    from urllib.parse import urlparse\n\nimport requests\n\n\nclass StanfordCoreNLP:\n    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, max_retries=5):\n        self.path_or_host = path_or_host\n        self.port = port\n        self.memory = memory\n",
  "slicing": "    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n"
 },
 "564": {
  "name": "lang",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "28",
  "column": "61",
  "context": "nit__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, ma",
  "context_lines": "except ImportError:\n    from urllib.parse import urlparse\n\nimport requests\n\n\nclass StanfordCoreNLP:\n    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, max_retries=5):\n        self.path_or_host = path_or_host\n        self.port = port\n        self.memory = memory\n",
  "slicing": "    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n"
 },
 "565": {
  "name": "timeout",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "28",
  "column": "72",
  "context": " path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, ma",
  "context_lines": "except ImportError:\n    from urllib.parse import urlparse\n\nimport requests\n\n\nclass StanfordCoreNLP:\n    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, max_retries=5):\n        self.path_or_host = path_or_host\n        self.port = port\n        self.memory = memory\n",
  "slicing": "    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n"
 },
 "566": {
  "name": "quiet",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "28",
  "column": "86",
  "context": " port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, ma",
  "context_lines": "except ImportError:\n    from urllib.parse import urlparse\n\nimport requests\n\n\nclass StanfordCoreNLP:\n    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, max_retries=5):\n        self.path_or_host = path_or_host\n        self.port = port\n        self.memory = memory\n",
  "slicing": "    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n"
 },
 "567": {
  "name": "logging_level",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "29",
  "column": "17",
  "context": "='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, max_retries=5):\n        self.path_or_host = path_or_host\n        s",
  "context_lines": "    from urllib.parse import urlparse\n\nimport requests\n\n\nclass StanfordCoreNLP:\n    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, max_retries=5):\n        self.path_or_host = path_or_host\n        self.port = port\n        self.memory = memory\n        self.lang = lang\n",
  "slicing": "                 logging_level=logging.WARNING, max_retries=5):\n"
 },
 "568": {
  "name": "max_retries",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "29",
  "column": "48",
  "context": ",\n                 logging_level=logging.WARNING, max_retries=5):\n        self.path_or_host = path_or_host\n        s",
  "context_lines": "    from urllib.parse import urlparse\n\nimport requests\n\n\nclass StanfordCoreNLP:\n    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, max_retries=5):\n        self.path_or_host = path_or_host\n        self.port = port\n        self.memory = memory\n        self.lang = lang\n",
  "slicing": "                 logging_level=logging.WARNING, max_retries=5):\n"
 },
 "569": {
  "name": "exc_type",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "133",
  "column": "23",
  "context": "elf):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n       ",
  "context_lines": "            time.sleep(1)\n        logging.info('The server is available.')\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n        logging.info('Cleanup...')\n        if hasattr(self, 'p'):\n",
  "slicing": "    def __exit__(self, exc_type, exc_val, exc_tb):\n"
 },
 "570": {
  "name": "exc_val",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "133",
  "column": "33",
  "context": "    return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n       ",
  "context_lines": "            time.sleep(1)\n        logging.info('The server is available.')\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n        logging.info('Cleanup...')\n        if hasattr(self, 'p'):\n",
  "slicing": "    def __exit__(self, exc_type, exc_val, exc_tb):\n"
 },
 "571": {
  "name": "exc_tb",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "133",
  "column": "42",
  "context": "n self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n       ",
  "context_lines": "            time.sleep(1)\n        logging.info('The server is available.')\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n        logging.info('Cleanup...')\n        if hasattr(self, 'p'):\n",
  "slicing": "    def __exit__(self, exc_type, exc_val, exc_tb):\n"
 },
 "572": {
  "name": "text",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "159",
  "column": "23",
  "context": "            parent.kill()\n\n    def annotate(self, text, properties=None):\n        if sys.version_info.major >= 3:\n          ",
  "context_lines": "                process.kill()\n\n            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n            # parent.send_signal(signal.SIGTERM)\n            parent.kill()\n\n    def annotate(self, text, properties=None):\n        if sys.version_info.major >= 3:\n            text = text.encode('utf-8')\n\n        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n                          headers={'Connection': 'close'})\n",
  "slicing": "    def annotate(self, text, properties=None):\n"
 },
 "573": {
  "name": "properties",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "159",
  "column": "29",
  "context": "      parent.kill()\n\n    def annotate(self, text, properties=None):\n        if sys.version_info.major >= 3:\n          ",
  "context_lines": "                process.kill()\n\n            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n            # parent.send_signal(signal.SIGTERM)\n            parent.kill()\n\n    def annotate(self, text, properties=None):\n        if sys.version_info.major >= 3:\n            text = text.encode('utf-8')\n\n        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n                          headers={'Connection': 'close'})\n",
  "slicing": "    def annotate(self, text, properties=None):\n"
 },
 "574": {
  "name": "sentence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "167",
  "column": "21",
  "context": "se'})\n        return r.text\n\n    def tregex(self, sentence, pattern):\n        tregex_url = self.url + '/tregex'\n        ",
  "context_lines": "            text = text.encode('utf-8')\n\n        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n                          headers={'Connection': 'close'})\n        return r.text\n\n    def tregex(self, sentence, pattern):\n        tregex_url = self.url + '/tregex'\n        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n        return r_dict\n\n    def tokensregex(self, sentence, pattern):\n",
  "slicing": "    def tregex(self, sentence, pattern):\n"
 },
 "575": {
  "name": "pattern",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "167",
  "column": "31",
  "context": "    return r.text\n\n    def tregex(self, sentence, pattern):\n        tregex_url = self.url + '/tregex'\n        ",
  "context_lines": "            text = text.encode('utf-8')\n\n        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n                          headers={'Connection': 'close'})\n        return r.text\n\n    def tregex(self, sentence, pattern):\n        tregex_url = self.url + '/tregex'\n        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n        return r_dict\n\n    def tokensregex(self, sentence, pattern):\n",
  "slicing": "    def tregex(self, sentence, pattern):\n"
 },
 "576": {
  "name": "sentence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "172",
  "column": "26",
  "context": "\n        return r_dict\n\n    def tokensregex(self, sentence, pattern):\n        tokensregex_url = self.url + '/tokensregex",
  "context_lines": "    def tregex(self, sentence, pattern):\n        tregex_url = self.url + '/tregex'\n        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n        return r_dict\n\n    def tokensregex(self, sentence, pattern):\n        tokensregex_url = self.url + '/tokensregex'\n        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def semgrex(self, sentence, pattern):\n",
  "slicing": "    def tokensregex(self, sentence, pattern):\n"
 },
 "577": {
  "name": "pattern",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "172",
  "column": "36",
  "context": "eturn r_dict\n\n    def tokensregex(self, sentence, pattern):\n        tokensregex_url = self.url + '/tokensregex",
  "context_lines": "    def tregex(self, sentence, pattern):\n        tregex_url = self.url + '/tregex'\n        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n        return r_dict\n\n    def tokensregex(self, sentence, pattern):\n        tokensregex_url = self.url + '/tokensregex'\n        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def semgrex(self, sentence, pattern):\n",
  "slicing": "    def tokensregex(self, sentence, pattern):\n"
 },
 "578": {
  "name": "sentence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "177",
  "column": "22",
  "context": "ern)\n        return r_dict\n\n    def semgrex(self, sentence, pattern):\n        semgrex_url = self.url + '/semgrex'\n      ",
  "context_lines": "    def tokensregex(self, sentence, pattern):\n        tokensregex_url = self.url + '/tokensregex'\n        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def semgrex(self, sentence, pattern):\n        semgrex_url = self.url + '/semgrex'\n        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def word_tokenize(self, sentence, span=False):\n",
  "slicing": "    def semgrex(self, sentence, pattern):\n"
 },
 "579": {
  "name": "pattern",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "177",
  "column": "32",
  "context": "   return r_dict\n\n    def semgrex(self, sentence, pattern):\n        semgrex_url = self.url + '/semgrex'\n      ",
  "context_lines": "    def tokensregex(self, sentence, pattern):\n        tokensregex_url = self.url + '/tokensregex'\n        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def semgrex(self, sentence, pattern):\n        semgrex_url = self.url + '/semgrex'\n        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def word_tokenize(self, sentence, span=False):\n",
  "slicing": "    def semgrex(self, sentence, pattern):\n"
 },
 "580": {
  "name": "sentence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "182",
  "column": "28",
  "context": "       return r_dict\n\n    def word_tokenize(self, sentence, span=False):\n        r_dict = self._request('ssplit,tokenize', ",
  "context_lines": "    def semgrex(self, sentence, pattern):\n        semgrex_url = self.url + '/semgrex'\n        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def word_tokenize(self, sentence, span=False):\n        r_dict = self._request('ssplit,tokenize', sentence)\n        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n\n        # Whether return token span\n        if span:\n",
  "slicing": "    def word_tokenize(self, sentence, span=False):\n"
 },
 "581": {
  "name": "span",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "182",
  "column": "38",
  "context": "urn r_dict\n\n    def word_tokenize(self, sentence, span=False):\n        r_dict = self._request('ssplit,tokenize', ",
  "context_lines": "    def semgrex(self, sentence, pattern):\n        semgrex_url = self.url + '/semgrex'\n        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n        return r_dict\n\n    def word_tokenize(self, sentence, span=False):\n        r_dict = self._request('ssplit,tokenize', sentence)\n        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n\n        # Whether return token span\n        if span:\n",
  "slicing": "    def word_tokenize(self, sentence, span=False):\n"
 },
 "582": {
  "name": "sentence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "194",
  "column": "22",
  "context": "\n            return tokens\n\n    def pos_tag(self, sentence):\n        r_dict = self._request(self.url, 'pos', se",
  "context_lines": "                     in s['tokens']]\n            return tokens, spans\n        else:\n            return tokens\n\n    def pos_tag(self, sentence):\n        r_dict = self._request(self.url, 'pos', sentence)\n        words = []\n        tags = []\n        for s in r_dict['sentences']:\n",
  "slicing": "    def pos_tag(self, sentence):\n"
 },
 "583": {
  "name": "sentence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "204",
  "column": "18",
  "context": " return list(zip(words, tags))\n\n    def ner(self, sentence):\n        r_dict = self._request(self.url, 'ner', se",
  "context_lines": "            for token in s['tokens']:\n                words.append(token['originalText'])\n                tags.append(token['pos'])\n        return list(zip(words, tags))\n\n    def ner(self, sentence):\n        r_dict = self._request(self.url, 'ner', sentence)\n        words = []\n        ner_tags = []\n        for s in r_dict['sentences']:\n",
  "slicing": "    def ner(self, sentence):\n"
 },
 "584": {
  "name": "sentence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "214",
  "column": "20",
  "context": "n list(zip(words, ner_tags))\n\n    def parse(self, sentence):\n        r_dict = self._request(self.url, 'pos,pars",
  "context_lines": "            for token in s['tokens']:\n                words.append(token['originalText'])\n                ner_tags.append(token['ner'])\n        return list(zip(words, ner_tags))\n\n    def parse(self, sentence):\n        r_dict = self._request(self.url, 'pos,parse', sentence)\n        return [s['parse'] for s in r_dict['sentences']][0]\n\n    def dependency_parse(self, sentence):\n        r_dict = self._request(self.url, 'depparse', sentence)\n",
  "slicing": "    def parse(self, sentence):\n"
 },
 "585": {
  "name": "sentence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "218",
  "column": "31",
  "context": "['sentences']][0]\n\n    def dependency_parse(self, sentence):\n        r_dict = self._request(self.url, 'depparse",
  "context_lines": "        return list(zip(words, ner_tags))\n\n    def parse(self, sentence):\n        r_dict = self._request(self.url, 'pos,parse', sentence)\n        return [s['parse'] for s in r_dict['sentences']][0]\n\n    def dependency_parse(self, sentence):\n        r_dict = self._request(self.url, 'depparse', sentence)\n        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n                s['basicDependencies']]\n\n    def coref(self, text):\n",
  "slicing": "    def dependency_parse(self, sentence):\n"
 },
 "586": {
  "name": "text",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "223",
  "column": "20",
  "context": "     s['basicDependencies']]\n\n    def coref(self, text):\n        r_dict = self._request('coref', text)\n\n   ",
  "context_lines": "    def dependency_parse(self, sentence):\n        r_dict = self._request(self.url, 'depparse', sentence)\n        return [(dep['dep'], dep['governor'], dep['dependent']) for s in r_dict['sentences'] for dep in\n                s['basicDependencies']]\n\n    def coref(self, text):\n        r_dict = self._request('coref', text)\n\n        corefs = []\n        for k, mentions in r_dict['corefs'].items():\n            simplified_mentions = []\n",
  "slicing": "    def coref(self, text):\n"
 },
 "587": {
  "name": "language",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "234",
  "column": "30",
  "context": "     return corefs\n\n    def switch_language(self, language=\"en\"):\n        self._check_language(language)\n        sel",
  "context_lines": "            for m in mentions:\n                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n            corefs.append(simplified_mentions)\n        return corefs\n\n    def switch_language(self, language=\"en\"):\n        self._check_language(language)\n        self.lang = language\n\n    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n        if sys.version_info.major >= 3:\n",
  "slicing": "    def switch_language(self, language=\"en\"):\n"
 },
 "588": {
  "name": "url",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "238",
  "column": "23",
  "context": "     self.lang = language\n\n    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n        if sys.version_info.major >= 3:\n          ",
  "context_lines": "        return corefs\n\n    def switch_language(self, language=\"en\"):\n        self._check_language(language)\n        self.lang = language\n\n    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n        if sys.version_info.major >= 3:\n            data = data.encode('utf-8')\n\n        properties = {'annotators': annotators, 'outputFormat': 'json'}\n        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
  "slicing": "    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n"
 },
 "589": {
  "name": "annotators",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "238",
  "column": "28",
  "context": "self.lang = language\n\n    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n        if sys.version_info.major >= 3:\n          ",
  "context_lines": "        return corefs\n\n    def switch_language(self, language=\"en\"):\n        self._check_language(language)\n        self.lang = language\n\n    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n        if sys.version_info.major >= 3:\n            data = data.encode('utf-8')\n\n        properties = {'annotators': annotators, 'outputFormat': 'json'}\n        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
  "slicing": "    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n"
 },
 "590": {
  "name": "data",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "238",
  "column": "45",
  "context": "age\n\n    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n        if sys.version_info.major >= 3:\n          ",
  "context_lines": "        return corefs\n\n    def switch_language(self, language=\"en\"):\n        self._check_language(language)\n        self.lang = language\n\n    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n        if sys.version_info.major >= 3:\n            data = data.encode('utf-8')\n\n        properties = {'annotators': annotators, 'outputFormat': 'json'}\n        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
  "slicing": "    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n"
 },
 "591": {
  "name": "lang",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/corenlp.py",
  "lineno": "258",
  "column": "30",
  "context": "6g, 8g and etc. ')\n\n    def _check_language(self, lang):\n        if lang not in ['en', 'zh', 'ar', 'fr', 'd",
  "context_lines": "    def _check_args(self):\n        self._check_language(self.lang)\n        if not re.match('\\dg', self.memory):\n            raise ValueError('memory=' + self.memory + ' not supported. Use 4g, 6g, 8g and etc. ')\n\n    def _check_language(self, lang):\n        if lang not in ['en', 'zh', 'ar', 'fr', 'de', 'es']:\n            raise ValueError('lang=' + self.lang + ' not supported. Use English(en), Chinese(zh), Arabic(ar), '\n",
  "slicing": "    def _check_language(self, lang):\n"
 },
 "592": {
  "name": "tag",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "10",
  "column": "23",
  "context": "t least one child.\n    \"\"\"\n    def __init__(self, tag, children=None, word=None, index=None):\n        self.tag = tag\n        if children:\n      ",
  "context_lines": "    \"\"\"A CoreNLP constituency parse (or a node in a parse tree).\n\n    Word-level constituents have |word| and |index| set and no children.\n    Phrase-level constituents have no |word| or |index| and have at least one child.\n    \"\"\"\n    def __init__(self, tag, children=None, word=None, index=None):\n        self.tag = tag\n        if children:\n            self.children = children\n        else:\n",
  "slicing": "    def __init__(self, tag, children=None, word=None, index=None):\n"
 },
 "593": {
  "name": "children",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "10",
  "column": "28",
  "context": "st one child.\n    \"\"\"\n    def __init__(self, tag, children=None, word=None, index=None):\n        self.tag = tag\n        if children:\n      ",
  "context_lines": "    \"\"\"A CoreNLP constituency parse (or a node in a parse tree).\n\n    Word-level constituents have |word| and |index| set and no children.\n    Phrase-level constituents have no |word| or |index| and have at least one child.\n    \"\"\"\n    def __init__(self, tag, children=None, word=None, index=None):\n        self.tag = tag\n        if children:\n            self.children = children\n        else:\n",
  "slicing": "    def __init__(self, tag, children=None, word=None, index=None):\n"
 },
 "594": {
  "name": "word",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "10",
  "column": "43",
  "context": "   \"\"\"\n    def __init__(self, tag, children=None, word=None, index=None):\n        self.tag = tag\n        if children:\n      ",
  "context_lines": "    \"\"\"A CoreNLP constituency parse (or a node in a parse tree).\n\n    Word-level constituents have |word| and |index| set and no children.\n    Phrase-level constituents have no |word| or |index| and have at least one child.\n    \"\"\"\n    def __init__(self, tag, children=None, word=None, index=None):\n        self.tag = tag\n        if children:\n            self.children = children\n        else:\n",
  "slicing": "    def __init__(self, tag, children=None, word=None, index=None):\n"
 },
 "595": {
  "name": "index",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "10",
  "column": "54",
  "context": "def __init__(self, tag, children=None, word=None, index=None):\n        self.tag = tag\n        if children:\n      ",
  "context_lines": "    \"\"\"A CoreNLP constituency parse (or a node in a parse tree).\n\n    Word-level constituents have |word| and |index| set and no children.\n    Phrase-level constituents have no |word| or |index| and have at least one child.\n    \"\"\"\n    def __init__(self, tag, children=None, word=None, index=None):\n        self.tag = tag\n        if children:\n            self.children = children\n        else:\n",
  "slicing": "    def __init__(self, tag, children=None, word=None, index=None):\n"
 },
 "596": {
  "name": "cls",
  "type": "ConstituencyParse",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "20",
  "column": "33",
  "context": "    @classmethod\n    def _recursive_parse_corenlp(cls, tokens, i, j):\n        orig_i = i\n        if tokens[i] == '(':\n  ",
  "context_lines": "            self.children = None\n        self.word = word\n        self.index = index\n\n    @classmethod\n    def _recursive_parse_corenlp(cls, tokens, i, j):\n        orig_i = i\n        if tokens[i] == '(':\n            tag = tokens[i + 1]\n            children = []\n",
  "slicing": "    def _recursive_parse_corenlp(cls, tokens, i, j):\n"
 },
 "597": {
  "name": "tokens",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "20",
  "column": "38",
  "context": "classmethod\n    def _recursive_parse_corenlp(cls, tokens, i, j):\n        orig_i = i\n        if tokens[i] == '(':\n  ",
  "context_lines": "            self.children = None\n        self.word = word\n        self.index = index\n\n    @classmethod\n    def _recursive_parse_corenlp(cls, tokens, i, j):\n        orig_i = i\n        if tokens[i] == '(':\n            tag = tokens[i + 1]\n            children = []\n",
  "slicing": "    def _recursive_parse_corenlp(cls, tokens, i, j):\n"
 },
 "598": {
  "name": "i",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "20",
  "column": "46",
  "context": "hod\n    def _recursive_parse_corenlp(cls, tokens, i, j):\n        orig_i = i\n        if tokens[i] == '(':\n  ",
  "context_lines": "            self.children = None\n        self.word = word\n        self.index = index\n\n    @classmethod\n    def _recursive_parse_corenlp(cls, tokens, i, j):\n        orig_i = i\n        if tokens[i] == '(':\n            tag = tokens[i + 1]\n            children = []\n",
  "slicing": "    def _recursive_parse_corenlp(cls, tokens, i, j):\n"
 },
 "599": {
  "name": "j",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "20",
  "column": "49",
  "context": "\n    def _recursive_parse_corenlp(cls, tokens, i, j):\n        orig_i = i\n        if tokens[i] == '(':\n  ",
  "context_lines": "            self.children = None\n        self.word = word\n        self.index = index\n\n    @classmethod\n    def _recursive_parse_corenlp(cls, tokens, i, j):\n        orig_i = i\n        if tokens[i] == '(':\n            tag = tokens[i + 1]\n            children = []\n",
  "slicing": "    def _recursive_parse_corenlp(cls, tokens, i, j):\n"
 },
 "600": {
  "name": "cls",
  "type": "ConstituencyParse",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "41",
  "column": "21",
  "context": ", i + 1, j\n\n    @classmethod\n    def from_corenlp(cls, s):\n        \"\"\"Parses the \"parse\" attribute returned b",
  "context_lines": "        else:\n            # Only other possibility is it's a word\n            return tokens[i], i + 1, j\n\n    @classmethod\n    def from_corenlp(cls, s):\n        \"\"\"Parses the \"parse\" attribute returned by CoreNLP parse annotator.\"\"\"\n        # \"parse\": \"(ROOT\\n  (SBARQ\\n    (WHNP (WDT What)\\n      (NP (NN portion)\\n        (PP (IN                       of)\\n          (NP\\n            (NP (NNS households))\\n            (PP (IN in)\\n              (NP (NNP             Jacksonville)))))))\\n    (SQ\\n      (VP (VBP have)\\n        (NP (RB only) (CD one) (NN person))))\\n    (. ?        )))\",\n        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n        tokens = [t for t in s_spaced.split(' ') if t]\n",
  "slicing": "    def from_corenlp(cls, s):\n"
 },
 "601": {
  "name": "s",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "41",
  "column": "26",
  "context": " 1, j\n\n    @classmethod\n    def from_corenlp(cls, s):\n        \"\"\"Parses the \"parse\" attribute returned b",
  "context_lines": "        else:\n            # Only other possibility is it's a word\n            return tokens[i], i + 1, j\n\n    @classmethod\n    def from_corenlp(cls, s):\n        \"\"\"Parses the \"parse\" attribute returned by CoreNLP parse annotator.\"\"\"\n        # \"parse\": \"(ROOT\\n  (SBARQ\\n    (WHNP (WDT What)\\n      (NP (NN portion)\\n        (PP (IN                       of)\\n          (NP\\n            (NP (NNS households))\\n            (PP (IN in)\\n              (NP (NNP             Jacksonville)))))))\\n    (SQ\\n      (VP (VBP have)\\n        (NP (RB only) (CD one) (NN person))))\\n    (. ?        )))\",\n        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n        tokens = [t for t in s_spaced.split(' ') if t]\n",
  "slicing": "    def from_corenlp(cls, s):\n"
 },
 "602": {
  "name": "indent",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "58",
  "column": "25",
  "context": ".is_singleton()\n        \n    def print_tree(self, indent=0):\n        spaces = '  ' * indent\n        if self.wor",
  "context_lines": "            return True\n        if len(self.children) > 1:\n            return False\n        return self.children[0].is_singleton()\n        \n    def print_tree(self, indent=0):\n        spaces = '  ' * indent\n        if self.word:\n            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n        else:\n",
  "slicing": "    def print_tree(self, indent=0):\n"
 },
 "603": {
  "name": "cls",
  "type": "ConstituencyParse",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "90",
  "column": "33",
  "context": "    @classmethod\n    def _recursive_replace_words(cls, tree, new_words, i):\n        if tree.word:\n            new_word = new_w",
  "context_lines": "        if self.index is not None:\n            return self.index + 1\n        return self.children[-1].get_end_index()\n\n    @classmethod\n    def _recursive_replace_words(cls, tree, new_words, i):\n        if tree.word:\n            new_word = new_words[i]\n            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n        new_children = []\n",
  "slicing": "    def _recursive_replace_words(cls, tree, new_words, i):\n"
 },
 "604": {
  "name": "tree",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "90",
  "column": "38",
  "context": "classmethod\n    def _recursive_replace_words(cls, tree, new_words, i):\n        if tree.word:\n            new_word = new_w",
  "context_lines": "        if self.index is not None:\n            return self.index + 1\n        return self.children[-1].get_end_index()\n\n    @classmethod\n    def _recursive_replace_words(cls, tree, new_words, i):\n        if tree.word:\n            new_word = new_words[i]\n            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n        new_children = []\n",
  "slicing": "    def _recursive_replace_words(cls, tree, new_words, i):\n"
 },
 "605": {
  "name": "new_words",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "90",
  "column": "44",
  "context": "ethod\n    def _recursive_replace_words(cls, tree, new_words, i):\n        if tree.word:\n            new_word = new_w",
  "context_lines": "        if self.index is not None:\n            return self.index + 1\n        return self.children[-1].get_end_index()\n\n    @classmethod\n    def _recursive_replace_words(cls, tree, new_words, i):\n        if tree.word:\n            new_word = new_words[i]\n            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n        new_children = []\n",
  "slicing": "    def _recursive_replace_words(cls, tree, new_words, i):\n"
 },
 "606": {
  "name": "i",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "90",
  "column": "55",
  "context": "ef _recursive_replace_words(cls, tree, new_words, i):\n        if tree.word:\n            new_word = new_w",
  "context_lines": "        if self.index is not None:\n            return self.index + 1\n        return self.children[-1].get_end_index()\n\n    @classmethod\n    def _recursive_replace_words(cls, tree, new_words, i):\n        if tree.word:\n            new_word = new_words[i]\n            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n        new_children = []\n",
  "slicing": "    def _recursive_replace_words(cls, tree, new_words, i):\n"
 },
 "607": {
  "name": "cls",
  "type": "ConstituencyParse",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "101",
  "column": "22",
  "context": "ldren), i\n\n    @classmethod\n    def replace_words(cls, tree, new_words):\n        \"\"\"Return a new tree, with new words repla",
  "context_lines": "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n            new_children.append(new_child)\n        return cls(tree.tag, children=new_children), i\n\n    @classmethod\n    def replace_words(cls, tree, new_words):\n        \"\"\"Return a new tree, with new words replacing old ones.\"\"\"\n        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n        if i != len(new_words):\n            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
  "slicing": "    def replace_words(cls, tree, new_words):\n"
 },
 "608": {
  "name": "tree",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "101",
  "column": "27",
  "context": "), i\n\n    @classmethod\n    def replace_words(cls, tree, new_words):\n        \"\"\"Return a new tree, with new words repla",
  "context_lines": "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n            new_children.append(new_child)\n        return cls(tree.tag, children=new_children), i\n\n    @classmethod\n    def replace_words(cls, tree, new_words):\n        \"\"\"Return a new tree, with new words replacing old ones.\"\"\"\n        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n        if i != len(new_words):\n            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
  "slicing": "    def replace_words(cls, tree, new_words):\n"
 },
 "609": {
  "name": "new_words",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "101",
  "column": "33",
  "context": "    @classmethod\n    def replace_words(cls, tree, new_words):\n        \"\"\"Return a new tree, with new words repla",
  "context_lines": "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n            new_children.append(new_child)\n        return cls(tree.tag, children=new_children), i\n\n    @classmethod\n    def replace_words(cls, tree, new_words):\n        \"\"\"Return a new tree, with new words replacing old ones.\"\"\"\n        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n        if i != len(new_words):\n            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
  "slicing": "    def replace_words(cls, tree, new_words):\n"
 },
 "610": {
  "name": "tokens",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "108",
  "column": "11",
  "context": "w_words), i))\n        return new_tree\n\ndef rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n    \"\"\"Rejoin tokens into the original sentence.\n\n",
  "context_lines": "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n        if i != len(new_words):\n            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n        return new_tree\n\ndef rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n    \"\"\"Rejoin tokens into the original sentence.\n\n    Args:\n    tokens: a list of dicts containing 'originalText' and 'before' fields.\n        All other fields will be ignored.\n",
  "slicing": [
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n"
  ]
 },
 "611": {
  "name": "sep",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "108",
  "column": "41",
  "context": "ew_tree\n\ndef rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n    \"\"\"Rejoin tokens into the original sentence.\n\n",
  "context_lines": "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n        if i != len(new_words):\n            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n        return new_tree\n\ndef rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n    \"\"\"Rejoin tokens into the original sentence.\n\n    Args:\n    tokens: a list of dicts containing 'originalText' and 'before' fields.\n        All other fields will be ignored.\n",
  "slicing": "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n"
 },
 "612": {
  "name": "answer_objs",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "125",
  "column": "27",
  "context": "t'] for t in tokens)\n\n\ndef get_tokens_for_answers(answer_objs: List[Tuple[int, Dict]], corenlp_obj: Dict) -> Tuple[int, List]:\n    \"\"\"Get CoreNLP tokens corresponding to a SQuAD",
  "context_lines": "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n    else:\n        # Use the given separator instead\n        return sep.join(t['originalText'] for t in tokens)\n\n\ndef get_tokens_for_answers(answer_objs: List[Tuple[int, Dict]], corenlp_obj: Dict) -> Tuple[int, List]:\n    \"\"\"Get CoreNLP tokens corresponding to a SQuAD answer object.\"\"\"\n    first_a_toks = None\n    for i, a_obj in enumerate(answer_objs):\n        a_toks = []\n",
  "slicing": "def get_tokens_for_answers(answer_objs: List[Tuple[int, Dict]], corenlp_obj: Dict) -> Tuple[int, List]:\n"
 },
 "613": {
  "name": "corenlp_obj",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "125",
  "column": "64",
  "context": "_for_answers(answer_objs: List[Tuple[int, Dict]], corenlp_obj: Dict) -> Tuple[int, List]:\n    \"\"\"Get CoreNLP tokens corresponding to a SQuAD",
  "context_lines": "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n    else:\n        # Use the given separator instead\n        return sep.join(t['originalText'] for t in tokens)\n\n\ndef get_tokens_for_answers(answer_objs: List[Tuple[int, Dict]], corenlp_obj: Dict) -> Tuple[int, List]:\n    \"\"\"Get CoreNLP tokens corresponding to a SQuAD answer object.\"\"\"\n    first_a_toks = None\n    for i, a_obj in enumerate(answer_objs):\n        a_toks = []\n",
  "slicing": "def get_tokens_for_answers(answer_objs: List[Tuple[int, Dict]], corenlp_obj: Dict) -> Tuple[int, List]:\n"
 },
 "614": {
  "name": "answer_objs",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "148",
  "column": "31",
  "context": "n 0, first_a_toks\n\ndef get_determiner_for_answers(answer_objs: List[Dict]) -> Optional[str]:\n    for ans in answer_objs:\n        words = ans['t",
  "context_lines": "            first_a_toks = a_toks\n    # None of the extracted token lists reconstruct the answer\n    # Default to the first\n    return 0, first_a_toks\n\ndef get_determiner_for_answers(answer_objs: List[Dict]) -> Optional[str]:\n    for ans in answer_objs:\n        words = ans['text'].split(' ')\n        if words[0].lower() == 'the':\n            return 'the'\n",
  "slicing": "def get_determiner_for_answers(answer_objs: List[Dict]) -> Optional[str]:\n"
 },
 "615": {
  "name": "tree",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "157",
  "column": "18",
  "context": "    return 'a'\n    return None\n\ndef compress_whnp(tree, inside_whnp=False):\n    if not tree.children: return tree  # Reached l",
  "context_lines": "            return 'the'\n        if words[0].lower() in ('a', 'an'):\n            return 'a'\n    return None\n\ndef compress_whnp(tree, inside_whnp=False):\n    if not tree.children: return tree  # Reached leaf\n    # Compress all children\n    for i, c in enumerate(tree.children):\n        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
  "slicing": [
   "            tag = tokens[i + 1]\n",
   "            children = []\n",
   "            i = i + 2\n",
   "                child, i, j = cls._recursive_parse_corenlp(tokens, i, j)\n",
   "                if isinstance(child, cls):\n",
   "                    children.append(child)\n",
   "                    if tokens[i] == ')': \n",
   "                        return cls(tag, children), i + 1, j\n",
   "                    if tokens[i] != ')':\n",
   "                    return cls(tag, word=child, index=j), i + 1, j + 1\n",
   "            return tokens[i], i + 1, j\n",
   "        s_spaced = s.replace('\\n', ' ').replace('(', ' ( ').replace(')', ' ) ')\n",
   "        tokens = [t for t in s_spaced.split(' ') if t]\n",
   "        tree, index, num_words = cls._recursive_parse_corenlp(tokens, 0, 0)\n",
   "        if index != len(tokens):\n",
   "            raise ValueError('Only parsed %d of %d tokens' % (index, len(tokens)))\n",
   "        return tree\n",
   "        spaces = '  ' * indent\n",
   "            print(f\"{spaces}{self.tag}: {self.word} ({self.index})\")\n",
   "            print(f\"{spaces}{self.tag}\")\n",
   "            for c in self.children:\n",
   "                c.print_tree(indent=indent + 1)\n",
   "        toks = []\n",
   "        for i, c in enumerate(self.children):\n",
   "            p = c.get_phrase()\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                toks.append(p)\n",
   "                toks.append(' ' + p)\n",
   "        return ''.join(toks)\n",
   "        if tree.word:\n",
   "            new_word = new_words[i]\n",
   "            return (cls(tree.tag, word=new_word, index=tree.index), i + 1)\n",
   "        new_children = []\n",
   "        for c in tree.children:\n",
   "            new_child, i = cls._recursive_replace_words(c, new_words, i)\n",
   "            new_children.append(new_child)\n",
   "        return cls(tree.tag, children=new_children), i\n",
   "        new_tree, i = cls._recursive_replace_words(tree, new_words, 0)\n",
   "        if i != len(new_words):\n",
   "            raise ValueError('len(new_words) == %d != i == %d' % (len(new_words), i))\n",
   "        return new_tree\n",
   "def rejoin(tokens: List[Dict[str, str]], sep: str = None) -> str:\n",
   "        return ''.join('%s%s' % (t['before'], t['originalText']) for t in tokens)\n",
   "        return sep.join(t['originalText'] for t in tokens)\n",
   "    first_a_toks = None\n",
   "    for i, a_obj in enumerate(answer_objs):\n",
   "        a_toks = []\n",
   "        answer_start = a_obj['answer_start']\n",
   "        answer_end = answer_start + len(a_obj['text'])\n",
   "        for sent in corenlp_obj['sentences']:\n",
   "            for tok in sent['tokens']:\n",
   "                if tok['characterOffsetBegin'] >= answer_end:\n",
   "                if tok['characterOffsetEnd'] <= answer_start:\n",
   "                a_toks.append(tok)\n",
   "        if rejoin(a_toks).strip() == a_obj['text']:\n",
   "            return i, a_toks\n",
   "        if i == 0:\n",
   "            first_a_toks = a_toks\n",
   "    return 0, first_a_toks\n",
   "    for ans in answer_objs:\n",
   "        words = ans['text'].split(' ')\n",
   "        if words[0].lower() == 'the':\n",
   "        if words[0].lower() in ('a', 'an'):\n",
   "def compress_whnp(tree, inside_whnp=False):\n",
   "    if not tree.children: return tree  # Reached leaf\n",
   "    for i, c in enumerate(tree.children):\n",
   "        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
   "    if tree.tag != 'WHNP':\n",
   "            return ConstituencyParse('NP', children=[tree])\n",
   "        return tree\n",
   "    wh_word = None\n",
   "    new_np_children = []\n",
   "    new_siblings = []\n",
   "    for i, c in enumerate(tree.children):\n",
   "        if i == 0:\n",
   "            if c.tag in ('WHNP', 'WHADJP', 'WHAVP', 'WHPP'):\n",
   "                wh_word = c.children[0]\n",
   "                new_np_children.extend(c.children[1:])\n",
   "            elif c.tag in ('WDT', 'WP', 'WP$', 'WRB'):\n",
   "                wh_word = c\n",
   "                return tree\n",
   "            if c.tag == 'SQ':  # Due to bad parse, SQ may show up here\n",
   "                new_siblings = tree.children[i:]\n",
   "            new_np_children.append(ConstituencyParse('NP', children=[c]))\n",
   "    if new_np_children:\n",
   "        new_np = ConstituencyParse('NP', children=new_np_children)\n",
   "        new_tree = ConstituencyParse('WHNP', children=[wh_word, new_np])\n",
   "        new_tree = tree\n",
   "    if new_siblings:\n",
   "        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n",
   "    return new_tree\n",
   "    tree = ConstituencyParse.from_corenlp(parse_str)\n",
   "    new_tree = compress_whnp(tree)\n",
   "    return new_tree\n"
  ]
 },
 "616": {
  "name": "inside_whnp",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "157",
  "column": "24",
  "context": "turn 'a'\n    return None\n\ndef compress_whnp(tree, inside_whnp=False):\n    if not tree.children: return tree  # Reached l",
  "context_lines": "            return 'the'\n        if words[0].lower() in ('a', 'an'):\n            return 'a'\n    return None\n\ndef compress_whnp(tree, inside_whnp=False):\n    if not tree.children: return tree  # Reached leaf\n    # Compress all children\n    for i, c in enumerate(tree.children):\n        tree.children[i] = compress_whnp(c, inside_whnp=inside_whnp or tree.tag == 'WHNP')\n",
  "slicing": "def compress_whnp(tree, inside_whnp=False):\n"
 },
 "617": {
  "name": "parse_str",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/utils.py",
  "lineno": "195",
  "column": "21",
  "context": "blings)\n    return new_tree\n\ndef read_const_parse(parse_str):\n    tree = ConstituencyParse.from_corenlp(parse_st",
  "context_lines": "        new_tree = tree\n    if new_siblings:\n        new_tree = ConstituencyParse('SBARQ', children=[new_tree] + new_siblings)\n    return new_tree\n\ndef read_const_parse(parse_str):\n    tree = ConstituencyParse.from_corenlp(parse_str)\n    new_tree = compress_whnp(tree)\n",
  "slicing": "def read_const_parse(parse_str):\n"
 },
 "618": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "8",
  "column": "15",
  "context": "october', 'november', 'december']\n\ndef ans_number(a, tokens, q, **kwargs):\n    out_toks = []\n    seen_num = False\n    for t i",
  "context_lines": "import math\n\nfrom adversarialnlp.generators.addsent.utils import rejoin\n\nMONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n                    'august', 'september', 'october', 'november', 'december']\n\ndef ans_number(a, tokens, q, **kwargs):\n    out_toks = []\n    seen_num = False\n    for t in tokens:\n        ner = t['ner']\n",
  "slicing": "def ans_number(a, tokens, q, **kwargs):\n"
 },
 "619": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "8",
  "column": "18",
  "context": "ober', 'november', 'december']\n\ndef ans_number(a, tokens, q, **kwargs):\n    out_toks = []\n    seen_num = False\n    for t i",
  "context_lines": "import math\n\nfrom adversarialnlp.generators.addsent.utils import rejoin\n\nMONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n                    'august', 'september', 'october', 'november', 'december']\n\ndef ans_number(a, tokens, q, **kwargs):\n    out_toks = []\n    seen_num = False\n    for t in tokens:\n        ner = t['ner']\n",
  "slicing": "def ans_number(a, tokens, q, **kwargs):\n"
 },
 "620": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "8",
  "column": "26",
  "context": "november', 'december']\n\ndef ans_number(a, tokens, q, **kwargs):\n    out_toks = []\n    seen_num = False\n    for t i",
  "context_lines": "import math\n\nfrom adversarialnlp.generators.addsent.utils import rejoin\n\nMONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n                    'august', 'september', 'october', 'november', 'december']\n\ndef ans_number(a, tokens, q, **kwargs):\n    out_toks = []\n    seen_num = False\n    for t in tokens:\n        ner = t['ner']\n",
  "slicing": "def ans_number(a, tokens, q, **kwargs):\n"
 },
 "621": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "78",
  "column": "13",
  "context": "rip()\n    else:\n        return None\n\ndef ans_date(a, tokens, q, **kwargs):\n    out_toks = []\n    if not all(t['ner'] == 'DATE",
  "context_lines": "    if seen_num:\n        return rejoin(out_toks).strip()\n    else:\n        return None\n\ndef ans_date(a, tokens, q, **kwargs):\n    out_toks = []\n    if not all(t['ner'] == 'DATE' for t in tokens):\n        return None\n    for t in tokens:\n",
  "slicing": "def ans_date(a, tokens, q, **kwargs):\n"
 },
 "622": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "78",
  "column": "16",
  "context": "()\n    else:\n        return None\n\ndef ans_date(a, tokens, q, **kwargs):\n    out_toks = []\n    if not all(t['ner'] == 'DATE",
  "context_lines": "    if seen_num:\n        return rejoin(out_toks).strip()\n    else:\n        return None\n\ndef ans_date(a, tokens, q, **kwargs):\n    out_toks = []\n    if not all(t['ner'] == 'DATE' for t in tokens):\n        return None\n    for t in tokens:\n",
  "slicing": "def ans_date(a, tokens, q, **kwargs):\n"
 },
 "623": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "78",
  "column": "24",
  "context": "lse:\n        return None\n\ndef ans_date(a, tokens, q, **kwargs):\n    out_toks = []\n    if not all(t['ner'] == 'DATE",
  "context_lines": "    if seen_num:\n        return rejoin(out_toks).strip()\n    else:\n        return None\n\ndef ans_date(a, tokens, q, **kwargs):\n    out_toks = []\n    if not all(t['ner'] == 'DATE' for t in tokens):\n        return None\n    for t in tokens:\n",
  "slicing": "def ans_date(a, tokens, q, **kwargs):\n"
 },
 "624": {
  "name": "ner_tag",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "104",
  "column": "20",
  "context": "turn None\n    return new_ans\n\ndef ans_entity_full(ner_tag, new_ans):\n    \"\"\"Returns a function that yields new_ans iff ",
  "context_lines": "        out_toks.append({'before': t['before'], 'originalText': new_val})\n    new_ans = rejoin(out_toks).strip()\n    if new_ans == a['text']: return None\n    return new_ans\n\ndef ans_entity_full(ner_tag, new_ans):\n    \"\"\"Returns a function that yields new_ans iff every token has |ner_tag|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        for t in tokens:\n            if t['ner'] != ner_tag: return None\n",
  "slicing": [
   "def ans_entity_full(ner_tag, new_ans):\n",
   "            if t['ner'] != ner_tag: return None\n"
  ]
 },
 "625": {
  "name": "new_ans",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "104",
  "column": "29",
  "context": "\n    return new_ans\n\ndef ans_entity_full(ner_tag, new_ans):\n    \"\"\"Returns a function that yields new_ans iff ",
  "context_lines": "        out_toks.append({'before': t['before'], 'originalText': new_val})\n    new_ans = rejoin(out_toks).strip()\n    if new_ans == a['text']: return None\n    return new_ans\n\ndef ans_entity_full(ner_tag, new_ans):\n    \"\"\"Returns a function that yields new_ans iff every token has |ner_tag|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        for t in tokens:\n            if t['ner'] != ner_tag: return None\n",
  "slicing": [
   "def ans_entity_full(ner_tag, new_ans):\n",
   "        return new_ans\n"
  ]
 },
 "626": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "106",
  "column": "13",
  "context": "ns iff every token has |ner_tag|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        for t in tokens:\n            if t['ner'] !",
  "context_lines": "    if new_ans == a['text']: return None\n    return new_ans\n\ndef ans_entity_full(ner_tag, new_ans):\n    \"\"\"Returns a function that yields new_ans iff every token has |ner_tag|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        for t in tokens:\n            if t['ner'] != ner_tag: return None\n        return new_ans\n    return func\n\n",
  "slicing": "    def func(a, tokens, q, **kwargs):\n"
 },
 "627": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "106",
  "column": "16",
  "context": "iff every token has |ner_tag|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        for t in tokens:\n            if t['ner'] !",
  "context_lines": "    if new_ans == a['text']: return None\n    return new_ans\n\ndef ans_entity_full(ner_tag, new_ans):\n    \"\"\"Returns a function that yields new_ans iff every token has |ner_tag|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        for t in tokens:\n            if t['ner'] != ner_tag: return None\n        return new_ans\n    return func\n\n",
  "slicing": "    def func(a, tokens, q, **kwargs):\n"
 },
 "628": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "106",
  "column": "24",
  "context": "y token has |ner_tag|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        for t in tokens:\n            if t['ner'] !",
  "context_lines": "    if new_ans == a['text']: return None\n    return new_ans\n\ndef ans_entity_full(ner_tag, new_ans):\n    \"\"\"Returns a function that yields new_ans iff every token has |ner_tag|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        for t in tokens:\n            if t['ner'] != ner_tag: return None\n        return new_ans\n    return func\n\n",
  "slicing": "    def func(a, tokens, q, **kwargs):\n"
 },
 "629": {
  "name": "new_ans",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "112",
  "column": "15",
  "context": "   return new_ans\n    return func\n\ndef ans_abbrev(new_ans):\n    def func(a, tokens, q, **kwargs):\n        s = ",
  "context_lines": "        for t in tokens:\n            if t['ner'] != ner_tag: return None\n        return new_ans\n    return func\n\ndef ans_abbrev(new_ans):\n    def func(a, tokens, q, **kwargs):\n        s = a['text']\n        if s == s.upper() and s != s.lower():\n            return new_ans\n",
  "slicing": [
   "def ans_abbrev(new_ans):\n",
   "        return new_ans\n"
  ]
 },
 "630": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "113",
  "column": "13",
  "context": "eturn func\n\ndef ans_abbrev(new_ans):\n    def func(a, tokens, q, **kwargs):\n        s = a['text']\n        if s == s.upper() an",
  "context_lines": "            if t['ner'] != ner_tag: return None\n        return new_ans\n    return func\n\ndef ans_abbrev(new_ans):\n    def func(a, tokens, q, **kwargs):\n        s = a['text']\n        if s == s.upper() and s != s.lower():\n            return new_ans\n        return None\n",
  "slicing": "    def func(a, tokens, q, **kwargs):\n"
 },
 "631": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "113",
  "column": "16",
  "context": "rn func\n\ndef ans_abbrev(new_ans):\n    def func(a, tokens, q, **kwargs):\n        s = a['text']\n        if s == s.upper() an",
  "context_lines": "            if t['ner'] != ner_tag: return None\n        return new_ans\n    return func\n\ndef ans_abbrev(new_ans):\n    def func(a, tokens, q, **kwargs):\n        s = a['text']\n        if s == s.upper() and s != s.lower():\n            return new_ans\n        return None\n",
  "slicing": "    def func(a, tokens, q, **kwargs):\n"
 },
 "632": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "113",
  "column": "24",
  "context": "\ndef ans_abbrev(new_ans):\n    def func(a, tokens, q, **kwargs):\n        s = a['text']\n        if s == s.upper() an",
  "context_lines": "            if t['ner'] != ner_tag: return None\n        return new_ans\n    return func\n\ndef ans_abbrev(new_ans):\n    def func(a, tokens, q, **kwargs):\n        s = a['text']\n        if s == s.upper() and s != s.lower():\n            return new_ans\n        return None\n",
  "slicing": "    def func(a, tokens, q, **kwargs):\n"
 },
 "633": {
  "name": "wh_word",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "120",
  "column": "17",
  "context": "    return None\n    return func\n\ndef ans_match_wh(wh_word, new_ans):\n    \"\"\"Returns a function that yields new_ans if t",
  "context_lines": "        if s == s.upper() and s != s.lower():\n            return new_ans\n        return None\n    return func\n\ndef ans_match_wh(wh_word, new_ans):\n    \"\"\"Returns a function that yields new_ans if the question starts with |wh_word|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        if q.lower().startswith(wh_word + ' '):\n            return new_ans\n",
  "slicing": [
   "def ans_match_wh(wh_word, new_ans):\n",
   "        if q.lower().startswith(wh_word + ' '):\n"
  ]
 },
 "634": {
  "name": "new_ans",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "120",
  "column": "26",
  "context": "n None\n    return func\n\ndef ans_match_wh(wh_word, new_ans):\n    \"\"\"Returns a function that yields new_ans if t",
  "context_lines": "        if s == s.upper() and s != s.lower():\n            return new_ans\n        return None\n    return func\n\ndef ans_match_wh(wh_word, new_ans):\n    \"\"\"Returns a function that yields new_ans if the question starts with |wh_word|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        if q.lower().startswith(wh_word + ' '):\n            return new_ans\n",
  "slicing": [
   "def ans_match_wh(wh_word, new_ans):\n",
   "        return new_ans\n"
  ]
 },
 "635": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "122",
  "column": "13",
  "context": "e question starts with |wh_word|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        if q.lower().startswith(wh_word + ' '):\n  ",
  "context_lines": "        return None\n    return func\n\ndef ans_match_wh(wh_word, new_ans):\n    \"\"\"Returns a function that yields new_ans if the question starts with |wh_word|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        if q.lower().startswith(wh_word + ' '):\n            return new_ans\n        return None\n    return func\n\n",
  "slicing": "    def func(a, tokens, q, **kwargs):\n"
 },
 "636": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "122",
  "column": "16",
  "context": "uestion starts with |wh_word|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        if q.lower().startswith(wh_word + ' '):\n  ",
  "context_lines": "        return None\n    return func\n\ndef ans_match_wh(wh_word, new_ans):\n    \"\"\"Returns a function that yields new_ans if the question starts with |wh_word|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        if q.lower().startswith(wh_word + ' '):\n            return new_ans\n        return None\n    return func\n\n",
  "slicing": "    def func(a, tokens, q, **kwargs):\n"
 },
 "637": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "122",
  "column": "24",
  "context": "starts with |wh_word|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        if q.lower().startswith(wh_word + ' '):\n  ",
  "context_lines": "        return None\n    return func\n\ndef ans_match_wh(wh_word, new_ans):\n    \"\"\"Returns a function that yields new_ans if the question starts with |wh_word|.\"\"\"\n    def func(a, tokens, q, **kwargs):\n        if q.lower().startswith(wh_word + ' '):\n            return new_ans\n        return None\n    return func\n\n",
  "slicing": "    def func(a, tokens, q, **kwargs):\n"
 },
 "638": {
  "name": "pos",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "128",
  "column": "12",
  "context": "\n        return None\n    return func\n\ndef ans_pos(pos, new_ans, end=False, add_dt=False):\n    \"\"\"Returns a function that yields new_ans if t",
  "context_lines": "        if q.lower().startswith(wh_word + ' '):\n            return new_ans\n        return None\n    return func\n\ndef ans_pos(pos, new_ans, end=False, add_dt=False):\n    \"\"\"Returns a function that yields new_ans if the first/last token has |pos|.\"\"\"\n    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n",
  "slicing": [
   "def ans_pos(pos, new_ans, end=False, add_dt=False):\n",
   "        if t['pos'] != pos: return None\n"
  ]
 },
 "639": {
  "name": "new_ans",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "128",
  "column": "17",
  "context": "    return None\n    return func\n\ndef ans_pos(pos, new_ans, end=False, add_dt=False):\n    \"\"\"Returns a function that yields new_ans if t",
  "context_lines": "        if q.lower().startswith(wh_word + ' '):\n            return new_ans\n        return None\n    return func\n\ndef ans_pos(pos, new_ans, end=False, add_dt=False):\n    \"\"\"Returns a function that yields new_ans if the first/last token has |pos|.\"\"\"\n    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n",
  "slicing": [
   "def ans_pos(pos, new_ans, end=False, add_dt=False):\n",
   "        return new_ans\n"
  ]
 },
 "640": {
  "name": "end",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "128",
  "column": "26",
  "context": "n None\n    return func\n\ndef ans_pos(pos, new_ans, end=False, add_dt=False):\n    \"\"\"Returns a function that yields new_ans if t",
  "context_lines": "        if q.lower().startswith(wh_word + ' '):\n            return new_ans\n        return None\n    return func\n\ndef ans_pos(pos, new_ans, end=False, add_dt=False):\n    \"\"\"Returns a function that yields new_ans if the first/last token has |pos|.\"\"\"\n    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n",
  "slicing": [
   "def ans_pos(pos, new_ans, end=False, add_dt=False):\n",
   "        ('pos_end_nn', ans_pos('NN', 'hamster', end=True, add_dt=True)),\n",
   "        ('pos_end_nnp', ans_pos('NNP', 'Central Park', end=True, add_dt=True)),\n",
   "        ('pos_end_nns', ans_pos('NNS', 'hamsters', end=True, add_dt=True)),\n",
   "        ('pos_end_nnps', ans_pos('NNPS', 'Kew Gardens', end=True, add_dt=True)),\n",
   "        ('pos_end_jj', ans_pos('JJ', 'deep', end=True)),\n",
   "        ('pos_end_jjr', ans_pos('JJR', 'deeper', end=True)),\n",
   "        ('pos_end_jjs', ans_pos('JJS', 'deepest', end=True)),\n",
   "        ('pos_end_rb', ans_pos('RB', 'silently', end=True)),\n",
   "        ('pos_end_vbg', ans_pos('VBG', 'learning', end=True)),\n"
  ]
 },
 "641": {
  "name": "add_dt",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "128",
  "column": "37",
  "context": "return func\n\ndef ans_pos(pos, new_ans, end=False, add_dt=False):\n    \"\"\"Returns a function that yields new_ans if t",
  "context_lines": "        if q.lower().startswith(wh_word + ' '):\n            return new_ans\n        return None\n    return func\n\ndef ans_pos(pos, new_ans, end=False, add_dt=False):\n    \"\"\"Returns a function that yields new_ans if the first/last token has |pos|.\"\"\"\n    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n",
  "slicing": [
   "def ans_pos(pos, new_ans, end=False, add_dt=False):\n",
   "        ('pos_end_nn', ans_pos('NN', 'hamster', end=True, add_dt=True)),\n",
   "        ('pos_end_nnp', ans_pos('NNP', 'Central Park', end=True, add_dt=True)),\n",
   "        ('pos_end_nns', ans_pos('NNS', 'hamsters', end=True, add_dt=True)),\n",
   "        ('pos_end_nnps', ans_pos('NNPS', 'Kew Gardens', end=True, add_dt=True)),\n"
  ]
 },
 "642": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "130",
  "column": "13",
  "context": "f the first/last token has |pos|.\"\"\"\n    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n       ",
  "context_lines": "        return None\n    return func\n\ndef ans_pos(pos, new_ans, end=False, add_dt=False):\n    \"\"\"Returns a function that yields new_ans if the first/last token has |pos|.\"\"\"\n    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n        else:\n            t = tokens[0]\n",
  "slicing": "    def func(a, tokens, q, determiner, **kwargs):\n"
 },
 "643": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "130",
  "column": "16",
  "context": "he first/last token has |pos|.\"\"\"\n    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n       ",
  "context_lines": "        return None\n    return func\n\ndef ans_pos(pos, new_ans, end=False, add_dt=False):\n    \"\"\"Returns a function that yields new_ans if the first/last token has |pos|.\"\"\"\n    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n        else:\n            t = tokens[0]\n",
  "slicing": "    def func(a, tokens, q, determiner, **kwargs):\n"
 },
 "644": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "130",
  "column": "24",
  "context": "/last token has |pos|.\"\"\"\n    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n       ",
  "context_lines": "        return None\n    return func\n\ndef ans_pos(pos, new_ans, end=False, add_dt=False):\n    \"\"\"Returns a function that yields new_ans if the first/last token has |pos|.\"\"\"\n    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n        else:\n            t = tokens[0]\n",
  "slicing": "    def func(a, tokens, q, determiner, **kwargs):\n"
 },
 "645": {
  "name": "determiner",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "130",
  "column": "27",
  "context": "st token has |pos|.\"\"\"\n    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n       ",
  "context_lines": "        return None\n    return func\n\ndef ans_pos(pos, new_ans, end=False, add_dt=False):\n    \"\"\"Returns a function that yields new_ans if the first/last token has |pos|.\"\"\"\n    def func(a, tokens, q, determiner, **kwargs):\n        if end:\n            t = tokens[-1]\n        else:\n            t = tokens[0]\n",
  "slicing": "    def func(a, tokens, q, determiner, **kwargs):\n"
 },
 "646": {
  "name": "new_ans",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "142",
  "column": "18",
  "context": "n new_ans\n    return func\n\n    \ndef ans_catch_all(new_ans):\n    def func(a, tokens, q, **kwargs):\n        retu",
  "context_lines": "        if add_dt and determiner:\n            return '%s %s' % (determiner, new_ans)\n        return new_ans\n    return func\n\n    \ndef ans_catch_all(new_ans):\n    def func(a, tokens, q, **kwargs):\n        return new_ans\n    return func\n\nANSWER_RULES = [\n",
  "slicing": [
   "def ans_catch_all(new_ans):\n",
   "        return new_ans\n"
  ]
 },
 "647": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "143",
  "column": "13",
  "context": "nc\n\n    \ndef ans_catch_all(new_ans):\n    def func(a, tokens, q, **kwargs):\n        return new_ans\n    return func\n\nANSWER_RUL",
  "context_lines": "            return '%s %s' % (determiner, new_ans)\n        return new_ans\n    return func\n\n    \ndef ans_catch_all(new_ans):\n    def func(a, tokens, q, **kwargs):\n        return new_ans\n    return func\n\nANSWER_RULES = [\n        ('date', ans_date),\n",
  "slicing": "    def func(a, tokens, q, **kwargs):\n"
 },
 "648": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "143",
  "column": "16",
  "context": "\n    \ndef ans_catch_all(new_ans):\n    def func(a, tokens, q, **kwargs):\n        return new_ans\n    return func\n\nANSWER_RUL",
  "context_lines": "            return '%s %s' % (determiner, new_ans)\n        return new_ans\n    return func\n\n    \ndef ans_catch_all(new_ans):\n    def func(a, tokens, q, **kwargs):\n        return new_ans\n    return func\n\nANSWER_RULES = [\n        ('date', ans_date),\n",
  "slicing": "    def func(a, tokens, q, **kwargs):\n"
 },
 "649": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/answer_rules.py",
  "lineno": "143",
  "column": "24",
  "context": "f ans_catch_all(new_ans):\n    def func(a, tokens, q, **kwargs):\n        return new_ans\n    return func\n\nANSWER_RUL",
  "context_lines": "            return '%s %s' % (determiner, new_ans)\n        return new_ans\n    return func\n\n    \ndef ans_catch_all(new_ans):\n    def func(a, tokens, q, **kwargs):\n        return new_ans\n    return func\n\nANSWER_RULES = [\n        ('date', ans_date),\n",
  "slicing": "    def func(a, tokens, q, **kwargs):\n"
 },
 "650": {
  "name": "token",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "18",
  "column": "18",
  "context": ".ADJ,\n        'JJS': wn.ADJ,\n}\n\ndef alter_special(token, **kwargs):\n    w = token['originalText']\n    if w in SPECIAL_",
  "context_lines": "        'JJ': wn.ADJ,\n        'JJR': wn.ADJ,\n        'JJS': wn.ADJ,\n}\n\ndef alter_special(token, **kwargs):\n    w = token['originalText']\n    if w in SPECIAL_ALTERATIONS:\n        return [SPECIAL_ALTERATIONS[w]]\n    return None\n\n",
  "slicing": "def alter_special(token, **kwargs):\n"
 },
 "651": {
  "name": "pos_list",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "24",
  "column": "17",
  "context": "ALTERATIONS[w]]\n    return None\n\ndef alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n    def func(token, nearby_word_dict=None, postag_",
  "context_lines": "    w = token['originalText']\n    if w in SPECIAL_ALTERATIONS:\n        return [SPECIAL_ALTERATIONS[w]]\n    return None\n\ndef alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n    def func(token, nearby_word_dict=None, postag_dict=None, **kwargs):\n        if token['pos'] not in pos_list: return None\n        if is_ner and token['ner'] not in ('PERSON', 'LOCATION', 'ORGANIZATION', 'MISC'):\n            return None\n",
  "slicing": [
   "def alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n",
   "        if token['pos'] not in pos_list: return None\n"
  ]
 },
 "652": {
  "name": "ignore_pos",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "24",
  "column": "27",
  "context": "S[w]]\n    return None\n\ndef alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n    def func(token, nearby_word_dict=None, postag_",
  "context_lines": "    w = token['originalText']\n    if w in SPECIAL_ALTERATIONS:\n        return [SPECIAL_ALTERATIONS[w]]\n    return None\n\ndef alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n    def func(token, nearby_word_dict=None, postag_dict=None, **kwargs):\n        if token['pos'] not in pos_list: return None\n        if is_ner and token['ner'] not in ('PERSON', 'LOCATION', 'ORGANIZATION', 'MISC'):\n            return None\n",
  "slicing": [
   "def alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n",
   "        ('nearbyNum', alter_nearby(['CD'], ignore_pos=True)),\n",
   "        ('nearbyProperNoun', alter_nearby(['NNP', 'NNPS'], ignore_pos=True)),\n"
  ]
 },
 "653": {
  "name": "is_ner",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "24",
  "column": "45",
  "context": "one\n\ndef alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n    def func(token, nearby_word_dict=None, postag_",
  "context_lines": "    w = token['originalText']\n    if w in SPECIAL_ALTERATIONS:\n        return [SPECIAL_ALTERATIONS[w]]\n    return None\n\ndef alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n    def func(token, nearby_word_dict=None, postag_dict=None, **kwargs):\n        if token['pos'] not in pos_list: return None\n        if is_ner and token['ner'] not in ('PERSON', 'LOCATION', 'ORGANIZATION', 'MISC'):\n            return None\n",
  "slicing": [
   "def alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n",
   "        ('nearbyEntityNouns', alter_nearby(['NN', 'NNS'], is_ner=True)),\n",
   "        ('nearbyEntityJJ', alter_nearby(['JJ', 'JJR', 'JJS'], is_ner=True)),\n"
  ]
 },
 "654": {
  "name": "token",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "25",
  "column": "13",
  "context": "st, ignore_pos=False, is_ner=False):\n    def func(token, nearby_word_dict=None, postag_dict=None, **kwargs):\n        if token['pos'] not in pos_list: return No",
  "context_lines": "    if w in SPECIAL_ALTERATIONS:\n        return [SPECIAL_ALTERATIONS[w]]\n    return None\n\ndef alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n    def func(token, nearby_word_dict=None, postag_dict=None, **kwargs):\n        if token['pos'] not in pos_list: return None\n        if is_ner and token['ner'] not in ('PERSON', 'LOCATION', 'ORGANIZATION', 'MISC'):\n            return None\n        w = token['word'].lower()\n",
  "slicing": "    def func(token, nearby_word_dict=None, postag_dict=None, **kwargs):\n"
 },
 "655": {
  "name": "nearby_word_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "25",
  "column": "20",
  "context": "ore_pos=False, is_ner=False):\n    def func(token, nearby_word_dict=None, postag_dict=None, **kwargs):\n        if token['pos'] not in pos_list: return No",
  "context_lines": "    if w in SPECIAL_ALTERATIONS:\n        return [SPECIAL_ALTERATIONS[w]]\n    return None\n\ndef alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n    def func(token, nearby_word_dict=None, postag_dict=None, **kwargs):\n        if token['pos'] not in pos_list: return None\n        if is_ner and token['ner'] not in ('PERSON', 'LOCATION', 'ORGANIZATION', 'MISC'):\n            return None\n        w = token['word'].lower()\n",
  "slicing": "    def func(token, nearby_word_dict=None, postag_dict=None, **kwargs):\n"
 },
 "656": {
  "name": "postag_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "25",
  "column": "43",
  "context": "alse):\n    def func(token, nearby_word_dict=None, postag_dict=None, **kwargs):\n        if token['pos'] not in pos_list: return No",
  "context_lines": "    if w in SPECIAL_ALTERATIONS:\n        return [SPECIAL_ALTERATIONS[w]]\n    return None\n\ndef alter_nearby(pos_list, ignore_pos=False, is_ner=False):\n    def func(token, nearby_word_dict=None, postag_dict=None, **kwargs):\n        if token['pos'] not in pos_list: return None\n        if is_ner and token['ner'] not in ('PERSON', 'LOCATION', 'ORGANIZATION', 'MISC'):\n            return None\n        w = token['word'].lower()\n",
  "slicing": "    def func(token, nearby_word_dict=None, postag_dict=None, **kwargs):\n"
 },
 "657": {
  "name": "token",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "48",
  "column": "23",
  "context": "new_words\n    return func\n\ndef alter_entity_glove(token, nearby_word_dict=None, **kwargs):\n    # NOTE: Deprecated\n    if token['ner'] not in ",
  "context_lines": "                if new_postag != token['pos']: continue \n            new_words.append(new_word)\n        return new_words\n    return func\n\ndef alter_entity_glove(token, nearby_word_dict=None, **kwargs):\n    # NOTE: Deprecated\n    if token['ner'] not in ('PERSON', 'LOCATION', 'ORGANIZATION', 'MISC'): return None\n    w = token['word'].lower()\n    if w == token['word']: return None  # Only do capitalized words\n",
  "slicing": "def alter_entity_glove(token, nearby_word_dict=None, **kwargs):\n"
 },
 "658": {
  "name": "nearby_word_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "48",
  "column": "30",
  "context": "ds\n    return func\n\ndef alter_entity_glove(token, nearby_word_dict=None, **kwargs):\n    # NOTE: Deprecated\n    if token['ner'] not in ",
  "context_lines": "                if new_postag != token['pos']: continue \n            new_words.append(new_word)\n        return new_words\n    return func\n\ndef alter_entity_glove(token, nearby_word_dict=None, **kwargs):\n    # NOTE: Deprecated\n    if token['ner'] not in ('PERSON', 'LOCATION', 'ORGANIZATION', 'MISC'): return None\n    w = token['word'].lower()\n    if w == token['word']: return None  # Only do capitalized words\n",
  "slicing": "def alter_entity_glove(token, nearby_word_dict=None, **kwargs):\n"
 },
 "659": {
  "name": "token",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "62",
  "column": "22",
  "context": "le())\n    return new_words\n\ndef alter_entity_type(token, **kwargs):\n    pos = token['pos']\n    ner = token['ner']\n    ",
  "context_lines": "            new_words.append(x['word'].upper())\n        else:\n            new_words.append(x['word'].title())\n    return new_words\n\ndef alter_entity_type(token, **kwargs):\n    pos = token['pos']\n    ner = token['ner']\n    word = token['word']\n    is_abbrev = word == word.upper() and not word == word.lower()\n",
  "slicing": "def alter_entity_type(token, **kwargs):\n"
 },
 "660": {
  "name": "token",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/alteration_rules.py",
  "lineno": "88",
  "column": "27",
  "context": "eks']\n    return None\n\ndef alter_wordnet_antonyms(token, **kwargs):\n    if token['pos'] not in POS_TO_WORDNET: return ",
  "context_lines": "        return ['Dalek']\n    elif pos == 'NNPS':\n        return ['Daleks']\n    return None\n\ndef alter_wordnet_antonyms(token, **kwargs):\n    if token['pos'] not in POS_TO_WORDNET: return None\n    w = token['word'].lower()\n    wn_pos = POS_TO_WORDNET[token['pos']]\n    synsets = wn.synsets(w, wn_pos)\n",
  "slicing": "def alter_wordnet_antonyms(token, **kwargs):\n"
 },
 "661": {
  "name": "node",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "25",
  "column": "17",
  "context": "', 'p', 'part', 'ppart', '1sg']\n\ndef _check_match(node, pattern_tok):\n    if pattern_tok in CONST_PARSE_MACROS:\n        ",
  "context_lines": "        'vbn': 'ppart',  # past participle\n}\n# Tenses prioritized by likelihood of arising\nPATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n\ndef _check_match(node, pattern_tok):\n    if pattern_tok in CONST_PARSE_MACROS:\n        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n    if ':' in pattern_tok:\n        # ':' means you match the LHS category and start with something on the right\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "662": {
  "name": "pattern_tok",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "25",
  "column": "23",
  "context": ", 'part', 'ppart', '1sg']\n\ndef _check_match(node, pattern_tok):\n    if pattern_tok in CONST_PARSE_MACROS:\n        ",
  "context_lines": "        'vbn': 'ppart',  # past participle\n}\n# Tenses prioritized by likelihood of arising\nPATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n\ndef _check_match(node, pattern_tok):\n    if pattern_tok in CONST_PARSE_MACROS:\n        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n    if ':' in pattern_tok:\n        # ':' means you match the LHS category and start with something on the right\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "663": {
  "name": "pattern_toks",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "41",
  "column": "29",
  "context": "node.word.lower()))\n\ndef _recursive_match_pattern(pattern_toks, stack, matches):\n    \"\"\"Recursively try to match a pattern, greedil",
  "context_lines": "    elif '/' in pattern_tok:\n        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n                    (node.word and pattern_tok.lower() == node.word.lower()))\n\ndef _recursive_match_pattern(pattern_toks, stack, matches):\n    \"\"\"Recursively try to match a pattern, greedily.\"\"\"\n    if len(matches) == len(pattern_toks):\n        # We matched everything in the pattern; also need stack to be empty\n        return len(stack) == 0\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "664": {
  "name": "stack",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "41",
  "column": "43",
  "context": "r()))\n\ndef _recursive_match_pattern(pattern_toks, stack, matches):\n    \"\"\"Recursively try to match a pattern, greedil",
  "context_lines": "    elif '/' in pattern_tok:\n        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n                    (node.word and pattern_tok.lower() == node.word.lower()))\n\ndef _recursive_match_pattern(pattern_toks, stack, matches):\n    \"\"\"Recursively try to match a pattern, greedily.\"\"\"\n    if len(matches) == len(pattern_toks):\n        # We matched everything in the pattern; also need stack to be empty\n        return len(stack) == 0\n",
  "slicing": [
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n"
  ]
 },
 "665": {
  "name": "matches",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "41",
  "column": "50",
  "context": "def _recursive_match_pattern(pattern_toks, stack, matches):\n    \"\"\"Recursively try to match a pattern, greedil",
  "context_lines": "    elif '/' in pattern_tok:\n        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n                    (node.word and pattern_tok.lower() == node.word.lower()))\n\ndef _recursive_match_pattern(pattern_toks, stack, matches):\n    \"\"\"Recursively try to match a pattern, greedily.\"\"\"\n    if len(matches) == len(pattern_toks):\n        # We matched everything in the pattern; also need stack to be empty\n        return len(stack) == 0\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "666": {
  "name": "pattern",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "65",
  "column": "18",
  "context": "(pattern_toks, stack, matches)\n\ndef match_pattern(pattern, const_parse):\n    pattern_toks = pattern.split(' ')\n    whole_ph",
  "context_lines": "    # Recurse to children\n    if not node.children: return False  # No children to recurse on, we failed\n    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n    return _recursive_match_pattern(pattern_toks, stack, matches)\n\ndef match_pattern(pattern, const_parse):\n    pattern_toks = pattern.split(' ')\n    whole_phrase = const_parse.get_phrase()\n    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n        # Match trailing punctuation as needed\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "667": {
  "name": "const_parse",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "65",
  "column": "27",
  "context": "toks, stack, matches)\n\ndef match_pattern(pattern, const_parse):\n    pattern_toks = pattern.split(' ')\n    whole_ph",
  "context_lines": "    # Recurse to children\n    if not node.children: return False  # No children to recurse on, we failed\n    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n    return _recursive_match_pattern(pattern_toks, stack, matches)\n\ndef match_pattern(pattern, const_parse):\n    pattern_toks = pattern.split(' ')\n    whole_phrase = const_parse.get_phrase()\n    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n        # Match trailing punctuation as needed\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "668": {
  "name": "s",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "78",
  "column": "23",
  "context": "else:\n        return None\n\ndef run_postprocessing(s, rules, all_args):\n    rule_list = rules.split(',')\n    for rule in r",
  "context_lines": "    if success:\n        return matches\n    else:\n        return None\n\ndef run_postprocessing(s, rules, all_args):\n    rule_list = rules.split(',')\n    for rule in rule_list:\n        if rule == 'lower':\n            s = s.lower()\n",
  "slicing": [
   "def run_postprocessing(s, rules, all_args):\n",
   "    return s + '.'\n"
  ]
 },
 "669": {
  "name": "rules",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "78",
  "column": "26",
  "context": "e:\n        return None\n\ndef run_postprocessing(s, rules, all_args):\n    rule_list = rules.split(',')\n    for rule in r",
  "context_lines": "    if success:\n        return matches\n    else:\n        return None\n\ndef run_postprocessing(s, rules, all_args):\n    rule_list = rules.split(',')\n    for rule in rule_list:\n        if rule == 'lower':\n            s = s.lower()\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "670": {
  "name": "all_args",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "78",
  "column": "33",
  "context": "    return None\n\ndef run_postprocessing(s, rules, all_args):\n    rule_list = rules.split(',')\n    for rule in r",
  "context_lines": "    if success:\n        return matches\n    else:\n        return None\n\ndef run_postprocessing(s, rules, all_args):\n    rule_list = rules.split(',')\n    for rule in rule_list:\n        if rule == 'lower':\n            s = s.lower()\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "671": {
  "name": "node",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "97",
  "column": "16",
  "context": "S_TO_PATTERN[rule])\n    return s\n\ndef convert_whp(node, q, a, tokens, quiet=False):\n    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'W",
  "context_lines": "            s = patten.conjugate(s, tense)\n        elif rule in POS_TO_PATTERN:\n            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n    return s\n\ndef convert_whp(node, q, a, tokens, quiet=False):\n    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n        # Apply WHP rules\n        cur_phrase = node.get_phrase()\n        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "672": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "97",
  "column": "22",
  "context": "ATTERN[rule])\n    return s\n\ndef convert_whp(node, q, a, tokens, quiet=False):\n    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'W",
  "context_lines": "            s = patten.conjugate(s, tense)\n        elif rule in POS_TO_PATTERN:\n            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n    return s\n\ndef convert_whp(node, q, a, tokens, quiet=False):\n    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n        # Apply WHP rules\n        cur_phrase = node.get_phrase()\n        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "673": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "97",
  "column": "25",
  "context": "ERN[rule])\n    return s\n\ndef convert_whp(node, q, a, tokens, quiet=False):\n    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'W",
  "context_lines": "            s = patten.conjugate(s, tense)\n        elif rule in POS_TO_PATTERN:\n            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n    return s\n\ndef convert_whp(node, q, a, tokens, quiet=False):\n    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n        # Apply WHP rules\n        cur_phrase = node.get_phrase()\n        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "674": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "97",
  "column": "28",
  "context": "[rule])\n    return s\n\ndef convert_whp(node, q, a, tokens, quiet=False):\n    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'W",
  "context_lines": "            s = patten.conjugate(s, tense)\n        elif rule in POS_TO_PATTERN:\n            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n    return s\n\ndef convert_whp(node, q, a, tokens, quiet=False):\n    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n        # Apply WHP rules\n        cur_phrase = node.get_phrase()\n        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "675": {
  "name": "quiet",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "97",
  "column": "36",
  "context": "    return s\n\ndef convert_whp(node, q, a, tokens, quiet=False):\n    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'W",
  "context_lines": "            s = patten.conjugate(s, tense)\n        elif rule in POS_TO_PATTERN:\n            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n    return s\n\ndef convert_whp(node, q, a, tokens, quiet=False):\n    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n        # Apply WHP rules\n        cur_phrase = node.get_phrase()\n        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
  "slicing": "def convert_whp(node, q, a, tokens, quiet=False):\n"
 },
 "676": {
  "name": "s",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "111",
  "column": "14",
  "context": "uestions into declarative sentences\ndef fix_style(s):\n    \"\"\"Minor, general style fixes for questions.\"\"",
  "context_lines": "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n                return phrase\n    return None\n\n### Rules for converting questions into declarative sentences\ndef fix_style(s):\n    \"\"\"Minor, general style fixes for questions.\"\"\"\n    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n    s = s.strip(' .')\n    if s[0] == s[0].lower():\n",
  "slicing": [
   "CONST_PARSE_MACROS = {\n",
   "POS_TO_PATTERN = {\n",
   "PATTERN_TENSES = ['inf', '3sg', 'p', 'part', 'ppart', '1sg']\n",
   "def _check_match(node, pattern_tok):\n",
   "    if pattern_tok in CONST_PARSE_MACROS:\n",
   "        pattern_tok = CONST_PARSE_MACROS[pattern_tok]\n",
   "    if ':' in pattern_tok:\n",
   "        lhs, rhs = pattern_tok.split(':')\n",
   "        match_lhs = _check_match(node, lhs)\n",
   "        if not match_lhs: return False\n",
   "        phrase = node.get_phrase().lower()\n",
   "        retval = any(phrase.startswith(w) for w in rhs.split('/'))\n",
   "        return retval\n",
   "    elif '/' in pattern_tok:\n",
   "        return any(_check_match(node, t) for t in pattern_tok.split('/'))\n",
   "    return ((pattern_tok.startswith('$') and pattern_tok[1:] == node.tag) or\n",
   "                    (node.word and pattern_tok.lower() == node.word.lower()))\n",
   "def _recursive_match_pattern(pattern_toks, stack, matches):\n",
   "    cur_tok = pattern_toks[len(matches)]\n",
   "    node = stack.pop()\n",
   "    is_match = _check_match(node, cur_tok)\n",
   "    if is_match:\n",
   "        cur_num_matches = len(matches)\n",
   "        matches.append(node)\n",
   "        new_stack = list(stack)\n",
   "        success = _recursive_match_pattern(pattern_toks, new_stack, matches)\n",
   "        if success: return True\n",
   "        while len(matches) > cur_num_matches:\n",
   "    if not node.children: return False  # No children to recurse on, we failed\n",
   "    stack.extend(node.children[::-1])  # Leftmost children should be popped first\n",
   "    return _recursive_match_pattern(pattern_toks, stack, matches)\n",
   "def match_pattern(pattern, const_parse):\n",
   "    pattern_toks = pattern.split(' ')\n",
   "    whole_phrase = const_parse.get_phrase()\n",
   "    if whole_phrase.endswith('?') or whole_phrase.endswith('.'):\n",
   "        pattern_toks.append(whole_phrase[-1])\n",
   "    matches = []\n",
   "    success = _recursive_match_pattern(pattern_toks, [const_parse], matches)\n",
   "    if success:\n",
   "        return matches\n",
   "def run_postprocessing(s, rules, all_args):\n",
   "    rule_list = rules.split(',')\n",
   "    for rule in rule_list:\n",
   "        if rule == 'lower':\n",
   "            s = s.lower()\n",
   "        elif rule.startswith('tense-'):\n",
   "            ind = int(rule[6:])\n",
   "            orig_vb = all_args[ind]\n",
   "            tenses = patten.tenses(orig_vb)\n",
   "            for tense in PATTERN_TENSES:  # Prioritize by PATTERN_TENSES\n",
   "                if tense in tenses:\n",
   "                tense = PATTERN_TENSES[0]\n",
   "            s = patten.conjugate(s, tense)\n",
   "        elif rule in POS_TO_PATTERN:\n",
   "            s = patten.conjugate(s, POS_TO_PATTERN[rule])\n",
   "    return s\n",
   "def convert_whp(node, q, a, tokens, quiet=False):\n",
   "    if node.tag in ('WHNP', 'WHADJP', 'WHADVP', 'WHPP'):\n",
   "        cur_phrase = node.get_phrase()\n",
   "        cur_tokens = tokens[node.get_start_index():node.get_end_index()]\n",
   "        for r in WHP_RULES:\n",
   "            phrase = r.convert(cur_phrase, a, cur_tokens, node, run_fix_style=False)\n",
   "            if phrase:\n",
   "                    print(f\"  WHP Rule '{r.name}': {phrase}\")\n",
   "                return phrase\n",
   "def fix_style(s):\n",
   "    s = s.replace('?', '')  # Delete question marks anywhere in sentence.\n",
   "    s = s.strip(' .')\n",
   "    if s[0] == s[0].lower():\n",
   "        s = s[0].upper() + s[1:]\n",
   "    return s + '.'\n",
   "        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n",
   "        match = match_pattern(self.in_pattern, const_parse)\n",
   "        appended_clause = False\n",
   "        if not match:\n",
   "            appended_clause = True\n",
   "            new_pattern = '$PP , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match:\n",
   "            new_pattern = '$SBAR , ' + self.in_pattern\n",
   "            pattern_toks = new_pattern.split(' ')\n",
   "            match = match_pattern(new_pattern, const_parse)\n",
   "        if not match: return None\n",
   "        appended_clause_match = None\n",
   "        fmt_args = [a]\n",
   "        for t, m in zip(pattern_toks, match):\n",
   "            if t.startswith('$') or '/' in t:\n",
   "                phrase = convert_whp(m, q, a, tokens)\n",
   "                if not phrase:\n",
   "                    phrase = m.get_phrase()\n",
   "                fmt_args.append(phrase)\n",
   "        if appended_clause:\n",
   "            appended_clause_match = fmt_args[1]\n",
   "            fmt_args = [a] + fmt_args[2:]\n",
   "        for i in range(len(fmt_args)):\n",
   "            if i in self.postproc:\n",
   "                fmt_args[i] = run_postprocessing(fmt_args[i], self.postproc[i], fmt_args)\n",
   "        output = self.gen_output(fmt_args)\n",
   "        if appended_clause:\n",
   "            output = appended_clause_match + ', ' + output\n",
   "            output = fix_style(output)\n",
   "        return output\n",
   "        return self.out_pattern.format(*fmt_args)\n",
   "        t_toks = self.target.split(' ')\n",
   "        q_toks = q.rstrip('?.').split(' ')\n",
   "        replacement_text = self.replacement.format(a)\n",
   "        for i in range(len(q_toks)):\n",
   "            if self.start and i != 0: continue\n",
   "            if ' '.join(q_toks[i:i + len(t_toks)]).rstrip(',').lower() == self.target:\n",
   "                begin = q_toks[:i]\n",
   "                end = q_toks[i + len(t_toks):]\n",
   "                output = ' '.join(begin + [replacement_text] + end)\n",
   "                    output = fix_style(output)\n",
   "                return output\n",
   "        if node.word:\n",
   "            return node.word, found_whp\n",
   "            whp_phrase = convert_whp(node, q, a, tokens)\n",
   "            if whp_phrase:\n",
   "                return whp_phrase, True\n",
   "        child_phrases = []\n",
   "        for c in node.children[::-1]:\n",
   "            c_phrase, found_whp = self._recursive_convert(c, q, a, tokens, found_whp)\n",
   "            child_phrases.append(c_phrase)\n",
   "        out_toks = []\n",
   "        for i, p in enumerate(child_phrases[::-1]):\n",
   "            if i == 0 or p.startswith(\"'\"):\n",
   "                out_toks.append(p)\n",
   "                out_toks.append(' ' + p)\n",
   "        return ''.join(out_toks), found_whp\n",
   "        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n",
   "        if found_whp:\n",
   "                out_phrase = fix_style(out_phrase)\n",
   "            return out_phrase\n",
   "        return a\n"
  ]
 },
 "677": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "120",
  "column": "22",
  "context": "ass ConversionRule(object):\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\nclass Constitue",
  "context_lines": "    if s[0] == s[0].lower():\n        s = s[0].upper() + s[1:]\n    return s + '.'\n\nclass ConversionRule(object):\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\nclass ConstituencyRule(ConversionRule):\n    \"\"\"A rule for converting question to sentence based on constituency parse.\"\"\"\n    def __init__(self, in_pattern, out_pattern, postproc=None):\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "678": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "120",
  "column": "25",
  "context": " ConversionRule(object):\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\nclass Constitue",
  "context_lines": "    if s[0] == s[0].lower():\n        s = s[0].upper() + s[1:]\n    return s + '.'\n\nclass ConversionRule(object):\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\nclass ConstituencyRule(ConversionRule):\n    \"\"\"A rule for converting question to sentence based on constituency parse.\"\"\"\n    def __init__(self, in_pattern, out_pattern, postproc=None):\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "679": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "120",
  "column": "28",
  "context": "nversionRule(object):\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\nclass Constitue",
  "context_lines": "    if s[0] == s[0].lower():\n        s = s[0].upper() + s[1:]\n    return s + '.'\n\nclass ConversionRule(object):\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\nclass ConstituencyRule(ConversionRule):\n    \"\"\"A rule for converting question to sentence based on constituency parse.\"\"\"\n    def __init__(self, in_pattern, out_pattern, postproc=None):\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "680": {
  "name": "const_parse",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "120",
  "column": "36",
  "context": "Rule(object):\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\nclass Constitue",
  "context_lines": "    if s[0] == s[0].lower():\n        s = s[0].upper() + s[1:]\n    return s + '.'\n\nclass ConversionRule(object):\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\nclass ConstituencyRule(ConversionRule):\n    \"\"\"A rule for converting question to sentence based on constituency parse.\"\"\"\n    def __init__(self, in_pattern, out_pattern, postproc=None):\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "681": {
  "name": "run_fix_style",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "120",
  "column": "49",
  "context": "\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\nclass Constitue",
  "context_lines": "    if s[0] == s[0].lower():\n        s = s[0].upper() + s[1:]\n    return s + '.'\n\nclass ConversionRule(object):\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\nclass ConstituencyRule(ConversionRule):\n    \"\"\"A rule for converting question to sentence based on constituency parse.\"\"\"\n    def __init__(self, in_pattern, out_pattern, postproc=None):\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "682": {
  "name": "in_pattern",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "125",
  "column": "23",
  "context": " on constituency parse.\"\"\"\n    def __init__(self, in_pattern, out_pattern, postproc=None):\n        self.in_pattern = in_pattern   # e.g. \"whe",
  "context_lines": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\nclass ConstituencyRule(ConversionRule):\n    \"\"\"A rule for converting question to sentence based on constituency parse.\"\"\"\n    def __init__(self, in_pattern, out_pattern, postproc=None):\n        self.in_pattern = in_pattern   # e.g. \"where did $NP $VP\"\n        self.out_pattern = out_pattern #unicode(out_pattern)\n                # e.g. \"{1} did {2} at {0}.\"  Answer is always 0\n        self.name = in_pattern\n",
  "slicing": "    def __init__(self, in_pattern, out_pattern, postproc=None):\n"
 },
 "683": {
  "name": "out_pattern",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "125",
  "column": "35",
  "context": "ency parse.\"\"\"\n    def __init__(self, in_pattern, out_pattern, postproc=None):\n        self.in_pattern = in_pattern   # e.g. \"whe",
  "context_lines": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\nclass ConstituencyRule(ConversionRule):\n    \"\"\"A rule for converting question to sentence based on constituency parse.\"\"\"\n    def __init__(self, in_pattern, out_pattern, postproc=None):\n        self.in_pattern = in_pattern   # e.g. \"where did $NP $VP\"\n        self.out_pattern = out_pattern #unicode(out_pattern)\n                # e.g. \"{1} did {2} at {0}.\"  Answer is always 0\n        self.name = in_pattern\n",
  "slicing": "    def __init__(self, in_pattern, out_pattern, postproc=None):\n"
 },
 "684": {
  "name": "postproc",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "125",
  "column": "48",
  "context": "\"\n    def __init__(self, in_pattern, out_pattern, postproc=None):\n        self.in_pattern = in_pattern   # e.g. \"whe",
  "context_lines": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        raise NotImplementedError\n\nclass ConstituencyRule(ConversionRule):\n    \"\"\"A rule for converting question to sentence based on constituency parse.\"\"\"\n    def __init__(self, in_pattern, out_pattern, postproc=None):\n        self.in_pattern = in_pattern   # e.g. \"where did $NP $VP\"\n        self.out_pattern = out_pattern #unicode(out_pattern)\n                # e.g. \"{1} did {2} at {0}.\"  Answer is always 0\n        self.name = in_pattern\n",
  "slicing": "    def __init__(self, in_pattern, out_pattern, postproc=None):\n"
 },
 "685": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "135",
  "column": "22",
  "context": "        self.postproc = {}\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ') ",
  "context_lines": "        if postproc:\n            self.postproc = postproc\n        else:\n            self.postproc = {}\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n        match = match_pattern(self.in_pattern, const_parse)\n        appended_clause = False\n        if not match:\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n"
 },
 "686": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "135",
  "column": "25",
  "context": "     self.postproc = {}\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ') ",
  "context_lines": "        if postproc:\n            self.postproc = postproc\n        else:\n            self.postproc = {}\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n        match = match_pattern(self.in_pattern, const_parse)\n        appended_clause = False\n        if not match:\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n"
 },
 "687": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "135",
  "column": "28",
  "context": "  self.postproc = {}\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ') ",
  "context_lines": "        if postproc:\n            self.postproc = postproc\n        else:\n            self.postproc = {}\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n        match = match_pattern(self.in_pattern, const_parse)\n        appended_clause = False\n        if not match:\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n"
 },
 "688": {
  "name": "const_parse",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "135",
  "column": "36",
  "context": "ostproc = {}\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ') ",
  "context_lines": "        if postproc:\n            self.postproc = postproc\n        else:\n            self.postproc = {}\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n        match = match_pattern(self.in_pattern, const_parse)\n        appended_clause = False\n        if not match:\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n"
 },
 "689": {
  "name": "run_fix_style",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "135",
  "column": "49",
  "context": "\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ') ",
  "context_lines": "        if postproc:\n            self.postproc = postproc\n        else:\n            self.postproc = {}\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n        pattern_toks = self.in_pattern.split(' ')   # Don't care about trailing punctuation\n        match = match_pattern(self.in_pattern, const_parse)\n        appended_clause = False\n        if not match:\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True) -> str:\n"
 },
 "690": {
  "name": "fmt_args",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "175",
  "column": "25",
  "context": "\n        return output\n\n\n    def gen_output(self, fmt_args):\n        \"\"\"By default, use self.out_pattern.  Can ",
  "context_lines": "            output = appended_clause_match + ', ' + output\n        if run_fix_style:\n            output = fix_style(output)\n        return output\n\n\n    def gen_output(self, fmt_args):\n        \"\"\"By default, use self.out_pattern.  Can be overridden.\"\"\"\n        return self.out_pattern.format(*fmt_args)\n\nclass ReplaceRule(ConversionRule):\n    \"\"\"A simple rule that replaces some tokens with the answer.\"\"\"\n",
  "slicing": "    def gen_output(self, fmt_args):\n"
 },
 "691": {
  "name": "target",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "181",
  "column": "23",
  "context": "tokens with the answer.\"\"\"\n    def __init__(self, target, replacement='{}', start=False):\n        self.target = target\n        self.replacem",
  "context_lines": "        \"\"\"By default, use self.out_pattern.  Can be overridden.\"\"\"\n        return self.out_pattern.format(*fmt_args)\n\nclass ReplaceRule(ConversionRule):\n    \"\"\"A simple rule that replaces some tokens with the answer.\"\"\"\n    def __init__(self, target, replacement='{}', start=False):\n        self.target = target\n        self.replacement = replacement #unicode(replacement)\n        self.name = 'replace(%s)' % target\n        self.start = start\n\n",
  "slicing": "    def __init__(self, target, replacement='{}', start=False):\n"
 },
 "692": {
  "name": "replacement",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "181",
  "column": "31",
  "context": "ith the answer.\"\"\"\n    def __init__(self, target, replacement='{}', start=False):\n        self.target = target\n        self.replacem",
  "context_lines": "        \"\"\"By default, use self.out_pattern.  Can be overridden.\"\"\"\n        return self.out_pattern.format(*fmt_args)\n\nclass ReplaceRule(ConversionRule):\n    \"\"\"A simple rule that replaces some tokens with the answer.\"\"\"\n    def __init__(self, target, replacement='{}', start=False):\n        self.target = target\n        self.replacement = replacement #unicode(replacement)\n        self.name = 'replace(%s)' % target\n        self.start = start\n\n",
  "slicing": "    def __init__(self, target, replacement='{}', start=False):\n"
 },
 "693": {
  "name": "start",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "181",
  "column": "49",
  "context": "\n    def __init__(self, target, replacement='{}', start=False):\n        self.target = target\n        self.replacem",
  "context_lines": "        \"\"\"By default, use self.out_pattern.  Can be overridden.\"\"\"\n        return self.out_pattern.format(*fmt_args)\n\nclass ReplaceRule(ConversionRule):\n    \"\"\"A simple rule that replaces some tokens with the answer.\"\"\"\n    def __init__(self, target, replacement='{}', start=False):\n        self.target = target\n        self.replacement = replacement #unicode(replacement)\n        self.name = 'replace(%s)' % target\n        self.start = start\n\n",
  "slicing": "    def __init__(self, target, replacement='{}', start=False):\n"
 },
 "694": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "187",
  "column": "22",
  "context": "        self.start = start\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_",
  "context_lines": "        self.target = target\n        self.replacement = replacement #unicode(replacement)\n        self.name = 'replace(%s)' % target\n        self.start = start\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_toks = q.rstrip('?.').split(' ')\n        replacement_text = self.replacement.format(a)\n        for i in range(len(q_toks)):\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "695": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "187",
  "column": "25",
  "context": "     self.start = start\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_",
  "context_lines": "        self.target = target\n        self.replacement = replacement #unicode(replacement)\n        self.name = 'replace(%s)' % target\n        self.start = start\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_toks = q.rstrip('?.').split(' ')\n        replacement_text = self.replacement.format(a)\n        for i in range(len(q_toks)):\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "696": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "187",
  "column": "28",
  "context": "  self.start = start\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_",
  "context_lines": "        self.target = target\n        self.replacement = replacement #unicode(replacement)\n        self.name = 'replace(%s)' % target\n        self.start = start\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_toks = q.rstrip('?.').split(' ')\n        replacement_text = self.replacement.format(a)\n        for i in range(len(q_toks)):\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "697": {
  "name": "const_parse",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "187",
  "column": "36",
  "context": "tart = start\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_",
  "context_lines": "        self.target = target\n        self.replacement = replacement #unicode(replacement)\n        self.name = 'replace(%s)' % target\n        self.start = start\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_toks = q.rstrip('?.').split(' ')\n        replacement_text = self.replacement.format(a)\n        for i in range(len(q_toks)):\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "698": {
  "name": "run_fix_style",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "187",
  "column": "49",
  "context": "\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_",
  "context_lines": "        self.target = target\n        self.replacement = replacement #unicode(replacement)\n        self.name = 'replace(%s)' % target\n        self.start = start\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        t_toks = self.target.split(' ')\n        q_toks = q.rstrip('?.').split(' ')\n        replacement_text = self.replacement.format(a)\n        for i in range(len(q_toks)):\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "699": {
  "name": "node",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "205",
  "column": "33",
  "context": "name = 'FindWHP'\n    def _recursive_convert(self, node, q, a, tokens, found_whp):\n        if node.word:\n            return node.word",
  "context_lines": "        return None\n\nclass FindWHPRule(ConversionRule):\n    \"\"\"A rule that looks for $WHP's from right to left and does replacements.\"\"\"\n    name = 'FindWHP'\n    def _recursive_convert(self, node, q, a, tokens, found_whp):\n        if node.word:\n            return node.word, found_whp\n        if not found_whp:\n            whp_phrase = convert_whp(node, q, a, tokens)\n",
  "slicing": "    def _recursive_convert(self, node, q, a, tokens, found_whp):\n"
 },
 "700": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "205",
  "column": "39",
  "context": " 'FindWHP'\n    def _recursive_convert(self, node, q, a, tokens, found_whp):\n        if node.word:\n            return node.word",
  "context_lines": "        return None\n\nclass FindWHPRule(ConversionRule):\n    \"\"\"A rule that looks for $WHP's from right to left and does replacements.\"\"\"\n    name = 'FindWHP'\n    def _recursive_convert(self, node, q, a, tokens, found_whp):\n        if node.word:\n            return node.word, found_whp\n        if not found_whp:\n            whp_phrase = convert_whp(node, q, a, tokens)\n",
  "slicing": "    def _recursive_convert(self, node, q, a, tokens, found_whp):\n"
 },
 "701": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "205",
  "column": "42",
  "context": "indWHP'\n    def _recursive_convert(self, node, q, a, tokens, found_whp):\n        if node.word:\n            return node.word",
  "context_lines": "        return None\n\nclass FindWHPRule(ConversionRule):\n    \"\"\"A rule that looks for $WHP's from right to left and does replacements.\"\"\"\n    name = 'FindWHP'\n    def _recursive_convert(self, node, q, a, tokens, found_whp):\n        if node.word:\n            return node.word, found_whp\n        if not found_whp:\n            whp_phrase = convert_whp(node, q, a, tokens)\n",
  "slicing": "    def _recursive_convert(self, node, q, a, tokens, found_whp):\n"
 },
 "702": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "205",
  "column": "45",
  "context": "WHP'\n    def _recursive_convert(self, node, q, a, tokens, found_whp):\n        if node.word:\n            return node.word",
  "context_lines": "        return None\n\nclass FindWHPRule(ConversionRule):\n    \"\"\"A rule that looks for $WHP's from right to left and does replacements.\"\"\"\n    name = 'FindWHP'\n    def _recursive_convert(self, node, q, a, tokens, found_whp):\n        if node.word:\n            return node.word, found_whp\n        if not found_whp:\n            whp_phrase = convert_whp(node, q, a, tokens)\n",
  "slicing": "    def _recursive_convert(self, node, q, a, tokens, found_whp):\n"
 },
 "703": {
  "name": "found_whp",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "205",
  "column": "53",
  "context": " def _recursive_convert(self, node, q, a, tokens, found_whp):\n        if node.word:\n            return node.word",
  "context_lines": "        return None\n\nclass FindWHPRule(ConversionRule):\n    \"\"\"A rule that looks for $WHP's from right to left and does replacements.\"\"\"\n    name = 'FindWHP'\n    def _recursive_convert(self, node, q, a, tokens, found_whp):\n        if node.word:\n            return node.word, found_whp\n        if not found_whp:\n            whp_phrase = convert_whp(node, q, a, tokens)\n",
  "slicing": "    def _recursive_convert(self, node, q, a, tokens, found_whp):\n"
 },
 "704": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "224",
  "column": "22",
  "context": ".join(out_toks), found_whp\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_co",
  "context_lines": "                out_toks.append(p)\n            else:\n                out_toks.append(' ' + p)\n        return ''.join(out_toks), found_whp\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n        if found_whp:\n            if run_fix_style:\n                out_phrase = fix_style(out_phrase)\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "705": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "224",
  "column": "25",
  "context": "in(out_toks), found_whp\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_co",
  "context_lines": "                out_toks.append(p)\n            else:\n                out_toks.append(' ' + p)\n        return ''.join(out_toks), found_whp\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n        if found_whp:\n            if run_fix_style:\n                out_phrase = fix_style(out_phrase)\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "706": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "224",
  "column": "28",
  "context": "out_toks), found_whp\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_co",
  "context_lines": "                out_toks.append(p)\n            else:\n                out_toks.append(' ' + p)\n        return ''.join(out_toks), found_whp\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n        if found_whp:\n            if run_fix_style:\n                out_phrase = fix_style(out_phrase)\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "707": {
  "name": "const_parse",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "224",
  "column": "36",
  "context": "), found_whp\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_co",
  "context_lines": "                out_toks.append(p)\n            else:\n                out_toks.append(' ' + p)\n        return ''.join(out_toks), found_whp\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n        if found_whp:\n            if run_fix_style:\n                out_phrase = fix_style(out_phrase)\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "708": {
  "name": "run_fix_style",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "224",
  "column": "49",
  "context": "\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_co",
  "context_lines": "                out_toks.append(p)\n            else:\n                out_toks.append(' ' + p)\n        return ''.join(out_toks), found_whp\n\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        out_phrase, found_whp = self._recursive_convert(const_parse, q, a, tokens, False)\n        if found_whp:\n            if run_fix_style:\n                out_phrase = fix_style(out_phrase)\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "709": {
  "name": "q",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "235",
  "column": "22",
  "context": "\"\"\"\n    name = 'AnswerRule'\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        return a\n\nCONVERSION_RULES = [\n        # S",
  "context_lines": "        return None\n\nclass AnswerRule(ConversionRule):\n    \"\"\"Just return the answer.\"\"\"\n    name = 'AnswerRule'\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        return a\n\nCONVERSION_RULES = [\n        # Special rules\n        ConstituencyRule('$WHP:what $Be $NP called that $VP', '{2} that {3} {1} called {1}'),\n\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "710": {
  "name": "a",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "235",
  "column": "25",
  "context": "\n    name = 'AnswerRule'\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        return a\n\nCONVERSION_RULES = [\n        # S",
  "context_lines": "        return None\n\nclass AnswerRule(ConversionRule):\n    \"\"\"Just return the answer.\"\"\"\n    name = 'AnswerRule'\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        return a\n\nCONVERSION_RULES = [\n        # Special rules\n        ConstituencyRule('$WHP:what $Be $NP called that $VP', '{2} that {3} {1} called {1}'),\n\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "711": {
  "name": "tokens",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "235",
  "column": "28",
  "context": "  name = 'AnswerRule'\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        return a\n\nCONVERSION_RULES = [\n        # S",
  "context_lines": "        return None\n\nclass AnswerRule(ConversionRule):\n    \"\"\"Just return the answer.\"\"\"\n    name = 'AnswerRule'\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        return a\n\nCONVERSION_RULES = [\n        # Special rules\n        ConstituencyRule('$WHP:what $Be $NP called that $VP', '{2} that {3} {1} called {1}'),\n\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "712": {
  "name": "const_parse",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "235",
  "column": "36",
  "context": " 'AnswerRule'\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        return a\n\nCONVERSION_RULES = [\n        # S",
  "context_lines": "        return None\n\nclass AnswerRule(ConversionRule):\n    \"\"\"Just return the answer.\"\"\"\n    name = 'AnswerRule'\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        return a\n\nCONVERSION_RULES = [\n        # Special rules\n        ConstituencyRule('$WHP:what $Be $NP called that $VP', '{2} that {3} {1} called {1}'),\n\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "713": {
  "name": "run_fix_style",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/addsent/rules/conversion_rules.py",
  "lineno": "235",
  "column": "49",
  "context": "\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        return a\n\nCONVERSION_RULES = [\n        # S",
  "context_lines": "        return None\n\nclass AnswerRule(ConversionRule):\n    \"\"\"Just return the answer.\"\"\"\n    name = 'AnswerRule'\n    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n        return a\n\nCONVERSION_RULES = [\n        # Special rules\n        ConstituencyRule('$WHP:what $Be $NP called that $VP', '{2} that {3} {1} called {1}'),\n\n",
  "slicing": "    def convert(self, q, a, tokens, const_parse, run_fix_style=True):\n"
 },
 "714": {
  "name": "lazy",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "56",
  "column": "17",
  "context": ".\n    \"\"\"\n    def __init__(self,\n                 lazy: bool = False,\n                 tokenizer: Tokenizer = None,\n    ",
  "context_lines": "        token_indexers : Indexers used to define input token\n            representations.\n    \"\"\"\n    def __init__(self,\n                 lazy: bool = False,\n                 tokenizer: Tokenizer = None,\n                 token_indexers: Dict[str, TokenIndexer] = None) -> None:\n        super().__init__(lazy)\n        self._tokenizer = tokenizer or WordTokenizer()\n",
  "slicing": "                 lazy: bool = False,\n"
 },
 "715": {
  "name": "tokenizer",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "57",
  "column": "17",
  "context": "             lazy: bool = False,\n                 tokenizer: Tokenizer = None,\n                 token_indexers: Dict[str, TokenIn",
  "context_lines": "            representations.\n    \"\"\"\n    def __init__(self,\n                 lazy: bool = False,\n                 tokenizer: Tokenizer = None,\n                 token_indexers: Dict[str, TokenIndexer] = None) -> None:\n        super().__init__(lazy)\n        self._tokenizer = tokenizer or WordTokenizer()\n        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n\n",
  "slicing": "                 tokenizer: Tokenizer = None,\n"
 },
 "716": {
  "name": "token_indexers",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "58",
  "column": "17",
  "context": "    tokenizer: Tokenizer = None,\n                 token_indexers: Dict[str, TokenIndexer] = None) -> None:\n        super().__init__(lazy)\n        self._token",
  "context_lines": "    \"\"\"\n    def __init__(self,\n                 lazy: bool = False,\n                 tokenizer: Tokenizer = None,\n                 token_indexers: Dict[str, TokenIndexer] = None) -> None:\n        super().__init__(lazy)\n        self._tokenizer = tokenizer or WordTokenizer()\n        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n\n    @overrides\n",
  "slicing": "                 token_indexers: Dict[str, TokenIndexer] = None) -> None:\n"
 },
 "717": {
  "name": "file_path",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "64",
  "column": "20",
  "context": "kenIndexer()}\n\n    @overrides\n    def _read(self, file_path):\n        with open(cached_path(file_path), \"r\") as ",
  "context_lines": "        super().__init__(lazy)\n        self._tokenizer = tokenizer or WordTokenizer()\n        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n\n    @overrides\n    def _read(self, file_path):\n        with open(cached_path(file_path), \"r\") as data_file:\n            logger.info(\"Reading instances from: %s\", file_path)\n            json_data = json.load(data_file)\n            for video_id, value in json_data.items():\n",
  "slicing": "    def _read(self, file_path):\n"
 },
 "718": {
  "name": "video_id",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "76",
  "column": "25",
  "context": "f text_to_instance(self,\n                         video_id: str,\n                         first_sentence: str,\n    ",
  "context_lines": "                for first_sentence, second_sentence in pairwise(sentences):\n                    yield self.text_to_instance(video_id, first_sentence, second_sentence)\n\n    @overrides\n    def text_to_instance(self,\n                         video_id: str,\n                         first_sentence: str,\n                         second_sentence: str) -> Instance:  # type: ignore\n        # pylint: disable=arguments-differ\n        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n",
  "slicing": "                         video_id: str,\n"
 },
 "719": {
  "name": "first_sentence",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "77",
  "column": "25",
  "context": "          video_id: str,\n                         first_sentence: str,\n                         second_sentence: str) -> ",
  "context_lines": "                    yield self.text_to_instance(video_id, first_sentence, second_sentence)\n\n    @overrides\n    def text_to_instance(self,\n                         video_id: str,\n                         first_sentence: str,\n                         second_sentence: str) -> Instance:  # type: ignore\n        # pylint: disable=arguments-differ\n        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n",
  "slicing": "                         first_sentence: str,\n"
 },
 "720": {
  "name": "second_sentence",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/activitynet_captions_reader.py",
  "lineno": "78",
  "column": "25",
  "context": "    first_sentence: str,\n                         second_sentence: str) -> Instance:  # type: ignore\n        # pylint: disable=arguments-differ\n       ",
  "context_lines": "    @overrides\n    def text_to_instance(self,\n                         video_id: str,\n                         first_sentence: str,\n                         second_sentence: str) -> Instance:  # type: ignore\n        # pylint: disable=arguments-differ\n        tokenized_first_sentence = self._tokenizer.tokenize(first_sentence)\n        tokenized_second_sentence = self._tokenizer.tokenize(second_sentence)\n        first_sentence_field = TextField(tokenized_first_sentence, self._token_indexers)\n",
  "slicing": "                         second_sentence: str) -> Instance:  # type: ignore\n"
 },
 "721": {
  "name": "generations",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "23",
  "column": "30",
  "context": "rt PackedSequence\n\n\ndef _de_duplicate_generations(generations):\n    \"\"\"\n    Given a list of list of strings, filte",
  "context_lines": "from allennlp.nn.util import sequence_cross_entropy_with_logits\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nfrom torch.nn.utils.rnn import PackedSequence\n\n\ndef _de_duplicate_generations(generations):\n    \"\"\"\n    Given a list of list of strings, filter out the ones that are duplicates. and return an idx corresponding\n    to the good ones\n    :param generations:\n",
  "slicing": [
   "def _de_duplicate_generations(generations):\n",
   "        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n",
   "        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n"
  ]
 },
 "722": {
  "name": "input_size",
  "type": "int",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "71",
  "column": "17",
  "context": "\n    \"\"\"\n\n    def __init__(self,\n                 input_size: int,\n                 hidden_size: int,\n               ",
  "context_lines": "        element, all outputs past the sequence length for that batch are\n        zero tensors.\n    \"\"\"\n\n    def __init__(self,\n                 input_size: int,\n                 hidden_size: int,\n                 num_layers: int,\n                 recurrent_dropout_probability: float = 0.0,\n                 use_highway: bool = True,\n",
  "slicing": "                 input_size: int,\n"
 },
 "723": {
  "name": "hidden_size",
  "type": "int",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "72",
  "column": "17",
  "context": "                input_size: int,\n                 hidden_size: int,\n                 num_layers: int,\n                ",
  "context_lines": "        zero tensors.\n    \"\"\"\n\n    def __init__(self,\n                 input_size: int,\n                 hidden_size: int,\n                 num_layers: int,\n                 recurrent_dropout_probability: float = 0.0,\n                 use_highway: bool = True,\n                 use_input_projection_bias: bool = True,\n",
  "slicing": "                 hidden_size: int,\n"
 },
 "724": {
  "name": "num_layers",
  "type": "int",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "73",
  "column": "17",
  "context": "               hidden_size: int,\n                 num_layers: int,\n                 recurrent_dropout_probability: fl",
  "context_lines": "    \"\"\"\n\n    def __init__(self,\n                 input_size: int,\n                 hidden_size: int,\n                 num_layers: int,\n                 recurrent_dropout_probability: float = 0.0,\n                 use_highway: bool = True,\n                 use_input_projection_bias: bool = True,\n                 go_forward: bool = True) -> None:\n",
  "slicing": "                 num_layers: int,\n"
 },
 "725": {
  "name": "recurrent_dropout_probability",
  "type": "float",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "74",
  "column": "17",
  "context": "                num_layers: int,\n                 recurrent_dropout_probability: float = 0.0,\n                 use_highway: bool = True,\n       ",
  "context_lines": "    def __init__(self,\n                 input_size: int,\n                 hidden_size: int,\n                 num_layers: int,\n                 recurrent_dropout_probability: float = 0.0,\n                 use_highway: bool = True,\n                 use_input_projection_bias: bool = True,\n                 go_forward: bool = True) -> None:\n        super(StackedLstm, self).__init__()\n\n",
  "slicing": "                 recurrent_dropout_probability: float = 0.0,\n"
 },
 "726": {
  "name": "use_highway",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "75",
  "column": "17",
  "context": "ropout_probability: float = 0.0,\n                 use_highway: bool = True,\n                 use_input_projection_bias: bool =",
  "context_lines": "                 input_size: int,\n                 hidden_size: int,\n                 num_layers: int,\n                 recurrent_dropout_probability: float = 0.0,\n                 use_highway: bool = True,\n                 use_input_projection_bias: bool = True,\n                 go_forward: bool = True) -> None:\n        super(StackedLstm, self).__init__()\n\n        # Required to be wrapped with a :class:`PytorchSeq2SeqWrapper`.\n",
  "slicing": "                 use_highway: bool = True,\n"
 },
 "727": {
  "name": "use_input_projection_bias",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "76",
  "column": "17",
  "context": "       use_highway: bool = True,\n                 use_input_projection_bias: bool = True,\n                 go_forward: bool = True) -> None:",
  "context_lines": "                 hidden_size: int,\n                 num_layers: int,\n                 recurrent_dropout_probability: float = 0.0,\n                 use_highway: bool = True,\n                 use_input_projection_bias: bool = True,\n                 go_forward: bool = True) -> None:\n        super(StackedLstm, self).__init__()\n\n        # Required to be wrapped with a :class:`PytorchSeq2SeqWrapper`.\n        self.input_size = input_size\n",
  "slicing": "                 use_input_projection_bias: bool = True,\n"
 },
 "728": {
  "name": "go_forward",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "77",
  "column": "17",
  "context": "ut_projection_bias: bool = True,\n                 go_forward: bool = True) -> None:\n        super(StackedLstm, self).__init__()\n\n     ",
  "context_lines": "                 num_layers: int,\n                 recurrent_dropout_probability: float = 0.0,\n                 use_highway: bool = True,\n                 use_input_projection_bias: bool = True,\n                 go_forward: bool = True) -> None:\n        super(StackedLstm, self).__init__()\n\n        # Required to be wrapped with a :class:`PytorchSeq2SeqWrapper`.\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n",
  "slicing": "                 go_forward: bool = True) -> None:\n"
 },
 "729": {
  "name": "inputs",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "98",
  "column": "16",
  "context": " pylint: disable=arguments-differ\n                inputs: PackedSequence,\n                initial_state: Optional[Tuple[torc",
  "context_lines": "            self.add_module('layer_{}'.format(layer_index), layer)\n            layers.append(layer)\n        self.lstm_layers = layers\n\n    def forward(self,  # pylint: disable=arguments-differ\n                inputs: PackedSequence,\n                initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n        \"\"\"\n        Parameters\n        ----------\n",
  "slicing": "                inputs: PackedSequence,\n"
 },
 "730": {
  "name": "initial_state",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "99",
  "column": "16",
  "context": "          inputs: PackedSequence,\n                initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n        \"\"\"\n        Parameters\n        ----------\n",
  "context_lines": "            layers.append(layer)\n        self.lstm_layers = layers\n\n    def forward(self,  # pylint: disable=arguments-differ\n                inputs: PackedSequence,\n                initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n        \"\"\"\n        Parameters\n        ----------\n        inputs : ``PackedSequence``, required.\n",
  "slicing": "                initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n"
 },
 "731": {
  "name": "vocab",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "140",
  "column": "17",
  "context": ".Module):\n    def __init__(self,\n                 vocab: Vocabulary,\n                 recurrent_dropout_probability: fl",
  "context_lines": "        final_state_tuple = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))\n        return output_sequence, final_state_tuple\n\n\nclass SimpleBiLM(torch.nn.Module):\n    def __init__(self,\n                 vocab: Vocabulary,\n                 recurrent_dropout_probability: float = 0.0,\n                 embedding_dropout_probability: float = 0.0,\n                 input_size=512,\n                 hidden_size=512) -> None:\n",
  "slicing": "                 vocab: Vocabulary,\n"
 },
 "732": {
  "name": "recurrent_dropout_probability",
  "type": "float",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "141",
  "column": "17",
  "context": "              vocab: Vocabulary,\n                 recurrent_dropout_probability: float = 0.0,\n                 embedding_dropout_probability: fl",
  "context_lines": "        return output_sequence, final_state_tuple\n\n\nclass SimpleBiLM(torch.nn.Module):\n    def __init__(self,\n                 vocab: Vocabulary,\n                 recurrent_dropout_probability: float = 0.0,\n                 embedding_dropout_probability: float = 0.0,\n                 input_size=512,\n                 hidden_size=512) -> None:\n        \"\"\"\n",
  "slicing": "                 recurrent_dropout_probability: float = 0.0,\n"
 },
 "733": {
  "name": "embedding_dropout_probability",
  "type": "float",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "142",
  "column": "17",
  "context": "ropout_probability: float = 0.0,\n                 embedding_dropout_probability: float = 0.0,\n                 input_size=512,\n                 ",
  "context_lines": "class SimpleBiLM(torch.nn.Module):\n    def __init__(self,\n                 vocab: Vocabulary,\n                 recurrent_dropout_probability: float = 0.0,\n                 embedding_dropout_probability: float = 0.0,\n                 input_size=512,\n                 hidden_size=512) -> None:\n        \"\"\"\n        :param options_file: for initializing elmo BiLM\n",
  "slicing": "                 embedding_dropout_probability: float = 0.0,\n"
 },
 "734": {
  "name": "input_size",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "143",
  "column": "17",
  "context": "ropout_probability: float = 0.0,\n                 input_size=512,\n                 hidden_size=512) -> None:\n       ",
  "context_lines": "    def __init__(self,\n                 vocab: Vocabulary,\n                 recurrent_dropout_probability: float = 0.0,\n                 embedding_dropout_probability: float = 0.0,\n                 input_size=512,\n                 hidden_size=512) -> None:\n        \"\"\"\n        :param options_file: for initializing elmo BiLM\n        :param weight_file: for initializing elmo BiLM\n",
  "slicing": "                 input_size=512,\n"
 },
 "735": {
  "name": "hidden_size",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "144",
  "column": "17",
  "context": "                 input_size=512,\n                 hidden_size=512) -> None:\n        \"\"\"\n        :param options_file: for initi",
  "context_lines": "                 vocab: Vocabulary,\n                 recurrent_dropout_probability: float = 0.0,\n                 embedding_dropout_probability: float = 0.0,\n                 input_size=512,\n                 hidden_size=512) -> None:\n        \"\"\"\n        :param options_file: for initializing elmo BiLM\n        :param weight_file: for initializing elmo BiLM\n        :param requires_grad: Whether or not to finetune the LSTM layers\n",
  "slicing": "                 hidden_size=512) -> None:\n"
 },
 "736": {
  "name": "words",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "174",
  "column": "26",
  "context": "ng_dropout_probability\n\n    def embed_words(self, words):\n        # assert words.dim() == 2\n        return F",
  "context_lines": "        self.register_buffer('invalid_tokens', torch.LongTensor([vocab.get_token_index(tok) for tok in\n                                                                 ['@@UNKNOWN@@', '@@PADDING@@', '@@bos@@', '@@eos@@',\n                                                                  '@@NEWLINE@@']]))\n        self.embedding_dropout_probability = embedding_dropout_probability\n\n    def embed_words(self, words):\n        # assert words.dim() == 2\n        return F.embedding(words, self.decoder.weight)\n        # if not self.training:\n        #       return F.embedding(words, self.decoder.weight)\n",
  "slicing": "    def embed_words(self, words):\n"
 },
 "737": {
  "name": "timestep_tokenized",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "190",
  "column": "30",
  "context": "   # return embeds\n\n    def timestep_to_ids(self, timestep_tokenized: List[str]):\n        \"\"\" Just a single timestep (so dont add BO",
  "context_lines": "        # padding_idx = 0\n        # embeds = self.decoder._backend.Embedding.apply(words, mask * self.decoder.weight, padding_idx, None,\n        #                                                2, False, False)\n        # return embeds\n\n    def timestep_to_ids(self, timestep_tokenized: List[str]):\n        \"\"\" Just a single timestep (so dont add BOS or EOS\"\"\"\n        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n\n    def batch_to_ids(self, stories_tokenized: List[List[str]]):\n        \"\"\"\n",
  "slicing": "    def timestep_to_ids(self, timestep_tokenized: List[str]):\n"
 },
 "738": {
  "name": "stories_tokenized",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "194",
  "column": "27",
  "context": "_tokenized])[:, None]\n\n    def batch_to_ids(self, stories_tokenized: List[List[str]]):\n        \"\"\"\n        Simple wrapper around _elmo_ba",
  "context_lines": "        # return embeds\n\n    def timestep_to_ids(self, timestep_tokenized: List[str]):\n        \"\"\" Just a single timestep (so dont add BOS or EOS\"\"\"\n        return torch.tensor([self.vocab.get_token_index(x) for x in timestep_tokenized])[:, None]\n\n    def batch_to_ids(self, stories_tokenized: List[List[str]]):\n        \"\"\"\n        Simple wrapper around _elmo_batch_to_ids\n        :param batch: A list of tokenized sentences.\n        :return: A tensor of padded character ids.\n",
  "slicing": "    def batch_to_ids(self, stories_tokenized: List[List[str]]):\n"
 },
 "739": {
  "name": "context",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "209",
  "column": "37",
  "context": "eturn words\n\n    def conditional_generation(self, context: List[str], gt_completion: List[str],\n                               batch_size: int = 1",
  "context_lines": "            for story in stories_tokenized])\n        batch.index_instances(self.vocab)\n        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n        return words\n\n    def conditional_generation(self, context: List[str], gt_completion: List[str],\n                               batch_size: int = 128, max_gen_length: int = 25,\n                               same_length_as_gt: bool = False, first_is_gold: bool = False):\n        \"\"\"\n        Generate conditoned on the context. While we're at it we'll score the GT going forwards\n",
  "slicing": "    def conditional_generation(self, context: List[str], gt_completion: List[str],\n"
 },
 "740": {
  "name": "gt_completion",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "209",
  "column": "57",
  "context": " conditional_generation(self, context: List[str], gt_completion: List[str],\n                               batch_size: int = 1",
  "context_lines": "            for story in stories_tokenized])\n        batch.index_instances(self.vocab)\n        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n        return words\n\n    def conditional_generation(self, context: List[str], gt_completion: List[str],\n                               batch_size: int = 128, max_gen_length: int = 25,\n                               same_length_as_gt: bool = False, first_is_gold: bool = False):\n        \"\"\"\n        Generate conditoned on the context. While we're at it we'll score the GT going forwards\n",
  "slicing": "    def conditional_generation(self, context: List[str], gt_completion: List[str],\n"
 },
 "741": {
  "name": "batch_size",
  "type": "int",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "210",
  "column": "31",
  "context": "letion: List[str],\n                               batch_size: int = 128, max_gen_length: int = 25,\n                               same_length_as_gt: ",
  "context_lines": "        batch.index_instances(self.vocab)\n        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n        return words\n\n    def conditional_generation(self, context: List[str], gt_completion: List[str],\n                               batch_size: int = 128, max_gen_length: int = 25,\n                               same_length_as_gt: bool = False, first_is_gold: bool = False):\n        \"\"\"\n        Generate conditoned on the context. While we're at it we'll score the GT going forwards\n        :param context: List of tokens to condition on. We'll add the BOS marker to it\n",
  "slicing": "                               batch_size: int = 128, max_gen_length: int = 25,\n"
 },
 "742": {
  "name": "max_gen_length",
  "type": "int",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "210",
  "column": "54",
  "context": "                           batch_size: int = 128, max_gen_length: int = 25,\n                               same_length_as_gt: ",
  "context_lines": "        batch.index_instances(self.vocab)\n        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n        return words\n\n    def conditional_generation(self, context: List[str], gt_completion: List[str],\n                               batch_size: int = 128, max_gen_length: int = 25,\n                               same_length_as_gt: bool = False, first_is_gold: bool = False):\n        \"\"\"\n        Generate conditoned on the context. While we're at it we'll score the GT going forwards\n        :param context: List of tokens to condition on. We'll add the BOS marker to it\n",
  "slicing": "                               batch_size: int = 128, max_gen_length: int = 25,\n"
 },
 "743": {
  "name": "same_length_as_gt",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "211",
  "column": "31",
  "context": "_length: int = 25,\n                               same_length_as_gt: bool = False, first_is_gold: bool = False):\n        \"\"\"\n        Generate conditoned on the con",
  "context_lines": "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n        return words\n\n    def conditional_generation(self, context: List[str], gt_completion: List[str],\n                               batch_size: int = 128, max_gen_length: int = 25,\n                               same_length_as_gt: bool = False, first_is_gold: bool = False):\n        \"\"\"\n        Generate conditoned on the context. While we're at it we'll score the GT going forwards\n        :param context: List of tokens to condition on. We'll add the BOS marker to it\n        :param gt_completion: The gold truth completion\n",
  "slicing": "                               same_length_as_gt: bool = False, first_is_gold: bool = False):\n"
 },
 "744": {
  "name": "first_is_gold",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "211",
  "column": "64",
  "context": "                 same_length_as_gt: bool = False, first_is_gold: bool = False):\n        \"\"\"\n        Generate conditoned on the con",
  "context_lines": "        words = {k: v['tokens'] for k, v in batch.as_tensor_dict().items()}['story']\n        return words\n\n    def conditional_generation(self, context: List[str], gt_completion: List[str],\n                               batch_size: int = 128, max_gen_length: int = 25,\n                               same_length_as_gt: bool = False, first_is_gold: bool = False):\n        \"\"\"\n        Generate conditoned on the context. While we're at it we'll score the GT going forwards\n        :param context: List of tokens to condition on. We'll add the BOS marker to it\n        :param gt_completion: The gold truth completion\n",
  "slicing": "                               same_length_as_gt: bool = False, first_is_gold: bool = False):\n"
 },
 "745": {
  "name": "activation",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "280",
  "column": "36",
  "context": "ch().numpy()\n\n    def _chunked_logsoftmaxes(self, activation, word_targets, chunk_size=256):\n        \"\"\"\n        do the softmax in chunks so me",
  "context_lines": "            for j, (_, v) in enumerate(gen[1:]):\n                generation_scores[i, j] = v\n\n        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n\n    def _chunked_logsoftmaxes(self, activation, word_targets, chunk_size=256):\n        \"\"\"\n        do the softmax in chunks so memory doesnt explode\n        :param activation: [batch, T, dim]\n        :param targets: [batch, T] indices\n",
  "slicing": "    def _chunked_logsoftmaxes(self, activation, word_targets, chunk_size=256):\n"
 },
 "746": {
  "name": "word_targets",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "280",
  "column": "48",
  "context": "\n\n    def _chunked_logsoftmaxes(self, activation, word_targets, chunk_size=256):\n        \"\"\"\n        do the softmax in chunks so me",
  "context_lines": "            for j, (_, v) in enumerate(gen[1:]):\n                generation_scores[i, j] = v\n\n        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n\n    def _chunked_logsoftmaxes(self, activation, word_targets, chunk_size=256):\n        \"\"\"\n        do the softmax in chunks so memory doesnt explode\n        :param activation: [batch, T, dim]\n        :param targets: [batch, T] indices\n",
  "slicing": "    def _chunked_logsoftmaxes(self, activation, word_targets, chunk_size=256):\n"
 },
 "747": {
  "name": "chunk_size",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "280",
  "column": "62",
  "context": "nked_logsoftmaxes(self, activation, word_targets, chunk_size=256):\n        \"\"\"\n        do the softmax in chunks so me",
  "context_lines": "            for j, (_, v) in enumerate(gen[1:]):\n                generation_scores[i, j] = v\n\n        generation_toks, idx = _de_duplicate_generations([[tok for (tok, score) in gen[1:]] for gen in generations])\n        return generation_toks, generation_scores[idx], forward_logprobs.cpu().detach().numpy()\n\n    def _chunked_logsoftmaxes(self, activation, word_targets, chunk_size=256):\n        \"\"\"\n        do the softmax in chunks so memory doesnt explode\n        :param activation: [batch, T, dim]\n        :param targets: [batch, T] indices\n",
  "slicing": "    def _chunked_logsoftmaxes(self, activation, word_targets, chunk_size=256):\n"
 },
 "748": {
  "name": "words",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "302",
  "column": "22",
  "context": "torch.cat(all_logprobs, 0)\n\n    def forward(self, words: torch.Tensor, use_forward: bool = True, use_reverse: bool = True,\n                compute_logprobs: bool = False) ->",
  "context_lines": "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n        return torch.cat(all_logprobs, 0)\n\n    def forward(self, words: torch.Tensor, use_forward: bool = True, use_reverse: bool = True,\n                compute_logprobs: bool = False) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:\n        \"\"\"\n        use this for training the LM\n        :param words: [batch_size, N] words. assuming you're starting with BOS and ending with EOS here\n",
  "slicing": "    def forward(self, words: torch.Tensor, use_forward: bool = True, use_reverse: bool = True,\n"
 },
 "749": {
  "name": "use_forward",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "302",
  "column": "43",
  "context": "s, 0)\n\n    def forward(self, words: torch.Tensor, use_forward: bool = True, use_reverse: bool = True,\n                compute_logprobs: bool = False) ->",
  "context_lines": "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n        return torch.cat(all_logprobs, 0)\n\n    def forward(self, words: torch.Tensor, use_forward: bool = True, use_reverse: bool = True,\n                compute_logprobs: bool = False) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:\n        \"\"\"\n        use this for training the LM\n        :param words: [batch_size, N] words. assuming you're starting with BOS and ending with EOS here\n",
  "slicing": "    def forward(self, words: torch.Tensor, use_forward: bool = True, use_reverse: bool = True,\n"
 },
 "750": {
  "name": "use_reverse",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "302",
  "column": "69",
  "context": "f, words: torch.Tensor, use_forward: bool = True, use_reverse: bool = True,\n                compute_logprobs: bool = False) ->",
  "context_lines": "                                         out=target_chunk.data.new(targets_flat.size(0))) / target_chunk.size(1)\n            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n        return torch.cat(all_logprobs, 0)\n\n    def forward(self, words: torch.Tensor, use_forward: bool = True, use_reverse: bool = True,\n                compute_logprobs: bool = False) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:\n        \"\"\"\n        use this for training the LM\n        :param words: [batch_size, N] words. assuming you're starting with BOS and ending with EOS here\n",
  "slicing": "    def forward(self, words: torch.Tensor, use_forward: bool = True, use_reverse: bool = True,\n"
 },
 "751": {
  "name": "compute_logprobs",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/simple_bilm.py",
  "lineno": "303",
  "column": "16",
  "context": "= True, use_reverse: bool = True,\n                compute_logprobs: bool = False) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:\n        \"\"\"\n        use this for training the LM\n ",
  "context_lines": "            all_logprobs.append(F.log_softmax(self.decoder(activation_chunk), 2)[\n                                    batch_indexer, time_indexer, targets_flat].view(*target_chunk.size()))\n        return torch.cat(all_logprobs, 0)\n\n    def forward(self, words: torch.Tensor, use_forward: bool = True, use_reverse: bool = True,\n                compute_logprobs: bool = False) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:\n        \"\"\"\n        use this for training the LM\n        :param words: [batch_size, N] words. assuming you're starting with BOS and ending with EOS here\n        :return:\n",
  "slicing": "                compute_logprobs: bool = False) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:\n"
 },
 "752": {
  "name": "default_seeds",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "51",
  "column": "17",
  "context": "\n\n    \"\"\"\n    def __init__(self,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n        sup",
  "context_lines": "        of the\n        `ActivityNet Captions dataset <https://cs.stanford.edu/people/ranjaykrishna/densevid/>`_.\n\n    \"\"\"\n    def __init__(self,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n        super().__init__(default_seeds, quiet)\n\n        lm_files = download_files(fnames=['vocabulary.zip',\n                                          'lm-fold-0.bin'],\n",
  "slicing": "                 default_seeds: Iterable = None,\n"
 },
 "753": {
  "name": "quiet",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "52",
  "column": "17",
  "context": " default_seeds: Iterable = None,\n                 quiet: bool = False):\n        super().__init__(default_seeds, quiet)\n\n  ",
  "context_lines": "        `ActivityNet Captions dataset <https://cs.stanford.edu/people/ranjaykrishna/densevid/>`_.\n\n    \"\"\"\n    def __init__(self,\n                 default_seeds: Iterable = None,\n                 quiet: bool = False):\n        super().__init__(default_seeds, quiet)\n\n        lm_files = download_files(fnames=['vocabulary.zip',\n                                          'lm-fold-0.bin'],\n                                  local_folder='swag_lm')\n\n",
  "slicing": "                 quiet: bool = False):\n"
 },
 "754": {
  "name": "tree",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "77",
  "column": "23",
  "context": "ult_seeds = default_seeds\n\n    def _find_VP(self, tree: JsonDict) -> List[Tuple[str, any]]:\n        r\"\"\"Recurse on a constituency parse tree u",
  "context_lines": "        if default_seeds is None:\n            self.default_seeds = ActivityNetCaptionsDatasetReader().read(activity_data_files[0] + '/train.json')\n        else:\n            self.default_seeds = default_seeds\n\n    def _find_VP(self, tree: JsonDict) -> List[Tuple[str, any]]:\n        r\"\"\"Recurse on a constituency parse tree until we find verb phrases\"\"\"\n\n        # Recursion is annoying because we need to check whether each is a list or not\n        def _recurse_on_children():\n            assert 'children' in tree\n",
  "slicing": "    def _find_VP(self, tree: JsonDict) -> List[Tuple[str, any]]:\n"
 },
 "755": {
  "name": "sentence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "112",
  "column": "33",
  "context": "e_on_children()\n\n    def _split_on_final_vp(self, sentence: Instance) -> (List[str], List[str]):\n        r\"\"\"Splits a sentence on the final verb ph",
  "context_lines": "        if len(tree['children']) == 1:\n            return _recurse_on_children()\n        # try recursing on everything\n        return _recurse_on_children()\n\n    def _split_on_final_vp(self, sentence: Instance) -> (List[str], List[str]):\n        r\"\"\"Splits a sentence on the final verb phrase \"\"\"\n        sentence_txt = ' '.join(t.text for t in sentence.tokens)\n        res = self.const_parser.predict(sentence_txt)\n        res_chunked = self._find_VP(res['hierplane_tree']['root'])\n",
  "slicing": "    def _split_on_final_vp(self, sentence: Instance) -> (List[str], List[str]):\n"
 },
 "756": {
  "name": "seed",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/swag_generator.py",
  "lineno": "125",
  "column": "33",
  "context": "n not_vp, is_vp\n\n    def generate_from_seed(self, seed: Tuple):\n        \"\"\"Edit a seed example.\n        \"\"\"\n      ",
  "context_lines": "        vp_ind = max(is_vp)\n        not_vp = [token for x in res_chunked[:vp_ind] for token in x[0].split(' ')]\n        is_vp = [token for x in res_chunked[vp_ind:] for token in x[0].split(' ')]\n        return not_vp, is_vp\n\n    def generate_from_seed(self, seed: Tuple):\n        \"\"\"Edit a seed example.\n        \"\"\"\n        first_sentence: TextField = seed.fields[\"first_sentence\"]\n        second_sentence: TextField = seed.fields[\"second_sentence\"]\n",
  "slicing": "    def generate_from_seed(self, seed: Tuple):\n"
 },
 "757": {
  "name": "vocab",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "28",
  "column": "17",
  "context": ".\n    \"\"\"\n    def __init__(self,\n                 vocab: Vocabulary,\n                 openai_token_embedder: OpenaiTran",
  "context_lines": "        begin-sentence and end-sentence tokens. If this flag is True\n        the corresponding embeddings will be removed from the return values.\n    \"\"\"\n    def __init__(self,\n                 vocab: Vocabulary,\n                 openai_token_embedder: OpenaiTransformerEmbedder,\n                 remove_bos_eos: bool = True) -> None:\n        super().__init__(vocab)\n        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n",
  "slicing": "                 vocab: Vocabulary,\n"
 },
 "758": {
  "name": "openai_token_embedder",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "29",
  "column": "17",
  "context": "              vocab: Vocabulary,\n                 openai_token_embedder: OpenaiTransformerEmbedder,\n                 remove_bos_eos: bool = True) -> N",
  "context_lines": "        the corresponding embeddings will be removed from the return values.\n    \"\"\"\n    def __init__(self,\n                 vocab: Vocabulary,\n                 openai_token_embedder: OpenaiTransformerEmbedder,\n                 remove_bos_eos: bool = True) -> None:\n        super().__init__(vocab)\n        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n",
  "slicing": "                 openai_token_embedder: OpenaiTransformerEmbedder,\n"
 },
 "759": {
  "name": "remove_bos_eos",
  "type": "bool",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "30",
  "column": "17",
  "context": "dder: OpenaiTransformerEmbedder,\n                 remove_bos_eos: bool = True) -> None:\n        super().__init__(vocab)\n        model_path",
  "context_lines": "    \"\"\"\n    def __init__(self,\n                 vocab: Vocabulary,\n                 openai_token_embedder: OpenaiTransformerEmbedder,\n                 remove_bos_eos: bool = True) -> None:\n        super().__init__(vocab)\n        model_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/openai-transformer-lm-2018.07.23.tar.gz\"\n        indexer = OpenaiTransformerBytePairIndexer(model_path=model_path)\n        transformer = OpenaiTransformer(model_path=model_path)\n",
  "slicing": "                 remove_bos_eos: bool = True) -> None:\n"
 },
 "760": {
  "name": "token_embeddings",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "39",
  "column": "36",
  "context": "bedding(self,\n                                    token_embeddings: torch.Tensor,\n                                    mask: torch.Te",
  "context_lines": "        transformer = OpenaiTransformer(model_path=model_path)\n        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n        self._remove_bos_eos = remove_bos_eos\n\n    def _get_target_token_embedding(self,\n                                    token_embeddings: torch.Tensor,\n                                    mask: torch.Tensor,\n                                    direction: int) -> torch.Tensor:\n        # Need to shift the mask in the correct direction\n        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n",
  "slicing": "                                    token_embeddings: torch.Tensor,\n"
 },
 "761": {
  "name": "mask",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "40",
  "column": "36",
  "context": "torch.Tensor,\n                                    mask: torch.Tensor,\n                                    direction: int",
  "context_lines": "        self._token_embedders = OpenaiTransformerEmbedder(transformer=transformer, top_layer_only=True)\n        self._remove_bos_eos = remove_bos_eos\n\n    def _get_target_token_embedding(self,\n                                    token_embeddings: torch.Tensor,\n                                    mask: torch.Tensor,\n                                    direction: int) -> torch.Tensor:\n        # Need to shift the mask in the correct direction\n        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n        if direction == 0:\n",
  "slicing": "                                    mask: torch.Tensor,\n"
 },
 "762": {
  "name": "direction",
  "type": "int",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "41",
  "column": "36",
  "context": "torch.Tensor,\n                                    direction: int) -> torch.Tensor:\n        # Need to shift the mask in the correct di",
  "context_lines": "        self._remove_bos_eos = remove_bos_eos\n\n    def _get_target_token_embedding(self,\n                                    token_embeddings: torch.Tensor,\n                                    mask: torch.Tensor,\n                                    direction: int) -> torch.Tensor:\n        # Need to shift the mask in the correct direction\n        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()\n        if direction == 0:\n            # forward direction, get token to right\n",
  "slicing": "                                    direction: int) -> torch.Tensor:\n"
 },
 "763": {
  "name": "lm_embeddings",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "52",
  "column": "22",
  "context": "    def _compute_loss(self,\n                      lm_embeddings: torch.Tensor,\n                      token_embeddings: torch.Tens",
  "context_lines": "        else:\n            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n\n    def _compute_loss(self,\n                      lm_embeddings: torch.Tensor,\n                      token_embeddings: torch.Tensor,\n                      forward_targets: torch.Tensor,\n                      backward_targets: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        # lm_embeddings is shape (batch_size, timesteps, dim * 2)\n",
  "slicing": "                      lm_embeddings: torch.Tensor,\n"
 },
 "764": {
  "name": "token_embeddings",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "53",
  "column": "22",
  "context": "m_embeddings: torch.Tensor,\n                      token_embeddings: torch.Tensor,\n                      forward_targets: torch.Tenso",
  "context_lines": "            shifted_mask = torch.cat([mask[:, 1:], zero_col], dim=1)\n        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n\n    def _compute_loss(self,\n                      lm_embeddings: torch.Tensor,\n                      token_embeddings: torch.Tensor,\n                      forward_targets: torch.Tensor,\n                      backward_targets: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        # lm_embeddings is shape (batch_size, timesteps, dim * 2)\n        # forward_targets, backward_targets are shape (batch_size, timesteps)\n",
  "slicing": "                      token_embeddings: torch.Tensor,\n"
 },
 "765": {
  "name": "forward_targets",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "54",
  "column": "22",
  "context": "n_embeddings: torch.Tensor,\n                      forward_targets: torch.Tensor,\n                      backward_targets: torch.Tens",
  "context_lines": "        return token_embeddings.masked_select(shifted_mask.unsqueeze(-1)).view(-1, self._forward_dim)\n\n    def _compute_loss(self,\n                      lm_embeddings: torch.Tensor,\n                      token_embeddings: torch.Tensor,\n                      forward_targets: torch.Tensor,\n                      backward_targets: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        # lm_embeddings is shape (batch_size, timesteps, dim * 2)\n        # forward_targets, backward_targets are shape (batch_size, timesteps)\n        # masked with 0\n",
  "slicing": "                      forward_targets: torch.Tensor,\n"
 },
 "766": {
  "name": "backward_targets",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "55",
  "column": "22",
  "context": "ward_targets: torch.Tensor,\n                      backward_targets: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        # lm_embeddings is shape (batch_size, time",
  "context_lines": "    def _compute_loss(self,\n                      lm_embeddings: torch.Tensor,\n                      token_embeddings: torch.Tensor,\n                      forward_targets: torch.Tensor,\n                      backward_targets: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        # lm_embeddings is shape (batch_size, timesteps, dim * 2)\n        # forward_targets, backward_targets are shape (batch_size, timesteps)\n        # masked with 0\n        forward_embeddings, backward_embeddings = lm_embeddings.chunk(2, -1)\n",
  "slicing": "                      backward_targets: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n"
 },
 "767": {
  "name": "source",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/openai_transformer_model.py",
  "lineno": "90",
  "column": "16",
  "context": "def forward(self,  # type: ignore\n                source: Dict[str, torch.LongTensor]) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Computes the averaged forward ",
  "context_lines": "                                            non_masked_targets,\n                                            non_masked_token_embedding))\n\n        return losses[0], losses[1]\n\n    def forward(self,  # type: ignore\n                source: Dict[str, torch.LongTensor]) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Computes the averaged forward and backward LM loss from the batch.\n\n        By convention, the input dict is required to have at least a ``\"tokens\"``\n        entry that's the output of a ``SingleIdTokenIndexer``, which is used\n",
  "slicing": "                source: Dict[str, torch.LongTensor]) -> Dict[str, torch.Tensor]:\n"
 },
 "768": {
  "name": "network",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "6",
  "column": "23",
  "context": "um2words import num2words\n\ndef optimistic_restore(network, state_dict):\n    mismatch = False\n    own_state = network.state",
  "context_lines": "import re\nfrom itertools import tee\n\nfrom num2words import num2words\n\ndef optimistic_restore(network, state_dict):\n    mismatch = False\n    own_state = network.state_dict()\n    for name, param in state_dict.items():\n        if name not in own_state:\n",
  "slicing": "def optimistic_restore(network, state_dict):\n"
 },
 "769": {
  "name": "state_dict",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "6",
  "column": "32",
  "context": "import num2words\n\ndef optimistic_restore(network, state_dict):\n    mismatch = False\n    own_state = network.state",
  "context_lines": "import re\nfrom itertools import tee\n\nfrom num2words import num2words\n\ndef optimistic_restore(network, state_dict):\n    mismatch = False\n    own_state = network.state_dict()\n    for name, param in state_dict.items():\n        if name not in own_state:\n",
  "slicing": "def optimistic_restore(network, state_dict):\n"
 },
 "770": {
  "name": "iterable",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "27",
  "column": "13",
  "context": "atch = True\n    return not mismatch\n\ndef pairwise(iterable):\n    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n    a, ",
  "context_lines": "    if len(missing) > 0:\n        print(\"We couldn't find {}\".format(','.join(missing)))\n        mismatch = True\n    return not mismatch\n\ndef pairwise(iterable):\n    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\n",
  "slicing": "def pairwise(iterable):\n"
 },
 "771": {
  "name": "num",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "33",
  "column": "11",
  "context": "   next(b, None)\n    return zip(a, b)\n\ndef n2w_1k(num, use_ordinal=False):\n    if num > 1000:\n        return ''\n    return nu",
  "context_lines": "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\ndef n2w_1k(num, use_ordinal=False):\n    if num > 1000:\n        return ''\n    return num2words(num, to='ordinal' if use_ordinal else 'cardinal')\n\ndef postprocess(sentence):\n",
  "slicing": [
   "def n2w_1k(num, use_ordinal=False):\n",
   "    return num2words(num, to='ordinal' if use_ordinal else 'cardinal')\n"
  ]
 },
 "772": {
  "name": "use_ordinal",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "33",
  "column": "16",
  "context": "xt(b, None)\n    return zip(a, b)\n\ndef n2w_1k(num, use_ordinal=False):\n    if num > 1000:\n        return ''\n    return nu",
  "context_lines": "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\ndef n2w_1k(num, use_ordinal=False):\n    if num > 1000:\n        return ''\n    return num2words(num, to='ordinal' if use_ordinal else 'cardinal')\n\ndef postprocess(sentence):\n",
  "slicing": [
   "def n2w_1k(num, use_ordinal=False):\n",
   "                   lambda x: n2w_1k(int(x.group(0)[:-2]), use_ordinal=True), sent2)\n"
  ]
 },
 "773": {
  "name": "sentence",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "38",
  "column": "16",
  "context": " if use_ordinal else 'cardinal')\n\ndef postprocess(sentence):\n    \"\"\"\n    make sure punctuation is followed by a",
  "context_lines": "def n2w_1k(num, use_ordinal=False):\n    if num > 1000:\n        return ''\n    return num2words(num, to='ordinal' if use_ordinal else 'cardinal')\n\ndef postprocess(sentence):\n    \"\"\"\n    make sure punctuation is followed by a space\n    :param sentence:\n    :return:\n",
  "slicing": "def postprocess(sentence):\n"
 },
 "774": {
  "name": "sent",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "69",
  "column": "19",
  "context": "ent6.strip()\n    return sent7\n\ndef remove_allcaps(sent):\n    \"\"\"\n    Given a sentence, filter it so that it",
  "context_lines": "    # Several spaces\n    sent6 = re.sub(r'\\s\\s+', ' ', sent5)\n\n    sent7 = sent6.strip()\n    return sent7\n\ndef remove_allcaps(sent):\n    \"\"\"\n    Given a sentence, filter it so that it doesn't contain some words that are ALLcaps\n    :param sent: string, like SOMEONE wheels SOMEONE on, mouthing silent words of earnest prayer.\n    :return:                  Someone wheels someone on, mouthing silent words of earnest prayer.\n",
  "slicing": "def remove_allcaps(sent):\n"
 },
 "775": {
  "name": "word",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "76",
  "column": "18",
  "context": ".\n    \"\"\"\n    # Remove all caps\n    def _sanitize(word, is_first):\n        if word == \"I\":\n            return word\n  ",
  "context_lines": "    :param sent: string, like SOMEONE wheels SOMEONE on, mouthing silent words of earnest prayer.\n    :return:                  Someone wheels someone on, mouthing silent words of earnest prayer.\n    \"\"\"\n    # Remove all caps\n    def _sanitize(word, is_first):\n        if word == \"I\":\n            return word\n        num_capitals = len([x for x in word if not x.islower()])\n        if num_capitals > len(word) // 2:\n",
  "slicing": [
   "    def _sanitize(word, is_first):\n",
   "    return ' '.join([_sanitize(word, i == 0) for i, word in enumerate(sent.split(' '))])\n"
  ]
 },
 "776": {
  "name": "is_first",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/generators/swag/utils.py",
  "lineno": "76",
  "column": "24",
  "context": "\"\"\"\n    # Remove all caps\n    def _sanitize(word, is_first):\n        if word == \"I\":\n            return word\n  ",
  "context_lines": "    :param sent: string, like SOMEONE wheels SOMEONE on, mouthing silent words of earnest prayer.\n    :return:                  Someone wheels someone on, mouthing silent words of earnest prayer.\n    \"\"\"\n    # Remove all caps\n    def _sanitize(word, is_first):\n        if word == \"I\":\n            return word\n        num_capitals = len([x for x in word if not x.islower()])\n        if num_capitals > len(word) // 2:\n",
  "slicing": [
   "    def _sanitize(word, is_first):\n",
   "            if is_first:\n"
  ]
 },
 "777": {
  "name": "prog",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/commands/__init__.py",
  "lineno": "14",
  "column": "9",
  "context": "ame__)  # pylint: disable=invalid-name\n\n\ndef main(prog: str = None,\n         subcommand_overrides: Dict[str, Subcomman",
  "context_lines": "from allennlp.common.util import import_submodules\n\nfrom adversarialnlp import __version__\nfrom adversarialnlp.commands.test_install import TestInstall\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef main(prog: str = None,\n         subcommand_overrides: Dict[str, Subcommand] = {}) -> None:\n    \"\"\"\n    :mod:`~adversarialnlp.run` command.\n    \"\"\"\n",
  "slicing": "def main(prog: str = None,\n"
 },
 "778": {
  "name": "subcommand_overrides",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/commands/__init__.py",
  "lineno": "15",
  "column": "9",
  "context": "nvalid-name\n\n\ndef main(prog: str = None,\n         subcommand_overrides: Dict[str, Subcommand] = {}) -> None:\n    \"\"\"\n    :mod:`~adversarialnlp.run` command.\n  ",
  "context_lines": "from adversarialnlp import __version__\nfrom adversarialnlp.commands.test_install import TestInstall\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef main(prog: str = None,\n         subcommand_overrides: Dict[str, Subcommand] = {}) -> None:\n    \"\"\"\n    :mod:`~adversarialnlp.run` command.\n    \"\"\"\n    # pylint: disable=dangerous-default-value\n",
  "slicing": "         subcommand_overrides: Dict[str, Subcommand] = {}) -> None:\n"
 },
 "779": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/commands/test_install.py",
  "lineno": "31",
  "column": "28",
  "context": "tInstall(Subcommand):\n    def add_subparser(self, name: str, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        # pylint: disable=protected-access\n       ",
  "context_lines": "from allennlp.commands.subcommand import Subcommand\n\nimport adversarialnlp\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nclass TestInstall(Subcommand):\n    def add_subparser(self, name: str, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        # pylint: disable=protected-access\n        description = '''Test that installation works by running the unit tests.'''\n        subparser = parser.add_parser(\n                name, description=description, help='Run the unit tests.')\n\n",
  "slicing": "    def add_subparser(self, name: str, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n"
 },
 "780": {
  "name": "parser",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/test_install.py",
  "lineno": "31",
  "column": "39",
  "context": "bcommand):\n    def add_subparser(self, name: str, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        # pylint: disable=protected-access\n       ",
  "context_lines": "from allennlp.commands.subcommand import Subcommand\n\nimport adversarialnlp\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nclass TestInstall(Subcommand):\n    def add_subparser(self, name: str, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        # pylint: disable=protected-access\n        description = '''Test that installation works by running the unit tests.'''\n        subparser = parser.add_parser(\n                name, description=description, help='Run the unit tests.')\n\n",
  "slicing": "    def add_subparser(self, name: str, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n"
 },
 "781": {
  "name": "args",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/commands/test_install.py",
  "lineno": "50",
  "column": "14",
  "context": "h(adversarialnlp.__file__).parent\n\n\ndef _run_test(args: argparse.Namespace):\n    initial_working_dir = os.getcwd()\n    module_p",
  "context_lines": "        subparser.set_defaults(func=_run_test)\n\n        return subparser\n\n\ndef _get_module_root():\n    return pathlib.Path(adversarialnlp.__file__).parent\n\n\ndef _run_test(args: argparse.Namespace):\n    initial_working_dir = os.getcwd()\n    module_parent = _get_module_root().parent\n    logger.info(\"Changing directory to %s\", module_parent)\n    os.chdir(module_parent)\n",
  "slicing": "def _run_test(args: argparse.Namespace):\n"
 },
 "782": {
  "name": "throttle",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "28",
  "column": "23",
  "context": "n human readable form.\"\"\"\n\n    def __init__(self, throttle=1, should_humanize=True):\n        \"\"\"Initialize Progress logger.\n        :pa",
  "context_lines": "PACKAGE_ROOT = MODULE_ROOT.parent\nDATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n\nclass ProgressLogger(object):\n    \"\"\"Throttles and display progress in human readable form.\"\"\"\n\n    def __init__(self, throttle=1, should_humanize=True):\n        \"\"\"Initialize Progress logger.\n        :param throttle: default 1, number in seconds to use as throttle rate\n        :param should_humanize: default True, whether to humanize data units\n        \"\"\"\n",
  "slicing": "    def __init__(self, throttle=1, should_humanize=True):\n"
 },
 "783": {
  "name": "should_humanize",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "28",
  "column": "35",
  "context": "able form.\"\"\"\n\n    def __init__(self, throttle=1, should_humanize=True):\n        \"\"\"Initialize Progress logger.\n        :pa",
  "context_lines": "PACKAGE_ROOT = MODULE_ROOT.parent\nDATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n\nclass ProgressLogger(object):\n    \"\"\"Throttles and display progress in human readable form.\"\"\"\n\n    def __init__(self, throttle=1, should_humanize=True):\n        \"\"\"Initialize Progress logger.\n        :param throttle: default 1, number in seconds to use as throttle rate\n        :param should_humanize: default True, whether to humanize data units\n        \"\"\"\n",
  "slicing": "    def __init__(self, throttle=1, should_humanize=True):\n"
 },
 "784": {
  "name": "num",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "37",
  "column": "23",
  "context": "umanize = should_humanize\n\n    def humanize(self, num, suffix='B'):\n        \"\"\"Convert units to more human-readable fo",
  "context_lines": "        \"\"\"\n        self.latest = time.time()\n        self.throttle_speed = throttle\n        self.should_humanize = should_humanize\n\n    def humanize(self, num, suffix='B'):\n        \"\"\"Convert units to more human-readable format.\"\"\"\n        if num < 0:\n            return num\n        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
  "slicing": "    def humanize(self, num, suffix='B'):\n"
 },
 "785": {
  "name": "suffix",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "37",
  "column": "28",
  "context": "ze = should_humanize\n\n    def humanize(self, num, suffix='B'):\n        \"\"\"Convert units to more human-readable fo",
  "context_lines": "        \"\"\"\n        self.latest = time.time()\n        self.throttle_speed = throttle\n        self.should_humanize = should_humanize\n\n    def humanize(self, num, suffix='B'):\n        \"\"\"Convert units to more human-readable format.\"\"\"\n        if num < 0:\n            return num\n        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
  "slicing": "    def humanize(self, num, suffix='B'):\n"
 },
 "786": {
  "name": "curr",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "47",
  "column": "18",
  "context": ".1f%s%s\" % (num, 'Yi', suffix)\n\n    def log(self, curr, total, width=40, force=False):\n        \"\"\"Display a bar showing the current progr",
  "context_lines": "            if abs(num) < 1024.0:\n                return \"%3.1f%s%s\" % (num, unit, suffix)\n            num /= 1024.0\n        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n\n    def log(self, curr, total, width=40, force=False):\n        \"\"\"Display a bar showing the current progress.\"\"\"\n        if curr == 0 and total == -1:\n            print('[ no data received for this file ]', end='\\r')\n            return\n",
  "slicing": "    def log(self, curr, total, width=40, force=False):\n"
 },
 "787": {
  "name": "total",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "47",
  "column": "24",
  "context": "s\" % (num, 'Yi', suffix)\n\n    def log(self, curr, total, width=40, force=False):\n        \"\"\"Display a bar showing the current progr",
  "context_lines": "            if abs(num) < 1024.0:\n                return \"%3.1f%s%s\" % (num, unit, suffix)\n            num /= 1024.0\n        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n\n    def log(self, curr, total, width=40, force=False):\n        \"\"\"Display a bar showing the current progress.\"\"\"\n        if curr == 0 and total == -1:\n            print('[ no data received for this file ]', end='\\r')\n            return\n",
  "slicing": "    def log(self, curr, total, width=40, force=False):\n"
 },
 "788": {
  "name": "width",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "47",
  "column": "31",
  "context": "um, 'Yi', suffix)\n\n    def log(self, curr, total, width=40, force=False):\n        \"\"\"Display a bar showing the current progr",
  "context_lines": "            if abs(num) < 1024.0:\n                return \"%3.1f%s%s\" % (num, unit, suffix)\n            num /= 1024.0\n        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n\n    def log(self, curr, total, width=40, force=False):\n        \"\"\"Display a bar showing the current progress.\"\"\"\n        if curr == 0 and total == -1:\n            print('[ no data received for this file ]', end='\\r')\n            return\n",
  "slicing": "    def log(self, curr, total, width=40, force=False):\n"
 },
 "789": {
  "name": "force",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "47",
  "column": "41",
  "context": "suffix)\n\n    def log(self, curr, total, width=40, force=False):\n        \"\"\"Display a bar showing the current progr",
  "context_lines": "            if abs(num) < 1024.0:\n                return \"%3.1f%s%s\" % (num, unit, suffix)\n            num /= 1024.0\n        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n\n    def log(self, curr, total, width=40, force=False):\n        \"\"\"Display a bar showing the current progress.\"\"\"\n        if curr == 0 and total == -1:\n            print('[ no data received for this file ]', end='\\r')\n            return\n",
  "slicing": "    def log(self, curr, total, width=40, force=False):\n"
 },
 "790": {
  "name": "path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "74",
  "column": "10",
  "context": "   )\n        print(progress, end='\\r')\n\ndef built(path, version_string=None):\n    \"\"\"Checks if '.built' flag has been set for th",
  "context_lines": "            curr,\n            total\n        )\n        print(progress, end='\\r')\n\ndef built(path, version_string=None):\n    \"\"\"Checks if '.built' flag has been set for that task.\n    If a version_string is provided, this has to match, or the version\n    is regarded as not built.\n    \"\"\"\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "791": {
  "name": "version_string",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "74",
  "column": "16",
  "context": "       print(progress, end='\\r')\n\ndef built(path, version_string=None):\n    \"\"\"Checks if '.built' flag has been set for th",
  "context_lines": "            curr,\n            total\n        )\n        print(progress, end='\\r')\n\ndef built(path, version_string=None):\n    \"\"\"Checks if '.built' flag has been set for that task.\n    If a version_string is provided, this has to match, or the version\n    is regarded as not built.\n    \"\"\"\n",
  "slicing": [
   "def built(path, version_string=None):\n",
   "        built_file.write('\\n' + version_string)\n",
   "    if not built(dpath, version):\n"
  ]
 },
 "792": {
  "name": "path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "93",
  "column": "14",
  "context": "tring if version_string else True\n\n\ndef mark_done(path, fnames, version_string='vXX'):\n    \"\"\"Marks the path as done by adding a '.built'",
  "context_lines": "        for fname in text[1:-1]:\n            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n                return False\n        return text[-1] == version_string if version_string else True\n\n\ndef mark_done(path, fnames, version_string='vXX'):\n    \"\"\"Marks the path as done by adding a '.built' file with the current\n    timestamp plus a version description string if specified.\n    \"\"\"\n    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "def mark_done(path, fnames, version_string='vXX'):\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "793": {
  "name": "fnames",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "93",
  "column": "20",
  "context": "if version_string else True\n\n\ndef mark_done(path, fnames, version_string='vXX'):\n    \"\"\"Marks the path as done by adding a '.built'",
  "context_lines": "        for fname in text[1:-1]:\n            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n                return False\n        return text[-1] == version_string if version_string else True\n\n\ndef mark_done(path, fnames, version_string='vXX'):\n    \"\"\"Marks the path as done by adding a '.built' file with the current\n    timestamp plus a version description string if specified.\n    \"\"\"\n    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
  "slicing": [
   "def mark_done(path, fnames, version_string='vXX'):\n",
   "        mark_done(dpath, fnames, version)\n"
  ]
 },
 "794": {
  "name": "version_string",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "93",
  "column": "28",
  "context": "on_string else True\n\n\ndef mark_done(path, fnames, version_string='vXX'):\n    \"\"\"Marks the path as done by adding a '.built'",
  "context_lines": "        for fname in text[1:-1]:\n            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n                return False\n        return text[-1] == version_string if version_string else True\n\n\ndef mark_done(path, fnames, version_string='vXX'):\n    \"\"\"Marks the path as done by adding a '.built' file with the current\n    timestamp plus a version description string if specified.\n    \"\"\"\n    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
  "slicing": [
   "def mark_done(path, fnames, version_string='vXX'):\n",
   "        built_file.write('\\n' + version_string)\n",
   "        mark_done(dpath, fnames, version)\n"
  ]
 },
 "795": {
  "name": "url",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "105",
  "column": "13",
  "context": "_file.write('\\n' + version_string)\n\n\ndef download(url, path, fname, redownload=False):\n    \"\"\"Downloads file using `requests`. If ``redow",
  "context_lines": "        for fname in fnames:\n            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n            built_file.write('\\n' + fname)\n        built_file.write('\\n' + version_string)\n\n\ndef download(url, path, fname, redownload=False):\n    \"\"\"Downloads file using `requests`. If ``redownload`` is set to false, then\n    will not download tar file again if it is present (default ``True``).\"\"\"\n    outfile = os.path.join(path, fname)\n    curr_download = not os.path.isfile(outfile) or redownload\n",
  "slicing": [
   "def download(url, path, fname, redownload=False):\n",
   "            download(url, dpath, fname)\n"
  ]
 },
 "796": {
  "name": "path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "105",
  "column": "18",
  "context": ".write('\\n' + version_string)\n\n\ndef download(url, path, fname, redownload=False):\n    \"\"\"Downloads file using `requests`. If ``redow",
  "context_lines": "        for fname in fnames:\n            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n            built_file.write('\\n' + fname)\n        built_file.write('\\n' + version_string)\n\n\ndef download(url, path, fname, redownload=False):\n    \"\"\"Downloads file using `requests`. If ``redownload`` is set to false, then\n    will not download tar file again if it is present (default ``True``).\"\"\"\n    outfile = os.path.join(path, fname)\n    curr_download = not os.path.isfile(outfile) or redownload\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "def download(url, path, fname, redownload=False):\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "797": {
  "name": "fname",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "105",
  "column": "24",
  "context": "('\\n' + version_string)\n\n\ndef download(url, path, fname, redownload=False):\n    \"\"\"Downloads file using `requests`. If ``redow",
  "context_lines": "        for fname in fnames:\n            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n            built_file.write('\\n' + fname)\n        built_file.write('\\n' + version_string)\n\n\ndef download(url, path, fname, redownload=False):\n    \"\"\"Downloads file using `requests`. If ``redownload`` is set to false, then\n    will not download tar file again if it is present (default ``True``).\"\"\"\n    outfile = os.path.join(path, fname)\n    curr_download = not os.path.isfile(outfile) or redownload\n",
  "slicing": [
   "def download(url, path, fname, redownload=False):\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n"
  ]
 },
 "798": {
  "name": "redownload",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "105",
  "column": "31",
  "context": " version_string)\n\n\ndef download(url, path, fname, redownload=False):\n    \"\"\"Downloads file using `requests`. If ``redow",
  "context_lines": "        for fname in fnames:\n            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n            built_file.write('\\n' + fname)\n        built_file.write('\\n' + version_string)\n\n\ndef download(url, path, fname, redownload=False):\n    \"\"\"Downloads file using `requests`. If ``redownload`` is set to false, then\n    will not download tar file again if it is present (default ``True``).\"\"\"\n    outfile = os.path.join(path, fname)\n    curr_download = not os.path.isfile(outfile) or redownload\n",
  "slicing": "def download(url, path, fname, redownload=False):\n"
 },
 "799": {
  "name": "path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "180",
  "column": "13",
  "context": "        move(resume_file, outfile)\n\n\ndef make_dir(path):\n    \"\"\"Makes the directory and any nonexistent par",
  "context_lines": "            raise RuntimeWarning('Received less data than specified in ' +\n                                 'Content-Length header for ' + url + '.' +\n                                 ' There may be a download problem.')\n        move(resume_file, outfile)\n\n\ndef make_dir(path):\n    \"\"\"Makes the directory and any nonexistent parent directories.\"\"\"\n    # the current working directory is a fine path\n    if path != '':\n        os.makedirs(path, exist_ok=True)\n\n\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "800": {
  "name": "path1",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "187",
  "column": "9",
  "context": "      os.makedirs(path, exist_ok=True)\n\n\ndef move(path1, path2):\n    \"\"\"Renames the given file.\"\"\"\n    shutil.move(",
  "context_lines": "    \"\"\"Makes the directory and any nonexistent parent directories.\"\"\"\n    # the current working directory is a fine path\n    if path != '':\n        os.makedirs(path, exist_ok=True)\n\n\ndef move(path1, path2):\n    \"\"\"Renames the given file.\"\"\"\n    shutil.move(path1, path2)\n\n\ndef remove_dir(path):\n    \"\"\"Removes the given directory, if it exists.\"\"\"\n",
  "slicing": "def move(path1, path2):\n"
 },
 "801": {
  "name": "path2",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "187",
  "column": "16",
  "context": "s.makedirs(path, exist_ok=True)\n\n\ndef move(path1, path2):\n    \"\"\"Renames the given file.\"\"\"\n    shutil.move(",
  "context_lines": "    \"\"\"Makes the directory and any nonexistent parent directories.\"\"\"\n    # the current working directory is a fine path\n    if path != '':\n        os.makedirs(path, exist_ok=True)\n\n\ndef move(path1, path2):\n    \"\"\"Renames the given file.\"\"\"\n    shutil.move(path1, path2)\n\n\ndef remove_dir(path):\n    \"\"\"Removes the given directory, if it exists.\"\"\"\n",
  "slicing": "def move(path1, path2):\n"
 },
 "802": {
  "name": "path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "192",
  "column": "15",
  "context": "\"\"\n    shutil.move(path1, path2)\n\n\ndef remove_dir(path):\n    \"\"\"Removes the given directory, if it exists.\"",
  "context_lines": "        os.makedirs(path, exist_ok=True)\n\n\ndef move(path1, path2):\n    \"\"\"Renames the given file.\"\"\"\n    shutil.move(path1, path2)\n\n\ndef remove_dir(path):\n    \"\"\"Removes the given directory, if it exists.\"\"\"\n    shutil.rmtree(path, ignore_errors=True)\n\n\ndef untar(path, fname, deleteTar=True):\n    \"\"\"Unpacks the given archive file to the same directory, then (by default)\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "803": {
  "name": "path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "197",
  "column": "10",
  "context": "util.rmtree(path, ignore_errors=True)\n\n\ndef untar(path, fname, deleteTar=True):\n    \"\"\"Unpacks the given archive file to the same ",
  "context_lines": "    shutil.move(path1, path2)\n\n\ndef remove_dir(path):\n    \"\"\"Removes the given directory, if it exists.\"\"\"\n    shutil.rmtree(path, ignore_errors=True)\n\n\ndef untar(path, fname, deleteTar=True):\n    \"\"\"Unpacks the given archive file to the same directory, then (by default)\n    deletes the archive file.\n    \"\"\"\n    print('unpacking ' + fname)\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "def untar(path, fname, deleteTar=True):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "804": {
  "name": "fname",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "197",
  "column": "16",
  "context": "mtree(path, ignore_errors=True)\n\n\ndef untar(path, fname, deleteTar=True):\n    \"\"\"Unpacks the given archive file to the same ",
  "context_lines": "    shutil.move(path1, path2)\n\n\ndef remove_dir(path):\n    \"\"\"Removes the given directory, if it exists.\"\"\"\n    shutil.rmtree(path, ignore_errors=True)\n\n\ndef untar(path, fname, deleteTar=True):\n    \"\"\"Unpacks the given archive file to the same directory, then (by default)\n    deletes the archive file.\n    \"\"\"\n    print('unpacking ' + fname)\n",
  "slicing": [
   "def untar(path, fname, deleteTar=True):\n",
   "                untar(dpath, fname)\n"
  ]
 },
 "805": {
  "name": "deleteTar",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "197",
  "column": "23",
  "context": "ath, ignore_errors=True)\n\n\ndef untar(path, fname, deleteTar=True):\n    \"\"\"Unpacks the given archive file to the same ",
  "context_lines": "    shutil.move(path1, path2)\n\n\ndef remove_dir(path):\n    \"\"\"Removes the given directory, if it exists.\"\"\"\n    shutil.rmtree(path, ignore_errors=True)\n\n\ndef untar(path, fname, deleteTar=True):\n    \"\"\"Unpacks the given archive file to the same directory, then (by default)\n    deletes the archive file.\n    \"\"\"\n    print('unpacking ' + fname)\n",
  "slicing": "def untar(path, fname, deleteTar=True):\n"
 },
 "806": {
  "name": "file1",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "211",
  "column": "8",
  "context": " deleteTar:\n        os.remove(fullpath)\n\n\ndef cat(file1, file2, outfile, deleteFiles=True):\n    with open(outfile, 'wb') as wfd:\n        for f",
  "context_lines": "    else:\n        shutil.unpack_archive(fullpath, path)\n    if deleteTar:\n        os.remove(fullpath)\n\n\ndef cat(file1, file2, outfile, deleteFiles=True):\n    with open(outfile, 'wb') as wfd:\n        for f in [file1, file2]:\n            with open(f, 'rb') as fd:\n                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
  "slicing": "def cat(file1, file2, outfile, deleteFiles=True):\n"
 },
 "807": {
  "name": "file2",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "211",
  "column": "15",
  "context": "Tar:\n        os.remove(fullpath)\n\n\ndef cat(file1, file2, outfile, deleteFiles=True):\n    with open(outfile, 'wb') as wfd:\n        for f",
  "context_lines": "    else:\n        shutil.unpack_archive(fullpath, path)\n    if deleteTar:\n        os.remove(fullpath)\n\n\ndef cat(file1, file2, outfile, deleteFiles=True):\n    with open(outfile, 'wb') as wfd:\n        for f in [file1, file2]:\n            with open(f, 'rb') as fd:\n                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
  "slicing": "def cat(file1, file2, outfile, deleteFiles=True):\n"
 },
 "808": {
  "name": "outfile",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "211",
  "column": "22",
  "context": "      os.remove(fullpath)\n\n\ndef cat(file1, file2, outfile, deleteFiles=True):\n    with open(outfile, 'wb') as wfd:\n        for f",
  "context_lines": "    else:\n        shutil.unpack_archive(fullpath, path)\n    if deleteTar:\n        os.remove(fullpath)\n\n\ndef cat(file1, file2, outfile, deleteFiles=True):\n    with open(outfile, 'wb') as wfd:\n        for f in [file1, file2]:\n            with open(f, 'rb') as fd:\n                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
  "slicing": "def cat(file1, file2, outfile, deleteFiles=True):\n"
 },
 "809": {
  "name": "deleteFiles",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "211",
  "column": "31",
  "context": "remove(fullpath)\n\n\ndef cat(file1, file2, outfile, deleteFiles=True):\n    with open(outfile, 'wb') as wfd:\n        for f",
  "context_lines": "    else:\n        shutil.unpack_archive(fullpath, path)\n    if deleteTar:\n        os.remove(fullpath)\n\n\ndef cat(file1, file2, outfile, deleteFiles=True):\n    with open(outfile, 'wb') as wfd:\n        for f in [file1, file2]:\n            with open(f, 'rb') as fd:\n                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
  "slicing": "def cat(file1, file2, outfile, deleteFiles=True):\n"
 },
 "810": {
  "name": "response",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "222",
  "column": "23",
  "context": "        os.remove(file2)\n\n\ndef _get_confirm_token(response):\n    for key, value in response.cookies.items():\n  ",
  "context_lines": "                # 10MB per writing chunk to avoid reading big file into memory.\n    if deleteFiles:\n        os.remove(file1)\n        os.remove(file2)\n\n\ndef _get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            return value\n    return None\n\n",
  "slicing": [
   "MODULE_ROOT = Path(__file__).parent.parent\n",
   "FIXTURES_ROOT = (MODULE_ROOT / \"tests\" / \"fixtures\").resolve()\n",
   "PACKAGE_ROOT = MODULE_ROOT.parent\n",
   "DATA_ROOT = (PACKAGE_ROOT / \"data\").resolve()\n",
   "        for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
   "                return \"%3.1f%s%s\" % (num, unit, suffix)\n",
   "            num /= 1024.0\n",
   "        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
   "        curr_time = time.time()\n",
   "        if not force and curr_time - self.latest < self.throttle_speed:\n",
   "            self.latest = curr_time\n",
   "        self.latest = curr_time\n",
   "        done = min(curr * width // total, width)\n",
   "        remain = width - done\n",
   "            curr = self.humanize(curr)\n",
   "            total = self.humanize(total)\n",
   "        progress = '[{}{}] {} / {}'.format(\n",
   "            ''.join(['|'] * done),\n",
   "            ''.join(['.'] * remain),\n",
   "            curr,\n",
   "            total\n",
   "        print(progress, end='\\r')\n",
   "def built(path, version_string=None):\n",
   "    built_file_path = os.path.join(path, '.built')\n",
   "    if not os.path.isfile(built_file_path):\n",
   "        with open(built_file_path, 'r') as built_file:\n",
   "            text = built_file.read().split('\\n')\n",
   "        if len(text) <= 2:\n",
   "        for fname in text[1:-1]:\n",
   "            if not os.path.isfile(os.path.join(path, fname)) and not os.path.isdir(os.path.join(path, fname)):\n",
   "        return text[-1] == version_string if version_string else True\n",
   "    with open(os.path.join(path, '.built'), 'w') as built_file:\n",
   "        built_file.write(str(datetime.datetime.today()))\n",
   "        for fname in fnames:\n",
   "            fname = fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "            built_file.write('\\n' + fname)\n",
   "        built_file.write('\\n' + version_string)\n",
   "    outfile = os.path.join(path, fname)\n",
   "    curr_download = not os.path.isfile(outfile) or redownload\n",
   "    print(\"[ downloading: \" + url + \" to \" + outfile + \" ]\")\n",
   "    retry = 5\n",
   "    exp_backoff = [2 ** r for r in reversed(range(retry))]\n",
   "    logger = ProgressLogger()\n",
   "    while curr_download and retry >= 0:\n",
   "        resume_file = outfile + '.part'\n",
   "        resume = os.path.isfile(resume_file)\n",
   "        if resume:\n",
   "            resume_pos = os.path.getsize(resume_file)\n",
   "            mode = 'ab'\n",
   "            resume_pos = 0\n",
   "            mode = 'wb'\n",
   "        response = None\n",
   "        with requests.Session() as session:\n",
   "                header = {'Range': 'bytes=%d-' % resume_pos,\n",
   "                          'Accept-Encoding': 'identity'} if resume else {}\n",
   "                response = session.get(url, stream=True, timeout=5, headers=header)\n",
   "                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':\n",
   "                    resume_pos = 0\n",
   "                    mode = 'wb'\n",
   "                CHUNK_SIZE = 32768\n",
   "                total_size = int(response.headers.get('Content-Length', -1))\n",
   "                total_size += resume_pos\n",
   "                done = resume_pos\n",
   "                with open(resume_file, mode) as f:\n",
   "                    for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                        if chunk:  # filter out keep-alive new chunks\n",
   "                            f.write(chunk)\n",
   "                        if total_size > 0:\n",
   "                            done += len(chunk)\n",
   "                            if total_size < done:\n",
   "                                total_size = done\n",
   "                            logger.log(done, total_size)\n",
   "                retry -= 1\n",
   "                if retry >= 0:\n",
   "                    print('Connection error, retrying. (%d retries left)' % retry)\n",
   "                    time.sleep(exp_backoff[retry])\n",
   "                if response:\n",
   "                    response.close()\n",
   "    if retry < 0:\n",
   "    if curr_download and retry > 0:\n",
   "        logger.log(done, total_size, force=True)\n",
   "        if done < total_size:\n",
   "        move(resume_file, outfile)\n",
   "def make_dir(path):\n",
   "def remove_dir(path):\n",
   "    print('unpacking ' + fname)\n",
   "    fullpath = os.path.join(path, fname)\n",
   "    if '.tar.gz' in fname:\n",
   "        shutil.unpack_archive(fullpath, path, format='gztar')\n",
   "        shutil.unpack_archive(fullpath, path)\n",
   "        os.remove(fullpath)\n",
   "    with open(outfile, 'wb') as wfd:\n",
   "        for f in [file1, file2]:\n",
   "            with open(f, 'rb') as fd:\n",
   "                shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n",
   "def _get_confirm_token(response):\n",
   "    for key, value in response.cookies.items():\n",
   "        if key.startswith('download_warning'):\n",
   "            return value\n",
   "    URL = 'https://docs.google.com/uc?export=download'\n",
   "    with requests.Session() as session:\n",
   "        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
   "        token = _get_confirm_token(response)\n",
   "        if token:\n",
   "            response.close()\n",
   "            params = {'id': gd_id, 'confirm': token}\n",
   "            response = session.get(URL, params=params, stream=True)\n",
   "        CHUNK_SIZE = 32768\n",
   "        with open(destination, 'wb') as f:\n",
   "            for chunk in response.iter_content(CHUNK_SIZE):\n",
   "                if chunk:  # filter out keep-alive new chunks\n",
   "                    f.write(chunk)\n",
   "        response.close()\n",
   "    dpath = str(DATA_ROOT / local_folder)\n",
   "    out_paths = list(dpath + '/' + fname.replace('.tar.gz', '').replace('.tgz', '').replace('.gz', '').replace('.zip', '')\n",
   "                     for fname in fnames)\n",
   "    if not built(dpath, version):\n",
   "        for fname in fnames:\n",
   "            print('[building data: ' + dpath + '/' + fname + ']')\n",
   "        if built(dpath):\n",
   "            remove_dir(dpath)\n",
   "        make_dir(dpath)\n",
   "            paths = [paths] * len(fnames)\n",
   "        for fname, path in zip(fnames, paths):\n",
   "            if path == 'aws':\n",
   "                url = 'http://huggingface.co/downloads/models/'\n",
   "                url += local_folder + '/'\n",
   "                url += fname\n",
   "                url = path + '/' + fname\n",
   "            download(url, dpath, fname)\n",
   "            if '.tar.gz' in fname or '.tgz' in fname or '.gz' in fname or '.zip' in fname:\n",
   "                untar(dpath, fname)\n",
   "        mark_done(dpath, fnames, version)\n",
   "    return out_paths\n"
  ]
 },
 "811": {
  "name": "gd_id",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "228",
  "column": "31",
  "context": "e\n    return None\n\ndef download_from_google_drive(gd_id, destination):\n    \"\"\"Uses the requests package to download a fil",
  "context_lines": "    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            return value\n    return None\n\ndef download_from_google_drive(gd_id, destination):\n    \"\"\"Uses the requests package to download a file from Google Drive.\"\"\"\n    URL = 'https://docs.google.com/uc?export=download'\n\n    with requests.Session() as session:\n        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
  "slicing": "def download_from_google_drive(gd_id, destination):\n"
 },
 "812": {
  "name": "destination",
  "type": "UNKNOWN",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "228",
  "column": "38",
  "context": "eturn None\n\ndef download_from_google_drive(gd_id, destination):\n    \"\"\"Uses the requests package to download a fil",
  "context_lines": "    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            return value\n    return None\n\ndef download_from_google_drive(gd_id, destination):\n    \"\"\"Uses the requests package to download a file from Google Drive.\"\"\"\n    URL = 'https://docs.google.com/uc?export=download'\n\n    with requests.Session() as session:\n        response = session.get(URL, params={'id': gd_id}, stream=True)\n",
  "slicing": "def download_from_google_drive(gd_id, destination):\n"
 },
 "813": {
  "name": "fnames",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "249",
  "column": "19",
  "context": "nk)\n        response.close()\n\n\ndef download_files(fnames: List[Union[str, Path]],\n                   local_folder: str,\n            ",
  "context_lines": "            for chunk in response.iter_content(CHUNK_SIZE):\n                if chunk:  # filter out keep-alive new chunks\n                    f.write(chunk)\n        response.close()\n\n\ndef download_files(fnames: List[Union[str, Path]],\n                   local_folder: str,\n                   version: str = 'v1.0',\n                   paths: Union[List[str], str] = 'aws') -> List[str]:\n    r\"\"\"Download model/data files from a url.\n\n",
  "slicing": "def download_files(fnames: List[Union[str, Path]],\n"
 },
 "814": {
  "name": "local_folder",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "250",
  "column": "19",
  "context": "names: List[Union[str, Path]],\n                   local_folder: str,\n                   version: str = 'v1.0',\n        ",
  "context_lines": "                if chunk:  # filter out keep-alive new chunks\n                    f.write(chunk)\n        response.close()\n\n\ndef download_files(fnames: List[Union[str, Path]],\n                   local_folder: str,\n                   version: str = 'v1.0',\n                   paths: Union[List[str], str] = 'aws') -> List[str]:\n    r\"\"\"Download model/data files from a url.\n\n    Args:\n",
  "slicing": "                   local_folder: str,\n"
 },
 "815": {
  "name": "version",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "251",
  "column": "19",
  "context": "            local_folder: str,\n                   version: str = 'v1.0',\n                   paths: Union[List[str], str] = ",
  "context_lines": "                    f.write(chunk)\n        response.close()\n\n\ndef download_files(fnames: List[Union[str, Path]],\n                   local_folder: str,\n                   version: str = 'v1.0',\n                   paths: Union[List[str], str] = 'aws') -> List[str]:\n    r\"\"\"Download model/data files from a url.\n\n    Args:\n        fnames: List of filenames to download\n",
  "slicing": "                   version: str = 'v1.0',\n"
 },
 "816": {
  "name": "paths",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "adversarialnlp/adversarialnlp/common/file_utils.py",
  "lineno": "252",
  "column": "19",
  "context": "        version: str = 'v1.0',\n                   paths: Union[List[str], str] = 'aws') -> List[str]:\n    r\"\"\"Download model/data files from a url.\n\n   ",
  "context_lines": "        response.close()\n\n\ndef download_files(fnames: List[Union[str, Path]],\n                   local_folder: str,\n                   version: str = 'v1.0',\n                   paths: Union[List[str], str] = 'aws') -> List[str]:\n    r\"\"\"Download model/data files from a url.\n\n    Args:\n        fnames: List of filenames to download\n        local_folder: Sub-folder of `./data` where models/data will\n",
  "slicing": "                   paths: Union[List[str], str] = 'aws') -> List[str]:\n"
 }
}